{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7c2d0ec-b1c2-4e74-8c57-562b03aff8f1",
   "metadata": {},
   "source": [
    "# Aumento de datos\n",
    "\n",
    "En este notebook se va a aplicar la técnica de ampliación de datos a un conjunto de reseñas de Google Maps separadas en dos ficheros: uno con las reseñas que se van a considerar válidas y el otro con las inválidas. Cada línea es una reseña nueva."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86fa20c9-4d14-4236-bcea-bb2e3b600f7e",
   "metadata": {},
   "source": [
    "# Aumento de datos tradicional"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0485a5-5543-499d-a2a6-ce3ab7333f3c",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f54d4a66-799a-460d-b32e-b71b3755cbf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/ibon/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/ibon/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from deep_translator import (GoogleTranslator, MyMemoryTranslator)\n",
    "import copy\n",
    "import time\n",
    "import random\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf20fd15-c646-4f3d-857f-3b2d8822f88e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Direcorio de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c311128e-92e1-4101-b796-8b8165fe12ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "validReviewsPath = \"/home/ibon/Documentos/GitHub/TFG/1. Data/4. Labeled Reviews/2. Without Emojis/ValidReviews.txt\"\n",
    "invalidReviewsPath = \"/home/ibon/Documentos/GitHub/TFG/1. Data/4. Labeled Reviews/2. Without Emojis/InvalidReviews.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6ac5b7-6508-4ea6-bca1-ffe46883ecc8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Pandas\n",
    "Se van a pasar los datos a dataframes: uno con las valoraciones validas y otro con las negativas. Cada fila del dataframe será una reseña"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66162710-f9ad-43e4-a0bf-1f5a92a065a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def importFromTxtToDF(source):\n",
    "    with open(source, 'r', encoding=\"utf-8\") as file:\n",
    "        #Generate a list with all the reviews\n",
    "        targetList = [line.strip() for line in file]\n",
    "\n",
    "    targetDF = pd.DataFrame(targetList, columns=['Text'])\n",
    "    return targetDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b5a9830-a956-4dd6-9b78-26f4866a408e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read the file with the valid reviews\n",
    "validReviewsDF = importFromTxtToDF(validReviewsPath)\n",
    "#Read the file with the invalid reviews\n",
    "invalidReviewsDF = importFromTxtToDF(invalidReviewsPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ca07cf-8f96-40c5-886e-9a26f7524bfe",
   "metadata": {},
   "source": [
    "Se muestran las primeras reseñas válidas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f1cdd36-9728-4962-a4bd-b6bf4731e01f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"Tiene fácil acceso para las personas con movi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"Espero que hayan mejorais\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"La estación es antigua, aparte de tener una s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"Bien\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"Bonito comodo\"</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text\n",
       "0  \"Tiene fácil acceso para las personas con movi...\n",
       "1                        \"Espero que hayan mejorais\"\n",
       "2  \"La estación es antigua, aparte de tener una s...\n",
       "3                                             \"Bien\"\n",
       "4                                    \"Bonito comodo\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validReviewsDF.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c57d6e4-e380-4a66-a893-b032b7257254",
   "metadata": {},
   "source": [
    "Se muestran las primeras reseñas inválidas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6074bca2-03f7-4503-8dcf-01069a61d953",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"He vivido 35 años en el barrio y reconozco qu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"localización con muchos bares interesantes\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"…\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"Muy rica comida..\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"Estación del.metro\"</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text\n",
       "0  \"He vivido 35 años en el barrio y reconozco qu...\n",
       "1       \"localización con muchos bares interesantes\"\n",
       "2                                                \"…\"\n",
       "3                                \"Muy rica comida..\"\n",
       "4                               \"Estación del.metro\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "invalidReviewsDF.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76053968-2837-4890-9018-53164468d5e2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Medidas de similitud\n",
    "Para poder comparar frases y seleccionar las mejores para generar los mejores datasets se van a desarrollar las siguientes medidas de similitud."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3cabc1a-a9a7-4338-b716-670faa6759f4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Similitud Semántica\n",
    "\n",
    "A continuación se va a diseñar una función para calcular la similitud semántica entre pares de oraciones. Es decir, se van a calcular los embeddings de oraciones de cada par de frases y se va a usar una métrica de similitud para ver como de parecido es el significado de ambas frases.\n",
    "\n",
    "Se va a usar una versión de SBERT, llamada MiniLM (Minimal Lenguaje Model), que utiliza una variante más pequeña. Se usa MiniLM de seis capas (L6), que logra una precisón buena con menos recursos.\n",
    "\n",
    "Este modelo fue entrenado usando un dataset que incluye datos en varios idiomas, entre ellos el español. Consecuentemente, no hay problema al introducir frases en castellano. Es cierto, que obtiene mejores resultados para frases en inglés, ya que se entreno con más datos en este idioma.\n",
    "\n",
    "MiniLM es un modelo específicamente entrenado para mapear frases y parrafos a un espacio vectorial de 384 dimensiones. Es decir, este modelo permite obtener un embedding de una frase directamente. Usando otros modelos esta tarea no es posible de forma directa, ya que devuelven un embedding para cada palabra del texto.\n",
    "\n",
    "El método de similitud que se va a usar es la similitud del coseno, por lo que los valores más cercanos a uno indicarán una mayor similitud entre las frases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f362a20-b0b3-47a0-a3c5-bd3d880b100a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "#Given two texts and a model, the semantic similarity of the texts is returned\n",
    "def getSemanticSimilarity(text1, text2, model):\n",
    "    #Get the embeddings of the senteces\n",
    "    embedding1 = model.encode(text1)\n",
    "    embedding2 = model.encode(text2)\n",
    "\n",
    "    #Get the cosine similarity of the senteces\n",
    "    similarity = cosine_similarity([embedding1], [embedding2])\n",
    "\n",
    "    return similarity[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12cfd655-adc8-4e17-b7e6-7317ee6ac73c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Similitud Léxica\n",
    "Se va a diseñar una función para calcular la similitud léxica entre pares de oraciones. La similitud léxica mide el grado de coincidencia de palabras o términos entre dos frases o textos, sin tener en cuenta el significado subyacente.\n",
    "\n",
    "Hay varias formas de realizar este cálculo: similitud del coseno basada en frecuencia de palabras, coeficiente de Jaccard,coeficiente de Dice ...\n",
    "\n",
    "En este caso, se cree que la mejor opción es usar el coeficiente de Jaccard ya que calcula la similitud en función de la proporción de palabras comunes sobre el total de palabras únicas. Consecuentemente, esto nos permitirá detectar frases con menos coincidencias exactas en palabras.\n",
    "\n",
    "Cuanto más cercano a uno sea el coeficiente de Jaccard más similares léxicamente serán las frases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9351fcd7-4bd3-44d4-87b5-f80332bb6b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import re\n",
    "\n",
    "#Clean up the text removing punctuation, accent marks and convertin everything to lowercase\n",
    "def cleanText(text):\n",
    "    text = unicodedata.normalize('NFKD', text.lower()).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca07c96b-cfa3-4497-af79-07918de06454",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSpanishStopWords():\n",
    "    determinantes = {\"el\", \"la\", \"los\", \"las\", \"un\", \"una\", \"unos\", \"unas\", \"este\", \"esta\", \"estos\", \"estas\",\n",
    "                 \"ese\", \"esa\", \"esos\", \"esas\", \"aquel\", \"aquella\", \"aquellos\", \"aquellas\", \"mi\", \"mis\",\n",
    "                 \"tu\", \"tus\", \"su\", \"sus\", \"nuestro\", \"nuestra\", \"nuestros\", \"nuestras\", \"vuestro\", \n",
    "                 \"vuestra\", \"vuestros\", \"vuestras\", \"primer\", \"primero\", \"primera\", \"segundo\", \"segunda\"}\n",
    "\n",
    "    preposiciones = {\"a\", \"ante\", \"bajo\", \"cabe\", \"con\", \"contra\", \"de\", \"desde\", \"durante\", \"en\", \"entre\", \n",
    "                 \"hacia\", \"hasta\", \"mediante\", \"para\", \"por\", \"según\", \"sin\", \"sobre\", \"tras\", \"versus\", \"vía\"}\n",
    "\n",
    "    conjunciones = {\"y\", \"e\", \"ni\", \"o\", \"u\", \"pero\", \"sino\", \"sino que\", \"mas\", \"aunque\", \"que\", \"porque\", \n",
    "                \"como\", \"cuando\", \"donde\", \"mientras\", \"para que\", \"a fin de que\", \"puesto que\", \"ya que\", \n",
    "                \"si\", \"siempre que\"}\n",
    "    pronombres = {\n",
    "        # Pronombres personales\n",
    "        \"yo\", \"tú\", \"vos\", \"él\", \"ella\", \"nosotros\", \"nosotras\", \n",
    "        \"vosotros\", \"vosotras\", \"ellos\", \"ellas\", \"usted\", \"ustedes\",\n",
    "        \"me\", \"te\", \"lo\", \"la\", \"nos\", \"os\", \"los\", \"las\", \"le\", \"les\", \"se\",\n",
    "    \n",
    "        # Pronombres posesivos\n",
    "        \"mío\", \"mía\", \"míos\", \"mías\", \n",
    "        \"tuyo\", \"tuya\", \"tuyos\", \"tuyas\", \n",
    "        \"suyo\", \"suya\", \"suyos\", \"suyas\", \n",
    "        \"nuestro\", \"nuestra\", \"nuestros\", \"nuestras\", \n",
    "        \"vuestro\", \"vuestra\", \"vuestros\", \"vuestras\",\n",
    "    \n",
    "        # Pronombres demostrativos\n",
    "        \"este\", \"esta\", \"estos\", \"estas\", \n",
    "        \"ese\", \"esa\", \"esos\", \"esas\", \n",
    "        \"aquel\", \"aquella\", \"aquellos\", \"aquellas\",\n",
    "    \n",
    "        # Pronombres relativos\n",
    "        \"que\", \"cual\", \"cuales\", \"quien\", \"quienes\", \n",
    "        \"cuyo\", \"cuya\", \"cuyos\", \"cuyas\", \"donde\",\n",
    "    \n",
    "        # Pronombres interrogativos y exclamativos\n",
    "        \"qué\", \"quién\", \"quiénes\", \"cuál\", \"cuáles\", \n",
    "        \"cuánto\", \"cuánta\", \"cuántos\", \"cuántas\", \n",
    "        \"dónde\", \"cómo\", \"cuándo\",\n",
    "    \n",
    "        # Pronombres indefinidos\n",
    "        \"alguien\", \"algo\", \"nadie\", \"nada\", \"cualquiera\", \n",
    "        \"todos\", \"todas\", \"varios\", \"varias\", \"muchos\", \n",
    "        \"muchas\", \"pocos\", \"pocas\", \"alguno\", \"alguna\", \n",
    "        \"algunos\", \"algunas\", \"ninguno\", \"ninguna\", \n",
    "        \"uno\", \"una\", \"unos\", \"unas\", \"demás\"\n",
    "    }\n",
    "\n",
    "    #Combine all the words in one set\n",
    "    spanishStopWords = determinantes | preposiciones | conjunciones | pronombres\n",
    "\n",
    "    return spanishStopWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9cb9a832-8b99-4a74-a0f4-9fd6fabd3716",
   "metadata": {},
   "outputs": [],
   "source": [
    "def revomeSpanishStopWords(text):\n",
    "    spanishStopWords = getSpanishStopWords()\n",
    "\n",
    "    textWithoutStopWords = [word for word in text.split() if word.lower() not in spanishStopWords]\n",
    "\n",
    "    return \" \".join(textWithoutStopWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c35a79c-e4e7-4854-9b4f-67b925b37928",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Jaccard similarity\n",
    "def jaccardSimilarity(text1, text2):\n",
    "    #Get the set of words of each text\n",
    "    wordsInText1 = set(revomeSpanishStopWords(cleanText(text1)).split())\n",
    "    wordsInText2 = set(revomeSpanishStopWords(cleanText(text2)).split())\n",
    "\n",
    "    intersection = len(wordsInText1.intersection(wordsInText2)) \n",
    "    union = len(wordsInText1.union(wordsInText2))\n",
    "    \n",
    "    if union == 0:\n",
    "        return 0\n",
    "\n",
    "    #intersection / union\n",
    "    return intersection / union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b3c7a577-4e14-4d46-9954-cf96575bf14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Given two texts, the Jaccard similarity of those texts is returned\n",
    "def getLexicalSimilarity(text1, text2):\n",
    "    return jaccardSimilarity(text1, text2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b81dbd-a09d-4a75-9fd0-ae00c1d45366",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Retrotraducción\n",
    "\n",
    "El primer método de ampliación de datos que se va a usar va a ser la retrotraducción. Consiste en traducir el texto a un idioma distinto y luego volverlo a traducir al idioma original. \n",
    "\n",
    "Este proceso puede genera texto con el mismo significado que el original pero distintas palabras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d137f41-d490-4723-b33d-252f91ea8040",
   "metadata": {},
   "outputs": [],
   "source": [
    "def BackTranslation(translatorsList, reviewsDF, targetPath):\n",
    "    #Generate a data list to store the text that has to be translated\n",
    "    notTranslatedList = reviewsDF['Text'].tolist()\n",
    "\n",
    "    #Translate the text as many times as needed\n",
    "    for translator in translatorsList:\n",
    "        #Generate a data frame to store the text that has been translated\n",
    "        translatedList = []\n",
    "        for elem in notTranslatedList:\n",
    "            #Translate all the reviews\n",
    "            try:\n",
    "                translation = translator.translate(elem)\n",
    "            except Exception as e: #If the translation fails \"\" is written\n",
    "                translation = '\"\"'\n",
    "            #If an error ocurred translate it to a \"\"\n",
    "            if translation == None:\n",
    "                translation = '\"\"'\n",
    "                \n",
    "            #Save the translations in the corresponding list \n",
    "            translatedList.append(translation)\n",
    "\n",
    "            #Wait 0.2 seconds not to collapse the server\n",
    "            time.sleep(0.2)\n",
    "\n",
    "        #Prepare to translate again if needed\n",
    "        notTranslatedList = copy.deepcopy(translatedList) \n",
    "       \n",
    "    #Open the file in which the translations are strored\n",
    "    translationFile = open(targetPath, 'w', encoding=\"utf-8\")\n",
    "    #write all the translations\n",
    "    for elem in translatedList:\n",
    "        translationFile.write(elem + \"\\n\")\n",
    "    #Close the file\n",
    "    translationFile.close()\n",
    "    \n",
    "    return pd.DataFrame(translatedList, columns=['Text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3dd031-96ec-4ac7-9c93-c7ea4cff8fce",
   "metadata": {},
   "source": [
    "### Google Translator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28785755-a28b-4caa-8c37-9f86cabb95e1",
   "metadata": {},
   "source": [
    "Primero se va a traducir del castellano al ingles y luego del inglés al castellano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d8c9d79f-598c-423d-9092-05b4ed9e0fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "validPath = '1. Back Translation\\\\1. Google Translator\\\\ValidReviewsTranslationsEsEnEnEs.txt'\n",
    "invalidPath = '1. Back Translation\\\\1. Google Translator\\\\InvalidReviewsTranslationsEsEnEnEs.txt'\n",
    "\n",
    "firstTranslator = GoogleTranslator(source = 'es', target = 'en')\n",
    "secondTranslator = GoogleTranslator(source='en', target='es')\n",
    "\n",
    "translatorList = [firstTranslator, secondTranslator]\n",
    "\n",
    "validSpanishReviewsGoogleEsEnEnEsDF = BackTranslation(translatorList, validReviewsDF, validPath)\n",
    "invalidSpanishReviewsGoogleEsEnEnESDF = BackTranslation(translatorList, invalidReviewsDF, invalidPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92e9e09-275c-4b2e-bc8a-0824a8e809fc",
   "metadata": {},
   "source": [
    "A continuación se va a traducir del castellano al japonés y del japonés al castellano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "587bc968-c0b2-437c-8f89-e3bd1e30ca7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "validPath = '1. Back Translation\\\\1. Google Translator\\\\ValidReviewsTranslationsEsJaJaEs.txt'\n",
    "invalidPath = '1. Back Translation\\\\1. Google Translator\\\\InvalidReviewsTranslationsEsJaJaEs.txt'\n",
    "\n",
    "firstTranslator = GoogleTranslator(source = 'es', target = 'ja')\n",
    "secondTranslator = GoogleTranslator(source='ja', target='es')\n",
    "\n",
    "translatorList = [firstTranslator, secondTranslator]\n",
    "\n",
    "validSpanishReviewsGoogleEsJaJaEsDF = BackTranslation(translatorList, validReviewsDF, validPath)\n",
    "invalidSpanishReviewsGoogleEsJaJaEsDF = BackTranslation(translatorList, invalidReviewsDF, invalidPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9adc18-5942-4324-9ce4-52d941f1ef0a",
   "metadata": {},
   "source": [
    "Por último se va a implementar una cadena de traducciones más larga: castellano a frances, frances a japones, japones a ruso y ruso a catellano."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "14f0b7f2-efd6-4329-8d88-07b12d32e130",
   "metadata": {},
   "outputs": [],
   "source": [
    "validPath = '1. Back Translation\\\\1. Google Translator\\\\ValidReviewsTranslationsEsFrFrJaJaRuRuEs.txt'\n",
    "invalidPath = '1. Back Translation\\\\1. Google Translator\\\\InvalidReviewsTranslationsEsFrFrJaJaRuRuEs.txt'\n",
    "\n",
    "firstTranslator = GoogleTranslator(source = 'es', target = 'fr')\n",
    "secondTranslator = GoogleTranslator(source='fr', target='ja')\n",
    "thirdTranslator = GoogleTranslator(source='ja', target='ru')\n",
    "fourthTranslator = GoogleTranslator(source='ru', target='es')\n",
    "\n",
    "translatorList = [firstTranslator, secondTranslator, thirdTranslator, fourthTranslator]\n",
    "\n",
    "validSpanishReviewsGoogleEsFrFrJaJaRuRuEsDF = BackTranslation(translatorList, validReviewsDF, validPath)\n",
    "invalidSpanishReviewsGoogleEsFrFrJaJaRuRuEsDF = BackTranslation(translatorList, invalidReviewsDF, invalidPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a245504-b6e0-47a3-9276-2498b5f147dd",
   "metadata": {},
   "source": [
    "### MyMemory Translator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79951b8d-efd6-414f-a644-d44895f204c3",
   "metadata": {},
   "source": [
    "Se van a realizar las mismas traducciones pero usando otro traductor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42669f5-986e-457f-943e-354d638a8da5",
   "metadata": {},
   "source": [
    "Castellano -> Inglés -> Castellano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "33609a91-efa8-4d7c-9ab0-cdd9238b1f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "validPath = '1. Back Translation\\\\2. MyMemory Translator\\\\ValidReviewsTranslationsEsEnEnEs.txt'\n",
    "invalidPath = '1. Back Translation\\\\2. MyMemory Translator\\\\InvalidReviewsTranslationsEsEnEnEs.txt'\n",
    "\n",
    "firstTranslator = MyMemoryTranslator(source = 'spanish', target = 'english')\n",
    "secondTranslator = MyMemoryTranslator(source='english', target='spanish')\n",
    "\n",
    "translatorList = [firstTranslator, secondTranslator]\n",
    "\n",
    "validSpanishReviewsMyMemoryEsEnEnEsDF = BackTranslation(translatorList, validReviewsDF, validPath)\n",
    "invalidSpanishReviewsMyMemoryEsEnEnEsDF = BackTranslation(translatorList, invalidReviewsDF, invalidPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90792d9d-d9df-43cf-96e1-4b8793450c79",
   "metadata": {},
   "source": [
    "Castellano -> Japonés -> Castellano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a207d69a-a95f-401b-ad78-0ca9c32cffba",
   "metadata": {},
   "outputs": [],
   "source": [
    "validPath = '1. Back Translation\\\\2. MyMemory Translator\\\\ValidReviewsTranslationsEsJaJaEs.txt'\n",
    "invalidPath = '1. Back Translation\\\\2. MyMemory Translator\\\\InvalidReviewsTranslationsEsJaJaEs.txt'\n",
    "\n",
    "firstTranslator = MyMemoryTranslator(source = 'spanish', target = 'japanese')\n",
    "secondTranslator = MyMemoryTranslator(source='japanese', target='spanish')\n",
    "\n",
    "translatorList = [firstTranslator, secondTranslator]\n",
    "\n",
    "validSpanishReviewsMyMemoryEsJaJaEsDF = BackTranslation(translatorList, validReviewsDF, validPath)\n",
    "invalidSpanishReviewsMyMemoryEsJaJaEsDF = BackTranslation(translatorList, invalidReviewsDF, invalidPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513691e0-fab9-4d6a-9ee5-0fb7716dc9ef",
   "metadata": {},
   "source": [
    "Castellano -> Francés -> Japonés -> Ruso -> Castellano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4a0f68de-f676-421f-ba70-d85af504a9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "validPath = '1. Back Translation\\\\2. MyMemory Translator\\\\ValidReviewsTranslationsEsFrFrJaJaRuRuEs.txt'\n",
    "invalidPath = '1. Back Translation\\\\2. MyMemory Translator\\\\InvalidReviewsTranslationsEsFrFrJaJaRuRuEs.txt'\n",
    "\n",
    "firstTranslator = MyMemoryTranslator(source = 'spanish', target = 'french')\n",
    "secondTranslator = MyMemoryTranslator(source='french', target='japanese')\n",
    "thirdTranslator = MyMemoryTranslator(source='japanese', target='russian')\n",
    "fourthTranslator = MyMemoryTranslator(source='russian', target='spanish')\n",
    "\n",
    "translatorList = [firstTranslator, secondTranslator, thirdTranslator, fourthTranslator]\n",
    "\n",
    "validSpanishReviewsMyMemoryEsFrFrJaJaRuRuEsDF = BackTranslation(translatorList, validReviewsDF, validPath)\n",
    "invalidSpanishReviewsMyMemoryEsFrFrJaJaRuRuEsDF = BackTranslation(translatorList, invalidReviewsDF, invalidPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bac7f30-7ad4-4660-8633-a0b4f2b725f2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Análisis de la retrotraducción y selección de los datos\n",
    "\n",
    "Dado que el código correspondiente a la traducción llevó largo rato y se dejo a la noche ejecutando, se vuelven a importar los datos a dataframes:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d80455-27fb-4758-90ca-a6b187e3649b",
   "metadata": {},
   "source": [
    "#### MyMemory genera errores en la traducción debido a problemas de conexión con el servidor. Consecuentemente, se procede a anilizar únicamente los datos generados por el traductor de Google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "676fead3-89d2-41ca-9d2c-75df79bb16bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def importFromTxtToList(source):\n",
    "    with open(source, 'r', encoding=\"utf-8\") as file:\n",
    "        #Generate a list with all the reviews\n",
    "        targetList = [line.strip() for line in file]\n",
    "    return targetList"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3319d605-4149-4b84-b1ce-3f4124398124",
   "metadata": {},
   "source": [
    "Frases originales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f6bd3ee6-89fa-4cf7-b50f-d1e49d2364e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "validPath = \"/home/ibon/Documentos/GitHub/TFG/1. Data/4. Labeled Reviews/2. Without Emojis/ValidReviews.txt\"\n",
    "invalidPath = \"/home/ibon/Documentos/GitHub/TFG/1. Data/4. Labeled Reviews/2. Without Emojis/InvalidReviews.txt\"\n",
    "\n",
    "validOriginal = importFromTxtToList(validPath)\n",
    "invalidOriginal = importFromTxtToList(invalidPath) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738bf88b-1fc7-4cad-a2ff-9464486bf40d",
   "metadata": {},
   "source": [
    "Se importan las frases traducidas de: Castellano -> Inglés -> Castellano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fa962044-f7c9-44e0-b81e-a3aed6b90550",
   "metadata": {},
   "outputs": [],
   "source": [
    "validPath = '1. Back Translation/1. Google Translator/ValidReviewsTranslationsEsEnEnEs.txt'\n",
    "invalidPath = '1. Back Translation/1. Google Translator/InvalidReviewsTranslationsEsEnEnEs.txt'\n",
    "\n",
    "validEsEnEnEsTraductionList = importFromTxtToList(validPath)\n",
    "invalidEsEnEnEsTraductionList = importFromTxtToList(invalidPath) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8aba3e7-7b3e-4edd-b6fa-6614cdb5b1f0",
   "metadata": {},
   "source": [
    "Castellano -> Japonés -> Castellano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b4a68b0-2a8a-4037-946e-e9571106c21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "validPath = '1. Back Translation/1. Google Translator/ValidReviewsTranslationsEsJaJaEs.txt'\n",
    "invalidPath = '1. Back Translation/1. Google Translator/InvalidReviewsTranslationsEsJaJaEs.txt'\n",
    "\n",
    "validEsJaJaEsTraductionList = importFromTxtToList(validPath)\n",
    "invalidEsJaJaEsTraductionList = importFromTxtToList(invalidPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e926d10c-9623-4b7d-bf42-6194e822d03b",
   "metadata": {},
   "source": [
    "Castellano -> Francés -> Japonés -> Ruso -> Castellano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6f757675-d52d-462c-8a24-7711a338fb2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "validPath = '1. Back Translation/1. Google Translator/ValidReviewsTranslationsEsFrFrJaJaRuRuEs.txt'\n",
    "invalidPath = '1. Back Translation/1. Google Translator/InvalidReviewsTranslationsEsFrFrJaJaRuRuEs.txt'\n",
    "\n",
    "validEsFrFrJaJaRuRuEsList = importFromTxtToList(validPath)\n",
    "invalidEsFrFrJaJaRuRuEsList = importFromTxtToList(invalidPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f49a67-e058-4a6a-b5d0-9c88c47f3052",
   "metadata": {},
   "source": [
    "Para cada frase del conjunto de datos original (las frases con las reseñas válidas e inválidas), se va a calcular la similitud semántica (usando a un modelo basado en SBERT, conocido como MiniLM, que es más ligero y rápido consiguiendo resultados bastante certeros) y léxica con sus tres correspondientes frases generadas mediante el métodod de retrotraducción. Se van a seleccionar las frases que tengan mayor similitud semántica y menor similitud léxica y se van a guardar en un fichero para su posterior uso.\n",
    "\n",
    "Se van a generar dos ficheros: un csv con dos elementos por fila (la frase original y la retrotraducción escogida) y el otro con solo las retrotraducciones seleccionadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "28a0918b-a1c2-4f87-8628-dc0db6ffe124",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "#originalDataList: list of texts representing the original dataset\n",
    "#allAugmentedDataList: list of list of texts representing the aumented data \n",
    "#(allAugmentedDataList = [augmentedDataList1, ... ,augmentedDataListN], where augmentedDataList = [augmentedData1, ..., augmentedDataM])\n",
    "#pathWithOriginal: path of the csv with two columns (the original text and the best augmented text)\n",
    "#pathAugmentedData: path of the file with only the augmented data (without the original text)\n",
    "def processAugmentation(originalDataList, allAugmentedDataList, pathWithOriginal, pathAugmentedData):\n",
    "    #Select the model for the semantic similarity\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "    #Open the files in which the augmented data will be strored\n",
    "    withOriginalFile = open(pathWithOriginal, \"w\", encoding=\"utf-8\")\n",
    "    augmentedDataFile = open(pathAugmentedData, \"w\", encoding=\"utf-8\")\n",
    "\n",
    "    #Write the titles of the csv\n",
    "    withOriginalFile.write(\"OriginalText,AugmentedText,SemanticSimilarity,LexicalSimilarity\\n\")\n",
    "    \n",
    "    resul = []\n",
    "    #Analize every phrase in the original data\n",
    "    for i, originalText in enumerate(originalDataList):\n",
    "        allAugmentedDataInfoDict = {}\n",
    "        bestIdx = 1\n",
    "        \n",
    "        #Analize every traduction\n",
    "        for j, augmentedDataList in enumerate(allAugmentedDataList):\n",
    "            #Compute the similarities of the corresponding traduction\n",
    "            semanticSimilarity = getSemanticSimilarity(originalText, augmentedDataList[i], model)\n",
    "            lexicalSimilarity = getLexicalSimilarity(originalText, augmentedDataList[i])\n",
    "\n",
    "            #Save the traduction and the similarities in a dictionary\n",
    "            allAugmentedDataInfoDict.update({\n",
    "                f\"augmented{j + 1}\": augmentedDataList[i],\n",
    "                f\"semanticSimilarity{j + 1}\": semanticSimilarity,\n",
    "                f\"lexicalSimilarity{j + 1}\": lexicalSimilarity\n",
    "            })\n",
    "\n",
    "            #Get the index of the traduction with greater semantic similarity and less lexical similarity\n",
    "            bestIdx = max(bestIdx, j + 1,\n",
    "                key = lambda k: (allAugmentedDataInfoDict[f\"semanticSimilarity{k}\"] - allAugmentedDataInfoDict[f\"lexicalSimilarity{k}\"])\n",
    "            )\n",
    "\n",
    "        #Select the information of the best augmentation\n",
    "        info = {\n",
    "            \"originalText\": originalText,\n",
    "            \"bestAugmentation\": allAugmentedDataInfoDict[f\"augmented{bestIdx}\"],\n",
    "            \"bestAugmentedDataSemanticSimilarity\": allAugmentedDataInfoDict[f\"semanticSimilarity{bestIdx}\"],\n",
    "            \"bestAugmentedDataLexicalSimiliratity\": allAugmentedDataInfoDict[f\"lexicalSimilarity{bestIdx}\"]\n",
    "        }\n",
    "        info.update(allAugmentedDataInfoDict)\n",
    "\n",
    "        #Save the information\n",
    "        resul.append(info)\n",
    "\n",
    "        #Write the information in the files\n",
    "        withOriginalFile.write(originalText + \",\" + allAugmentedDataInfoDict[f\"augmented{bestIdx}\"] + \",\" + str(allAugmentedDataInfoDict[f\"semanticSimilarity{bestIdx}\"]) + \",\" + str(allAugmentedDataInfoDict[f\"lexicalSimilarity{bestIdx}\"]) + \"\\n\")\n",
    "        #If the text is not empty write it on  the file\n",
    "        if allAugmentedDataInfoDict[f\"augmented{bestIdx}\"]  != '\"\"':\n",
    "            augmentedDataFile.write(allAugmentedDataInfoDict[f\"augmented{bestIdx}\"] + \"\\n\")\n",
    "\n",
    "    #Close the files\n",
    "    withOriginalFile.close()\n",
    "    augmentedDataFile.close()\n",
    "    \n",
    "    return resul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b5dc006b-52fc-45e1-9c46-af3329c88d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "validWithOriginalPath = '1. Back Translation/3. Augmented Data/ValidBackTranslationWithOriginal.csv'\n",
    "validAugmentedPath = '1. Back Translation/3. Augmented Data/ValidBackTranslationData.txt'\n",
    "invalidWithOriginalPath = '1. Back Translation/3. Augmented Data/InvalidBackTranslationWithOriginal.csv'\n",
    "invalidAugmentedPath = '1. Back Translation/3. Augmented Data/InvalidBackTranslationData.txt'\n",
    "\n",
    "infoValid = processAugmentation(validOriginal, [validEsEnEnEsTraductionList, validEsJaJaEsTraductionList, validEsFrFrJaJaRuRuEsList], validWithOriginalPath, validAugmentedPath)\n",
    "infoValidDF = pd.DataFrame(infoValid)\n",
    "infoInvalid = processAugmentation(invalidOriginal, [invalidEsEnEnEsTraductionList, invalidEsJaJaEsTraductionList, invalidEsFrFrJaJaRuRuEsList], invalidWithOriginalPath, invalidAugmentedPath)\n",
    "infoInvalidDF = pd.DataFrame(infoInvalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "72d7a1d5-4171-4400-adf0-7f0e407f8ac7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>originalText</th>\n",
       "      <th>bestAugmentation</th>\n",
       "      <th>bestAugmentedDataSemanticSimilarity</th>\n",
       "      <th>bestAugmentedDataLexicalSimiliratity</th>\n",
       "      <th>augmented1</th>\n",
       "      <th>semanticSimilarity1</th>\n",
       "      <th>lexicalSimilarity1</th>\n",
       "      <th>augmented2</th>\n",
       "      <th>semanticSimilarity2</th>\n",
       "      <th>lexicalSimilarity2</th>\n",
       "      <th>augmented3</th>\n",
       "      <th>semanticSimilarity3</th>\n",
       "      <th>lexicalSimilarity3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"Tiene fácil acceso para las personas con movi...</td>\n",
       "      <td>“Es de fácil acceso para personas con discapac...</td>\n",
       "      <td>0.854630</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>“Tiene fácil acceso para personas con movilida...</td>\n",
       "      <td>0.978443</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>“Es de fácil acceso para personas con movilida...</td>\n",
       "      <td>0.925599</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>“Es de fácil acceso para personas con discapac...</td>\n",
       "      <td>0.854630</td>\n",
       "      <td>0.450000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"Espero que hayan mejorais\"</td>\n",
       "      <td>\"Espero que hayas mejorado\"</td>\n",
       "      <td>0.916635</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>\"Espero que hayas mejorado\"</td>\n",
       "      <td>0.916635</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>\"Espero que las cosas estén mejorando\".</td>\n",
       "      <td>0.785516</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>\"Espero que la situación esté mejorando\".</td>\n",
       "      <td>0.736162</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"La estación es antigua, aparte de tener una s...</td>\n",
       "      <td>“Además de que esta estación es antigua y tien...</td>\n",
       "      <td>0.932238</td>\n",
       "      <td>0.393939</td>\n",
       "      <td>“La estación es antigua, además de tener una ú...</td>\n",
       "      <td>0.883212</td>\n",
       "      <td>0.592593</td>\n",
       "      <td>“La estación es antigua e inaccesible para per...</td>\n",
       "      <td>0.932935</td>\n",
       "      <td>0.419355</td>\n",
       "      <td>“Además de que esta estación es antigua y tien...</td>\n",
       "      <td>0.932238</td>\n",
       "      <td>0.393939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"Bien\"</td>\n",
       "      <td>\"Bien\"</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>\"Bien\"</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>\"bien\"</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>\"bien\"</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"Bonito comodo\"</td>\n",
       "      <td>\"Maravilloso confort\"</td>\n",
       "      <td>0.415461</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>\"Bonito y cómodo\"</td>\n",
       "      <td>0.955331</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>\"Maravilloso confort\"</td>\n",
       "      <td>0.415461</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>“Muy conveniente”</td>\n",
       "      <td>0.229466</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        originalText  \\\n",
       "0  \"Tiene fácil acceso para las personas con movi...   \n",
       "1                        \"Espero que hayan mejorais\"   \n",
       "2  \"La estación es antigua, aparte de tener una s...   \n",
       "3                                             \"Bien\"   \n",
       "4                                    \"Bonito comodo\"   \n",
       "\n",
       "                                    bestAugmentation  \\\n",
       "0  “Es de fácil acceso para personas con discapac...   \n",
       "1                        \"Espero que hayas mejorado\"   \n",
       "2  “Además de que esta estación es antigua y tien...   \n",
       "3                                             \"Bien\"   \n",
       "4                              \"Maravilloso confort\"   \n",
       "\n",
       "   bestAugmentedDataSemanticSimilarity  bestAugmentedDataLexicalSimiliratity  \\\n",
       "0                             0.854630                              0.450000   \n",
       "1                             0.916635                              0.200000   \n",
       "2                             0.932238                              0.393939   \n",
       "3                             1.000000                              1.000000   \n",
       "4                             0.415461                              0.000000   \n",
       "\n",
       "                                          augmented1  semanticSimilarity1  \\\n",
       "0  “Tiene fácil acceso para personas con movilida...             0.978443   \n",
       "1                        \"Espero que hayas mejorado\"             0.916635   \n",
       "2  “La estación es antigua, además de tener una ú...             0.883212   \n",
       "3                                             \"Bien\"             1.000000   \n",
       "4                                  \"Bonito y cómodo\"             0.955331   \n",
       "\n",
       "   lexicalSimilarity1                                         augmented2  \\\n",
       "0            0.812500  “Es de fácil acceso para personas con movilida...   \n",
       "1            0.200000            \"Espero que las cosas estén mejorando\".   \n",
       "2            0.592593  “La estación es antigua e inaccesible para per...   \n",
       "3            1.000000                                             \"bien\"   \n",
       "4            1.000000                              \"Maravilloso confort\"   \n",
       "\n",
       "   semanticSimilarity2  lexicalSimilarity2  \\\n",
       "0             0.925599            0.611111   \n",
       "1             0.785516            0.166667   \n",
       "2             0.932935            0.419355   \n",
       "3             1.000000            1.000000   \n",
       "4             0.415461            0.000000   \n",
       "\n",
       "                                          augmented3  semanticSimilarity3  \\\n",
       "0  “Es de fácil acceso para personas con discapac...             0.854630   \n",
       "1          \"Espero que la situación esté mejorando\".             0.736162   \n",
       "2  “Además de que esta estación es antigua y tien...             0.932238   \n",
       "3                                             \"bien\"             1.000000   \n",
       "4                                  “Muy conveniente”             0.229466   \n",
       "\n",
       "   lexicalSimilarity3  \n",
       "0            0.450000  \n",
       "1            0.200000  \n",
       "2            0.393939  \n",
       "3            1.000000  \n",
       "4            0.000000  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infoValidDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "670d35a6-0253-4468-b6c4-d98e85abf51b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>originalText</th>\n",
       "      <th>bestAugmentation</th>\n",
       "      <th>bestAugmentedDataSemanticSimilarity</th>\n",
       "      <th>bestAugmentedDataLexicalSimiliratity</th>\n",
       "      <th>augmented1</th>\n",
       "      <th>semanticSimilarity1</th>\n",
       "      <th>lexicalSimilarity1</th>\n",
       "      <th>augmented2</th>\n",
       "      <th>semanticSimilarity2</th>\n",
       "      <th>lexicalSimilarity2</th>\n",
       "      <th>augmented3</th>\n",
       "      <th>semanticSimilarity3</th>\n",
       "      <th>lexicalSimilarity3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"He vivido 35 años en el barrio y reconozco qu...</td>\n",
       "      <td>“Vivo en esta zona desde hace 35 años y recono...</td>\n",
       "      <td>0.695063</td>\n",
       "      <td>0.322581</td>\n",
       "      <td>“Llevo 35 años viviendo en el barrio y reconoz...</td>\n",
       "      <td>0.949282</td>\n",
       "      <td>0.629630</td>\n",
       "      <td>\"He vivido en esta zona durante 35 años y reco...</td>\n",
       "      <td>0.818717</td>\n",
       "      <td>0.448276</td>\n",
       "      <td>“Vivo en esta zona desde hace 35 años y recono...</td>\n",
       "      <td>0.695063</td>\n",
       "      <td>0.322581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"localización con muchos bares interesantes\"</td>\n",
       "      <td>\"Ubicación con muchos bares interesantes\"</td>\n",
       "      <td>0.793883</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>\"Ubicación con muchos bares interesantes\"</td>\n",
       "      <td>0.793883</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>\"Ubicación con muchos bares interesantes\"</td>\n",
       "      <td>0.793883</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>“Un lugar con muchos bares interesantes”</td>\n",
       "      <td>0.685196</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"…\"</td>\n",
       "      <td>\"\"</td>\n",
       "      <td>0.905728</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>\"\"</td>\n",
       "      <td>0.905728</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>\"...\"</td>\n",
       "      <td>0.838349</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>\"\"</td>\n",
       "      <td>0.905728</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"Muy rica comida..\"</td>\n",
       "      <td>\"Es una comida muy deliciosa\".</td>\n",
       "      <td>0.644577</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>\"Comida muy deliciosa..\"</td>\n",
       "      <td>0.677902</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>\"Es una comida muy deliciosa\".</td>\n",
       "      <td>0.644577</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>“Comida muy sabrosa.”</td>\n",
       "      <td>0.706546</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"Estación del.metro\"</td>\n",
       "      <td>\"Estación de metro\"</td>\n",
       "      <td>0.963894</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>\"Estación de metro\"</td>\n",
       "      <td>0.963894</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>\"estación de metro\"</td>\n",
       "      <td>0.963894</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>\"estación de metro\"</td>\n",
       "      <td>0.963894</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        originalText  \\\n",
       "0  \"He vivido 35 años en el barrio y reconozco qu...   \n",
       "1       \"localización con muchos bares interesantes\"   \n",
       "2                                                \"…\"   \n",
       "3                                \"Muy rica comida..\"   \n",
       "4                               \"Estación del.metro\"   \n",
       "\n",
       "                                    bestAugmentation  \\\n",
       "0  “Vivo en esta zona desde hace 35 años y recono...   \n",
       "1          \"Ubicación con muchos bares interesantes\"   \n",
       "2                                                 \"\"   \n",
       "3                     \"Es una comida muy deliciosa\".   \n",
       "4                                \"Estación de metro\"   \n",
       "\n",
       "   bestAugmentedDataSemanticSimilarity  bestAugmentedDataLexicalSimiliratity  \\\n",
       "0                             0.695063                              0.322581   \n",
       "1                             0.793883                              0.500000   \n",
       "2                             0.905728                              0.000000   \n",
       "3                             0.644577                              0.400000   \n",
       "4                             0.963894                              0.333333   \n",
       "\n",
       "                                          augmented1  semanticSimilarity1  \\\n",
       "0  “Llevo 35 años viviendo en el barrio y reconoz...             0.949282   \n",
       "1          \"Ubicación con muchos bares interesantes\"             0.793883   \n",
       "2                                                 \"\"             0.905728   \n",
       "3                           \"Comida muy deliciosa..\"             0.677902   \n",
       "4                                \"Estación de metro\"             0.963894   \n",
       "\n",
       "   lexicalSimilarity1                                         augmented2  \\\n",
       "0            0.629630  \"He vivido en esta zona durante 35 años y reco...   \n",
       "1            0.500000          \"Ubicación con muchos bares interesantes\"   \n",
       "2            0.000000                                              \"...\"   \n",
       "3            0.500000                     \"Es una comida muy deliciosa\".   \n",
       "4            0.333333                                \"estación de metro\"   \n",
       "\n",
       "   semanticSimilarity2  lexicalSimilarity2  \\\n",
       "0             0.818717            0.448276   \n",
       "1             0.793883            0.500000   \n",
       "2             0.838349            0.000000   \n",
       "3             0.644577            0.400000   \n",
       "4             0.963894            0.333333   \n",
       "\n",
       "                                          augmented3  semanticSimilarity3  \\\n",
       "0  “Vivo en esta zona desde hace 35 años y recono...             0.695063   \n",
       "1           “Un lugar con muchos bares interesantes”             0.685196   \n",
       "2                                                 \"\"             0.905728   \n",
       "3                              “Comida muy sabrosa.”             0.706546   \n",
       "4                                \"estación de metro\"             0.963894   \n",
       "\n",
       "   lexicalSimilarity3  \n",
       "0            0.322581  \n",
       "1            0.500000  \n",
       "2            0.000000  \n",
       "3            0.500000  \n",
       "4            0.333333  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infoInvalidDF.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf741edf-ec0d-4341-8de9-496e46fabf6c",
   "metadata": {},
   "source": [
    "Como se puede ver, el método de retrotraducción genera frases con una similitud semántica muy parecida pero con gran variavilidad en la similitud léxica. Por lo tanto, se consiguen frases con distinto vocabulario pero mismo significado."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd28f2b-34c8-4672-ba82-8bf50710dd15",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Reemplazo por sinónimos\n",
    "\n",
    "Este método consiste en elejir aleatoriamente n palabras del texto que no sean palabras vacías, y reemplazar cada una de estas palabras por uno de sus sinónimos elegido al azar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39aa6c88-9d18-48d6-a8e6-dec71fdcc744",
   "metadata": {},
   "source": [
    "Esta función, dada una palabra devuelve, si existe, un sinónimo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8b349e9a-15e2-4691-b13c-29e48f62a3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Swaps the word given by its synonym\n",
    "def swapSynonym(word):\n",
    "    #gets all synonyms from the word given\n",
    "    synset = wordnet.synsets(word, lang='spa')\n",
    "    if synset:\n",
    "        #if the word has one or more synonym we swap it\n",
    "        synset = wordnet.synsets(word, lang='spa')[0]\n",
    "        synonymsList = synset.lemma_names('spa') \n",
    "        cleanList = [synonym.replace('_', ' ').strip() for synonym in synonymsList]\n",
    "        #filter to make sure its a diferent word\n",
    "        differentList = [s for s in cleanList if s.lower() != word.lower()]\n",
    "        #choose a random synonym if the word has one\n",
    "        if differentList:\n",
    "            chosen = random.choice(differentList)\n",
    "            return chosen\n",
    "        else:\n",
    "            return word\n",
    "    else:\n",
    "        return word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d33496-f7e1-466b-ad8b-4477a9c095e0",
   "metadata": {},
   "source": [
    "Dado un texto, cambia con un prob% de probabilidad las palabras no vacías por un sinónimo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "156c4503-cac2-4543-ad6d-6ab8ae514d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def swapBySynonymLine(line, prob):\n",
    "    # Split the line into individual words\n",
    "    words = line.split();\n",
    "    newWords = []\n",
    "\n",
    "    #Get the spanish stop words\n",
    "    spanishStopWords = getSpanishStopWords()\n",
    "\n",
    "    #Analyze all the words in the given text\n",
    "    for word in words:\n",
    "        # Check if the word is not a stop word\n",
    "        if word not in spanishStopWords: \n",
    "            # With prob probability, replace the word with a synonym\n",
    "            if random.random() <= prob:\n",
    "                newWord = swapSynonym(word)\n",
    "            else: \n",
    "                newWord = word\n",
    "            newWords.append(newWord)\n",
    "        else:\n",
    "            newWords.append(word)\n",
    "    # Join the words back into a single line and return it\n",
    "    return ' '.join(newWords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f640ff-e8e3-4ad0-a50e-449195994063",
   "metadata": {},
   "source": [
    "Dado una lista de textos y una ruta, aplica el métododo de sustitución por sinónimos a todos los elementos de la lista y los alamacena en la ruta proporcionada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b5bbee92-a4ab-4fb8-af1b-3af832c66b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "#synonym replacement method\n",
    "def synonymReplacement(textList, prob, targetPath):\n",
    "    #Open the file\n",
    "    targetFile = open(targetPath, \"w\", encoding = \"utf-8\")\n",
    "    \n",
    "    newList = []\n",
    "    for line in textList:\n",
    "        newLine = swapBySynonymLine(line, prob)\n",
    "        newList.append(newLine)\n",
    "        targetFile.write(newLine + \"\\n\")\n",
    "\n",
    "    #Close the file\n",
    "    targetFile.close()\n",
    "    \n",
    "    return newList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "beba2eee-ee0e-4f8b-833c-5b31569e1b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def importFromTxtToList(source):\n",
    "    with open(source, 'r', encoding=\"utf-8\") as file:\n",
    "        #Generate a list with all the reviews\n",
    "        targetList = [line.strip() for line in file]\n",
    "    return targetList"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe593df-ec69-4a56-90d2-51e350e55a57",
   "metadata": {},
   "source": [
    "Se va a aplicar el método de reemplazo por sinónimos tres veces para generar tres conjuntos de datos diferentes. Además, la probabilidad de sustitución por sinónimo va a aumentar en cada conjunto de datos: inicialmente 0.25, después 0.45 y finalmente 0.65."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "34c8a63c-fd48-4704-a922-a466d6c7dda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the original data\n",
    "validOriginalPath = \"/home/ibon/Documentos/GitHub/TFG/1. Data/4. Labeled Reviews/2. Without Emojis/ValidReviews.txt\"\n",
    "invalidOriginalPath = \"/home/ibon/Documentos/GitHub/TFG/1. Data/4. Labeled Reviews/2. Without Emojis/InvalidReviews.txt\"\n",
    "\n",
    "validOriginal = importFromTxtToList(validOriginalPath)\n",
    "invalidOriginal = importFromTxtToList(invalidOriginalPath) \n",
    "\n",
    "#Apply the synonym replacement 3 times to both datasets\n",
    "for i in range(3):\n",
    "    validPath = f\"3. Synonym Replacement/1. All Augmented Data/validSynonymsReviews{i + 1}.txt\"\n",
    "    invalidPath = f\"3. Synonym Replacement/1. All Augmented Data/invalidSynonymsReviews{i + 1}.txt\"\n",
    "\n",
    "    prob = 0.25 + 2 * i / 10\n",
    "    \n",
    "    synonymReplacement(validOriginal, prob, validPath)\n",
    "    synonymReplacement(invalidOriginal, prob, invalidPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44359ec6-da25-440e-9987-6bdfe37bb705",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Análisis del reemplazo por sinónimos y selección de los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3df32aa-0194-46c1-a2fa-92aee28a8597",
   "metadata": {},
   "source": [
    "Importar los datos originales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bb253981-d4c4-40e2-b1c1-8ad911c8c28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "validOriginalPath = \"/home/ibon/Documentos/GitHub/TFG/1. Data/4. Labeled Reviews/2. Without Emojis/ValidReviews.txt\"\n",
    "invalidOriginalPath = \"/home/ibon/Documentos/GitHub/TFG/1. Data/4. Labeled Reviews/2. Without Emojis/InvalidReviews.txt\"\n",
    "\n",
    "validOriginal = importFromTxtToList(validOriginalPath)\n",
    "invalidOriginal = importFromTxtToList(invalidOriginalPath) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d6e36c-9306-4eff-87fd-bc1c4f0f9610",
   "metadata": {},
   "source": [
    "Importar los tres datasets generados en el reemplazo por sinónimos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3bb530ae-286a-4d8b-b8aa-2fd2543997e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import all the data \n",
    "validSynonymReplacementList = []\n",
    "invalidSynonymReplacementList = []\n",
    "for i in range(3):\n",
    "    validPath = f\"3. Synonym Replacement/1. All Augmented Data/validSynonymsReviews{i + 1}.txt\"\n",
    "    invalidPath = f\"3. Synonym Replacement/1. All Augmented Data/invalidSynonymsReviews{i + 1}.txt\"\n",
    "\n",
    "    validSynonymReplacementList.append(importFromTxtToList(validPath))\n",
    "    invalidSynonymReplacementList.append(importFromTxtToList(invalidPath))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b50528d-a507-4b59-9484-c8edfc4cbd4b",
   "metadata": {},
   "source": [
    "Realizar el análisis y la selección de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8bb43223-e110-41c0-ab34-de40ca92a107",
   "metadata": {},
   "outputs": [],
   "source": [
    "validWithOriginalPath = '3. Synonym Replacement/2. Augmented Data/ ValidSynonymReplacementWithOriginal.csv'\n",
    "validAugmentedPath = '3. Synonym Replacement/2. Augmented Data/ValidSynonymReplacementData.txt'\n",
    "invalidWithOriginalPath = '3. Synonym Replacement/2. Augmented Data/InvalidSynonymReplacementWithOriginal.csv'\n",
    "invalidAugmentedPath = '3. Synonym Replacement/2. Augmented Data/InvalidSynonymReplacementData.txt'\n",
    "\n",
    "infoValid = processAugmentation(validOriginal, validSynonymReplacementList, validWithOriginalPath, validAugmentedPath)\n",
    "infoValidDF = pd.DataFrame(infoValid)\n",
    "infoInvalid = processAugmentation(invalidOriginal, invalidSynonymReplacementList, invalidWithOriginalPath, invalidAugmentedPath)\n",
    "infoInvalidDF = pd.DataFrame(infoInvalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "29c678ed-7527-4a2c-9cd3-77300176f6f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>originalText</th>\n",
       "      <th>bestAugmentation</th>\n",
       "      <th>bestAugmentedDataSemanticSimilarity</th>\n",
       "      <th>bestAugmentedDataLexicalSimiliratity</th>\n",
       "      <th>augmented1</th>\n",
       "      <th>semanticSimilarity1</th>\n",
       "      <th>lexicalSimilarity1</th>\n",
       "      <th>augmented2</th>\n",
       "      <th>semanticSimilarity2</th>\n",
       "      <th>lexicalSimilarity2</th>\n",
       "      <th>augmented3</th>\n",
       "      <th>semanticSimilarity3</th>\n",
       "      <th>lexicalSimilarity3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"Tiene fácil acceso para las personas con movi...</td>\n",
       "      <td>\"Tiene fácil entrada para las personas con mov...</td>\n",
       "      <td>0.989551</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>\"Tiene fácil acceso para las personas con movi...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>\"Tiene fácil entrada para las personas con mov...</td>\n",
       "      <td>0.989551</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>\"Tiene fácil acceso para las personas con movi...</td>\n",
       "      <td>0.988377</td>\n",
       "      <td>0.875000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"Espero que hayan mejorais\"</td>\n",
       "      <td>\"Espero que hayan mejorais\"</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>\"Espero que hayan mejorais\"</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>\"Espero que hayan mejorais\"</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>\"Espero que hayan mejorais\"</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"La estación es antigua, aparte de tener una s...</td>\n",
       "      <td>\"La estación es antigua, a un lado de parir un...</td>\n",
       "      <td>0.805127</td>\n",
       "      <td>0.516129</td>\n",
       "      <td>\"La estación es antigua, aparte de tener una s...</td>\n",
       "      <td>0.949019</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>\"La estación es antigua, a un lado de tener un...</td>\n",
       "      <td>0.952585</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>\"La estación es antigua, a un lado de parir un...</td>\n",
       "      <td>0.805127</td>\n",
       "      <td>0.516129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"Bien\"</td>\n",
       "      <td>\"Bien\"</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>\"Bien\"</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>\"Bien\"</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>\"Bien\"</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"Bonito comodo\"</td>\n",
       "      <td>\"Bonito comodo\"</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>\"Bonito comodo\"</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>\"Bonito comodo\"</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>\"Bonito comodo\"</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        originalText  \\\n",
       "0  \"Tiene fácil acceso para las personas con movi...   \n",
       "1                        \"Espero que hayan mejorais\"   \n",
       "2  \"La estación es antigua, aparte de tener una s...   \n",
       "3                                             \"Bien\"   \n",
       "4                                    \"Bonito comodo\"   \n",
       "\n",
       "                                    bestAugmentation  \\\n",
       "0  \"Tiene fácil entrada para las personas con mov...   \n",
       "1                        \"Espero que hayan mejorais\"   \n",
       "2  \"La estación es antigua, a un lado de parir un...   \n",
       "3                                             \"Bien\"   \n",
       "4                                    \"Bonito comodo\"   \n",
       "\n",
       "   bestAugmentedDataSemanticSimilarity  bestAugmentedDataLexicalSimiliratity  \\\n",
       "0                             0.989551                              0.875000   \n",
       "1                             1.000000                              1.000000   \n",
       "2                             0.805127                              0.516129   \n",
       "3                             1.000000                              1.000000   \n",
       "4                             1.000000                              1.000000   \n",
       "\n",
       "                                          augmented1  semanticSimilarity1  \\\n",
       "0  \"Tiene fácil acceso para las personas con movi...             1.000000   \n",
       "1                        \"Espero que hayan mejorais\"             1.000000   \n",
       "2  \"La estación es antigua, aparte de tener una s...             0.949019   \n",
       "3                                             \"Bien\"             1.000000   \n",
       "4                                    \"Bonito comodo\"             1.000000   \n",
       "\n",
       "   lexicalSimilarity1                                         augmented2  \\\n",
       "0            1.000000  \"Tiene fácil entrada para las personas con mov...   \n",
       "1            1.000000                        \"Espero que hayan mejorais\"   \n",
       "2            0.833333  \"La estación es antigua, a un lado de tener un...   \n",
       "3            1.000000                                             \"Bien\"   \n",
       "4            1.000000                                    \"Bonito comodo\"   \n",
       "\n",
       "   semanticSimilarity2  lexicalSimilarity2  \\\n",
       "0             0.989551            0.875000   \n",
       "1             1.000000            1.000000   \n",
       "2             0.952585            0.692308   \n",
       "3             1.000000            1.000000   \n",
       "4             1.000000            1.000000   \n",
       "\n",
       "                                          augmented3  semanticSimilarity3  \\\n",
       "0  \"Tiene fácil acceso para las personas con movi...             0.988377   \n",
       "1                        \"Espero que hayan mejorais\"             1.000000   \n",
       "2  \"La estación es antigua, a un lado de parir un...             0.805127   \n",
       "3                                             \"Bien\"             1.000000   \n",
       "4                                    \"Bonito comodo\"             1.000000   \n",
       "\n",
       "   lexicalSimilarity3  \n",
       "0            0.875000  \n",
       "1            1.000000  \n",
       "2            0.516129  \n",
       "3            1.000000  \n",
       "4            1.000000  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infoValidDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e3b5eb25-2f87-472f-b189-b59d3a5afcc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>originalText</th>\n",
       "      <th>bestAugmentation</th>\n",
       "      <th>bestAugmentedDataSemanticSimilarity</th>\n",
       "      <th>bestAugmentedDataLexicalSimiliratity</th>\n",
       "      <th>augmented1</th>\n",
       "      <th>semanticSimilarity1</th>\n",
       "      <th>lexicalSimilarity1</th>\n",
       "      <th>augmented2</th>\n",
       "      <th>semanticSimilarity2</th>\n",
       "      <th>lexicalSimilarity2</th>\n",
       "      <th>augmented3</th>\n",
       "      <th>semanticSimilarity3</th>\n",
       "      <th>lexicalSimilarity3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"He vivido 35 años en el barrio y reconozco qu...</td>\n",
       "      <td>\"He vivido 35 largo tiempo en el barrio y reco...</td>\n",
       "      <td>0.920601</td>\n",
       "      <td>0.703704</td>\n",
       "      <td>\"He vivido 35 largo tiempo en el barrio y reco...</td>\n",
       "      <td>0.926218</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>\"He vivido 35 largo tiempo en el barrio y reco...</td>\n",
       "      <td>0.920601</td>\n",
       "      <td>0.703704</td>\n",
       "      <td>\"He vivido 35 mucho tiempo en el barrio y reco...</td>\n",
       "      <td>0.986137</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"localización con muchos bares interesantes\"</td>\n",
       "      <td>\"localización con muchos bares interesantes\"</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>\"localización con muchos bares interesantes\"</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>\"localización con muchos bares interesantes\"</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>\"localización con muchos bares interesantes\"</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"…\"</td>\n",
       "      <td>\"…\"</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>\"…\"</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>\"…\"</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>\"…\"</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"Muy rica comida..\"</td>\n",
       "      <td>\"Muy rica comida..\"</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>\"Muy rica comida..\"</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>\"Muy rica comida..\"</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>\"Muy rica comida..\"</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"Estación del.metro\"</td>\n",
       "      <td>\"Estación del.metro\"</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>\"Estación del.metro\"</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>\"Estación del.metro\"</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>\"Estación del.metro\"</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        originalText  \\\n",
       "0  \"He vivido 35 años en el barrio y reconozco qu...   \n",
       "1       \"localización con muchos bares interesantes\"   \n",
       "2                                                \"…\"   \n",
       "3                                \"Muy rica comida..\"   \n",
       "4                               \"Estación del.metro\"   \n",
       "\n",
       "                                    bestAugmentation  \\\n",
       "0  \"He vivido 35 largo tiempo en el barrio y reco...   \n",
       "1       \"localización con muchos bares interesantes\"   \n",
       "2                                                \"…\"   \n",
       "3                                \"Muy rica comida..\"   \n",
       "4                               \"Estación del.metro\"   \n",
       "\n",
       "   bestAugmentedDataSemanticSimilarity  bestAugmentedDataLexicalSimiliratity  \\\n",
       "0                             0.920601                              0.703704   \n",
       "1                             1.000000                              1.000000   \n",
       "2                             1.000000                              0.000000   \n",
       "3                             1.000000                              1.000000   \n",
       "4                             1.000000                              1.000000   \n",
       "\n",
       "                                          augmented1  semanticSimilarity1  \\\n",
       "0  \"He vivido 35 largo tiempo en el barrio y reco...             0.926218   \n",
       "1       \"localización con muchos bares interesantes\"             1.000000   \n",
       "2                                                \"…\"             1.000000   \n",
       "3                                \"Muy rica comida..\"             1.000000   \n",
       "4                               \"Estación del.metro\"             1.000000   \n",
       "\n",
       "   lexicalSimilarity1                                         augmented2  \\\n",
       "0            0.769231  \"He vivido 35 largo tiempo en el barrio y reco...   \n",
       "1            1.000000       \"localización con muchos bares interesantes\"   \n",
       "2            0.000000                                                \"…\"   \n",
       "3            1.000000                                \"Muy rica comida..\"   \n",
       "4            1.000000                               \"Estación del.metro\"   \n",
       "\n",
       "   semanticSimilarity2  lexicalSimilarity2  \\\n",
       "0             0.920601            0.703704   \n",
       "1             1.000000            1.000000   \n",
       "2             1.000000            0.000000   \n",
       "3             1.000000            1.000000   \n",
       "4             1.000000            1.000000   \n",
       "\n",
       "                                          augmented3  semanticSimilarity3  \\\n",
       "0  \"He vivido 35 mucho tiempo en el barrio y reco...             0.986137   \n",
       "1       \"localización con muchos bares interesantes\"             1.000000   \n",
       "2                                                \"…\"             1.000000   \n",
       "3                                \"Muy rica comida..\"             1.000000   \n",
       "4                               \"Estación del.metro\"             1.000000   \n",
       "\n",
       "   lexicalSimilarity3  \n",
       "0                 0.8  \n",
       "1                 1.0  \n",
       "2                 0.0  \n",
       "3                 1.0  \n",
       "4                 1.0  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infoInvalidDF.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c42cda7-c1da-491a-a27d-690cc265803a",
   "metadata": {},
   "source": [
    "A diferencia de la retrotraducción, este método no genera variabilidad léxica. No altera el significado de la frase, pero tampoco altera las palabras que la componen. Creemos que esto se debe a la reducida capacidad de la librelia nltk para palabras en castellano."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3141c50-b5e9-437e-9da8-73265ee193c8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Inserción aleatoria\n",
    "\n",
    "Este método consiste en encontrar un sinónimo aleatorio de una palabra aleatoria en la oración que no sea una palabra vacía e insertar ese sinónimo en una posición aleatoria de la oración."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af13b666-95df-4f5c-9d84-89ce123e19d0",
   "metadata": {},
   "source": [
    "Función para calcular el número de modificaciones (inserciones, eliminaciones, sustituciones ...) de un texto dado dependiendo de su longitud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aa2cc69e-928e-4743-8368-e22d1fbbbb05",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to calculate the number of insertions or replacements or deletions on a given text depending on its length\n",
    "def calculateModifications(text):\n",
    "    return max(1, int(len(text.split()) * 0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0b031f-1716-4045-a258-a798d30b3d0e",
   "metadata": {},
   "source": [
    "Función para añadir dobles comillas al inicio y al final del texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "10424487-e839-49d6-89a3-4e761b9304f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add quotes to the given text\n",
    "def addQuotes(text):\n",
    "    return f'\"{text}\"'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f5e60f-d3f9-4b44-85ed-fe5f9e080185",
   "metadata": {},
   "source": [
    "Función para eliminar las dobles comillas del inicio y el final de un texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "19a74aab-2583-4ac7-95b4-2f7a8b663680",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove quotes from the given text\n",
    "def removeQuotes(text):\n",
    "    if text.startswith('\"') and text.endswith('\"'):\n",
    "        return text[1:-1]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec271714-e2cf-4073-90aa-bb1f841c195c",
   "metadata": {},
   "source": [
    "Función que realiza la inserción aleatoria de un texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "db70cb0c-60f1-4c45-83f2-2e9e7003968c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function that executes the random insertion on a given text\n",
    "def wordInsertion(text):\n",
    "    #Split the text\n",
    "    wordsList = text.split()   \n",
    "    \n",
    "    # Clean the line of text and remove extra spaces or special characters\n",
    "    cleanTextStr = cleanText(text)\n",
    "    \n",
    "    # Remove Spanish stop words and split the result into words\n",
    "    withoutStopWordsList = revomeSpanishStopWords(cleanTextStr).split() \n",
    "    \n",
    "    # Proceed only if there are words left after removing stop words\n",
    "    if withoutStopWordsList:\n",
    "        # Determine the number of insertions based on line length\n",
    "        for i in range(calculateModifications(text)):\n",
    "            # Choose a random important word from the list without stop words\n",
    "            chosen = random.choice(withoutStopWordsList)    \n",
    "            # Get a synonym of the chosen word\n",
    "            synonym = swapSynonym(chosen) \n",
    "            # Choose a random position to insert the synonym\n",
    "            pos = random.randint(0, len(wordsList))    \n",
    "            # Insert the synonym at the chosen position, removing any extra spaces\n",
    "            wordsList.insert(pos, synonym.strip())   \n",
    "            \n",
    "    # Return the modified line with quotation marks around it\n",
    "    return ' '.join(wordsList)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e853d08-4ec1-4371-9ea3-039c39c1322d",
   "metadata": {},
   "source": [
    "Función que realiza la inserción aleatoria a todos los elementos de una lista de textos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "592f6148-d4b8-48ad-9d03-bf674df3bab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function that executes the random insertion to all the elements of a list of texts and strores it in a file\n",
    "def randomInsertion(textList, targetPath):\n",
    "    #Open the file\n",
    "    targetFile = open(targetPath, \"w\", encoding = \"utf-8\")\n",
    "    \n",
    "    newList = []\n",
    "    for text in textList:\n",
    "        newLine = wordInsertion(text)\n",
    "        newList.append(newLine)\n",
    "        targetFile.write(newLine + \"\\n\")\n",
    "\n",
    "    #Close the file\n",
    "    targetFile.close()\n",
    "    \n",
    "    return newList"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13bd606-ecef-4f6c-9295-424463837901",
   "metadata": {},
   "source": [
    "Se va a realizar este proceso tres veces para generar más textos distintos de los que se posteriormente se elegirán los mejores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dd806145-45f9-4b85-ab6f-4624b24c3520",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the original data\n",
    "validOriginalPath = \"/home/ibon/Documentos/GitHub/TFG/1. Data/4. Labeled Reviews/2. Without Emojis/ValidReviews.txt\"\n",
    "invalidOriginalPath = \"/home/ibon/Documentos/GitHub/TFG/1. Data/4. Labeled Reviews/2. Without Emojis/InvalidReviews.txt\"\n",
    "\n",
    "validOriginal = importFromTxtToList(validOriginalPath)\n",
    "invalidOriginal = importFromTxtToList(invalidOriginalPath) \n",
    "\n",
    "#Apply the synonym replacement 3 times to both datasets\n",
    "for i in range(3):\n",
    "    validPath = f\"4. Random Insertion/1. All Augmented Data/validRandomInsertionReviews{i + 1}.txt\"\n",
    "    invalidPath = f\"4. Random Insertion/1. All Augmented Data/invalidRandomInsertionReviews{i + 1}.txt\"\n",
    "\n",
    "    \n",
    "    randomInsertion(validOriginal, validPath)\n",
    "    randomInsertion(invalidOriginal, invalidPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a3e6fe-55c7-484d-99a1-9d606b5d74aa",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Análisis de la inserción aleatoria y selección de los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532723c8-779d-47b8-820e-5a1bdec24068",
   "metadata": {},
   "source": [
    "Importar los datos originales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "03c3ec6b-0764-4532-bc63-4b7c20f8a117",
   "metadata": {},
   "outputs": [],
   "source": [
    "validOriginalPath = \"/home/ibon/Documentos/GitHub/TFG/1. Data/4. Labeled Reviews/2. Without Emojis/ValidReviews.txt\"\n",
    "invalidOriginalPath = \"/home/ibon/Documentos/GitHub/TFG/1. Data/4. Labeled Reviews/2. Without Emojis/InvalidReviews.txt\"\n",
    "\n",
    "validOriginal = importFromTxtToList(validOriginalPath)\n",
    "invalidOriginal = importFromTxtToList(invalidOriginalPath) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5d1e9d-abe1-4a3c-aacf-b25e303f8570",
   "metadata": {},
   "source": [
    "Importar los datasets generados en la inserción aleatoria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1c0de326-d0d3-44b1-9b3d-1b2a7f63b5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import all the data \n",
    "validRandomInsertionList = []\n",
    "invalidRandomInsertionList = []\n",
    "for i in range(3):\n",
    "    validPath = f\"4. Random Insertion/1. All Augmented Data/validRandomInsertionReviews{i + 1}.txt\"\n",
    "    invalidPath = f\"4. Random Insertion/1. All Augmented Data/invalidRandomInsertionReviews{i + 1}.txt\"\n",
    "\n",
    "    validRandomInsertionList.append(importFromTxtToList(validPath))\n",
    "    invalidRandomInsertionList.append(importFromTxtToList(invalidPath))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad86aa79-5ba6-41f7-b726-48594edbcd6a",
   "metadata": {},
   "source": [
    "Realizar el análisis y la selección de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2a2d6285-e4a4-4f74-ac74-b7d56f2ad6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "validWithOriginalPath = '4. Random Insertion/2. Augmented Data/ ValidRandomInsertionWithOriginal.csv'\n",
    "validAugmentedPath = '4. Random Insertion/2. Augmented Data/ValidRandomInsertionData.txt'\n",
    "invalidWithOriginalPath = '4. Random Insertion/2. Augmented Data/InvalidRandomInsertionWithOriginal.csv'\n",
    "invalidAugmentedPath = '4. Random Insertion/2. Augmented Data/InvalidRandomInsertionData.txt'\n",
    "\n",
    "infoValid = processAugmentation(validOriginal, validRandomInsertionList, validWithOriginalPath, validAugmentedPath)\n",
    "infoValidDF = pd.DataFrame(infoValid)\n",
    "infoInvalid = processAugmentation(invalidOriginal, invalidRandomInsertionList, invalidWithOriginalPath, invalidAugmentedPath)\n",
    "infoInvalidDF = pd.DataFrame(infoInvalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3543b70f-dda4-40ef-b06c-3850e8a660da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>originalText</th>\n",
       "      <th>bestAugmentation</th>\n",
       "      <th>bestAugmentedDataSemanticSimilarity</th>\n",
       "      <th>bestAugmentedDataLexicalSimiliratity</th>\n",
       "      <th>augmented1</th>\n",
       "      <th>semanticSimilarity1</th>\n",
       "      <th>lexicalSimilarity1</th>\n",
       "      <th>augmented2</th>\n",
       "      <th>semanticSimilarity2</th>\n",
       "      <th>lexicalSimilarity2</th>\n",
       "      <th>augmented3</th>\n",
       "      <th>semanticSimilarity3</th>\n",
       "      <th>lexicalSimilarity3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"Tiene fácil acceso para las personas con movi...</td>\n",
       "      <td>\"Tiene fácil acceso para las personas con movi...</td>\n",
       "      <td>0.993096</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>\"Tiene julian fácil acceso para las personas c...</td>\n",
       "      <td>0.958814</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>\"Tiene fácil acceso otro para las personas con...</td>\n",
       "      <td>0.965412</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>\"Tiene fácil acceso para las personas con movi...</td>\n",
       "      <td>0.993096</td>\n",
       "      <td>0.937500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"Espero que hayan mejorais\"</td>\n",
       "      <td>\"Espero que hayan mejorais\" hayan</td>\n",
       "      <td>0.988550</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>espero \"Espero que hayan mejorais\"</td>\n",
       "      <td>0.977621</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>\"Espero que hayan mejorais\" hayan</td>\n",
       "      <td>0.988550</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>\"Espero hayan que hayan mejorais\"</td>\n",
       "      <td>0.987335</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"La estación es antigua, aparte de tener una s...</td>\n",
       "      <td>\"La estación es antigua, aparte de tener una s...</td>\n",
       "      <td>0.986128</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>\"La estación es antigua, aparte de tener una s...</td>\n",
       "      <td>0.986128</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>\"La estación es antigua, aparte de tener una s...</td>\n",
       "      <td>0.978366</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>\"La estación es antigua, aparte de tener una a...</td>\n",
       "      <td>0.968685</td>\n",
       "      <td>0.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"Bien\"</td>\n",
       "      <td>\"Bien\" bien</td>\n",
       "      <td>0.956442</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>\"Bien\" bien</td>\n",
       "      <td>0.956442</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>\"Bien\" bien</td>\n",
       "      <td>0.956442</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>\"Bien\" bien</td>\n",
       "      <td>0.956442</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"Bonito comodo\"</td>\n",
       "      <td>precioso \"Bonito comodo\"</td>\n",
       "      <td>0.932838</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>precioso \"Bonito comodo\"</td>\n",
       "      <td>0.932838</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>\"Bonito comodo comodo\"</td>\n",
       "      <td>0.973187</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>\"Bonito comodo\" comodo</td>\n",
       "      <td>0.978755</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        originalText  \\\n",
       "0  \"Tiene fácil acceso para las personas con movi...   \n",
       "1                        \"Espero que hayan mejorais\"   \n",
       "2  \"La estación es antigua, aparte de tener una s...   \n",
       "3                                             \"Bien\"   \n",
       "4                                    \"Bonito comodo\"   \n",
       "\n",
       "                                    bestAugmentation  \\\n",
       "0  \"Tiene fácil acceso para las personas con movi...   \n",
       "1                  \"Espero que hayan mejorais\" hayan   \n",
       "2  \"La estación es antigua, aparte de tener una s...   \n",
       "3                                        \"Bien\" bien   \n",
       "4                           precioso \"Bonito comodo\"   \n",
       "\n",
       "   bestAugmentedDataSemanticSimilarity  bestAugmentedDataLexicalSimiliratity  \\\n",
       "0                             0.993096                              0.937500   \n",
       "1                             0.988550                              1.000000   \n",
       "2                             0.986128                              0.880000   \n",
       "3                             0.956442                              1.000000   \n",
       "4                             0.932838                              0.666667   \n",
       "\n",
       "                                          augmented1  semanticSimilarity1  \\\n",
       "0  \"Tiene julian fácil acceso para las personas c...             0.958814   \n",
       "1                 espero \"Espero que hayan mejorais\"             0.977621   \n",
       "2  \"La estación es antigua, aparte de tener una s...             0.986128   \n",
       "3                                        \"Bien\" bien             0.956442   \n",
       "4                           precioso \"Bonito comodo\"             0.932838   \n",
       "\n",
       "   lexicalSimilarity1                                         augmented2  \\\n",
       "0            1.000000  \"Tiene fácil acceso otro para las personas con...   \n",
       "1            1.000000                  \"Espero que hayan mejorais\" hayan   \n",
       "2            0.880000  \"La estación es antigua, aparte de tener una s...   \n",
       "3            1.000000                                        \"Bien\" bien   \n",
       "4            0.666667                             \"Bonito comodo comodo\"   \n",
       "\n",
       "   semanticSimilarity2  lexicalSimilarity2  \\\n",
       "0             0.965412            1.000000   \n",
       "1             0.988550            1.000000   \n",
       "2             0.978366            0.916667   \n",
       "3             0.956442            1.000000   \n",
       "4             0.973187            1.000000   \n",
       "\n",
       "                                          augmented3  semanticSimilarity3  \\\n",
       "0  \"Tiene fácil acceso para las personas con movi...             0.993096   \n",
       "1                  \"Espero hayan que hayan mejorais\"             0.987335   \n",
       "2  \"La estación es antigua, aparte de tener una a...             0.968685   \n",
       "3                                        \"Bien\" bien             0.956442   \n",
       "4                             \"Bonito comodo\" comodo             0.978755   \n",
       "\n",
       "   lexicalSimilarity3  \n",
       "0            0.937500  \n",
       "1            1.000000  \n",
       "2            0.916667  \n",
       "3            1.000000  \n",
       "4            1.000000  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infoValidDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "92b380ef-24b0-4abe-a627-53dbf32e9a09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>originalText</th>\n",
       "      <th>bestAugmentation</th>\n",
       "      <th>bestAugmentedDataSemanticSimilarity</th>\n",
       "      <th>bestAugmentedDataLexicalSimiliratity</th>\n",
       "      <th>augmented1</th>\n",
       "      <th>semanticSimilarity1</th>\n",
       "      <th>lexicalSimilarity1</th>\n",
       "      <th>augmented2</th>\n",
       "      <th>semanticSimilarity2</th>\n",
       "      <th>lexicalSimilarity2</th>\n",
       "      <th>augmented3</th>\n",
       "      <th>semanticSimilarity3</th>\n",
       "      <th>lexicalSimilarity3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"He vivido 35 años en el barrio y reconozco qu...</td>\n",
       "      <td>\"He vivido dolor 35 años en joven el barrio y ...</td>\n",
       "      <td>0.980646</td>\n",
       "      <td>0.88</td>\n",
       "      <td>\"He vivido dolor 35 años en joven el barrio y ...</td>\n",
       "      <td>0.980646</td>\n",
       "      <td>0.88</td>\n",
       "      <td>\"He zonas joven vivido 35 años en el barrio y ...</td>\n",
       "      <td>0.946283</td>\n",
       "      <td>1.00</td>\n",
       "      <td>\"He vivido 35 años en el zonas barrio desapare...</td>\n",
       "      <td>0.967516</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"localización con muchos bares interesantes\"</td>\n",
       "      <td>\"localización con bares muchos bares interesan...</td>\n",
       "      <td>0.977484</td>\n",
       "      <td>1.00</td>\n",
       "      <td>\"localización bares con muchos bares interesan...</td>\n",
       "      <td>0.974223</td>\n",
       "      <td>1.00</td>\n",
       "      <td>\"localización con muchos bares bares interesan...</td>\n",
       "      <td>0.966685</td>\n",
       "      <td>1.00</td>\n",
       "      <td>\"localización con bares muchos bares interesan...</td>\n",
       "      <td>0.977484</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"…\"</td>\n",
       "      <td>\"…\"</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>\"…\"</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>\"…\"</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>\"…\"</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"Muy rica comida..\"</td>\n",
       "      <td>\"Muy rica comida..\" sustancialmente</td>\n",
       "      <td>0.935908</td>\n",
       "      <td>0.75</td>\n",
       "      <td>\"Muy rica sustancialmente comida..\"</td>\n",
       "      <td>0.878053</td>\n",
       "      <td>0.75</td>\n",
       "      <td>\"Muy rica comida..\" sustancialmente</td>\n",
       "      <td>0.935908</td>\n",
       "      <td>0.75</td>\n",
       "      <td>alimento \"Muy rica comida..\"</td>\n",
       "      <td>0.929850</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"Estación del.metro\"</td>\n",
       "      <td>\"Estación estacion del.metro\"</td>\n",
       "      <td>0.980573</td>\n",
       "      <td>1.00</td>\n",
       "      <td>estacion \"Estación del.metro\"</td>\n",
       "      <td>0.972385</td>\n",
       "      <td>1.00</td>\n",
       "      <td>\"Estación del.metro\" estacion</td>\n",
       "      <td>0.978640</td>\n",
       "      <td>1.00</td>\n",
       "      <td>\"Estación estacion del.metro\"</td>\n",
       "      <td>0.980573</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        originalText  \\\n",
       "0  \"He vivido 35 años en el barrio y reconozco qu...   \n",
       "1       \"localización con muchos bares interesantes\"   \n",
       "2                                                \"…\"   \n",
       "3                                \"Muy rica comida..\"   \n",
       "4                               \"Estación del.metro\"   \n",
       "\n",
       "                                    bestAugmentation  \\\n",
       "0  \"He vivido dolor 35 años en joven el barrio y ...   \n",
       "1  \"localización con bares muchos bares interesan...   \n",
       "2                                                \"…\"   \n",
       "3                \"Muy rica comida..\" sustancialmente   \n",
       "4                      \"Estación estacion del.metro\"   \n",
       "\n",
       "   bestAugmentedDataSemanticSimilarity  bestAugmentedDataLexicalSimiliratity  \\\n",
       "0                             0.980646                                  0.88   \n",
       "1                             0.977484                                  1.00   \n",
       "2                             1.000000                                  0.00   \n",
       "3                             0.935908                                  0.75   \n",
       "4                             0.980573                                  1.00   \n",
       "\n",
       "                                          augmented1  semanticSimilarity1  \\\n",
       "0  \"He vivido dolor 35 años en joven el barrio y ...             0.980646   \n",
       "1  \"localización bares con muchos bares interesan...             0.974223   \n",
       "2                                                \"…\"             1.000000   \n",
       "3                \"Muy rica sustancialmente comida..\"             0.878053   \n",
       "4                      estacion \"Estación del.metro\"             0.972385   \n",
       "\n",
       "   lexicalSimilarity1                                         augmented2  \\\n",
       "0                0.88  \"He zonas joven vivido 35 años en el barrio y ...   \n",
       "1                1.00  \"localización con muchos bares bares interesan...   \n",
       "2                0.00                                                \"…\"   \n",
       "3                0.75                \"Muy rica comida..\" sustancialmente   \n",
       "4                1.00                      \"Estación del.metro\" estacion   \n",
       "\n",
       "   semanticSimilarity2  lexicalSimilarity2  \\\n",
       "0             0.946283                1.00   \n",
       "1             0.966685                1.00   \n",
       "2             1.000000                0.00   \n",
       "3             0.935908                0.75   \n",
       "4             0.978640                1.00   \n",
       "\n",
       "                                          augmented3  semanticSimilarity3  \\\n",
       "0  \"He vivido 35 años en el zonas barrio desapare...             0.967516   \n",
       "1  \"localización con bares muchos bares interesan...             0.977484   \n",
       "2                                                \"…\"             1.000000   \n",
       "3                       alimento \"Muy rica comida..\"             0.929850   \n",
       "4                      \"Estación estacion del.metro\"             0.980573   \n",
       "\n",
       "   lexicalSimilarity3  \n",
       "0                1.00  \n",
       "1                1.00  \n",
       "2                0.00  \n",
       "3                0.75  \n",
       "4                1.00  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infoInvalidDF.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a54ae238-b3cb-4d07-b0f8-341d1d237680",
   "metadata": {},
   "source": [
    "Como se puede ver, da mejores resultados que el anterior método, pero aun asi hay muy poca variación tanto en la similitud semántica como en la léxica (el mayor de los problemas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b22b432-8417-4e7d-9c46-4a52e5f79e05",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Intercambio aleatorio\n",
    "\n",
    "Este método consiste en cambiar n veces dos palabras aleatoriamente en un texto."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7436f963-b4c9-4cc0-9cf1-e266eaa4b2f2",
   "metadata": {},
   "source": [
    "Esta función va a coger dos índices aleatorios y distintos de una lista."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "97829934-b3cd-4d60-a378-178aca235864",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get two different indexes of a list\n",
    "def getRandomIndexes(wordsList):\n",
    "    \n",
    "    # Generate a random index within the range of the words list\n",
    "    pos1 = random.randint(0, len(wordsList) - 1)\n",
    "    \n",
    "    # Keep generating a new index until it is not equal to the first index\n",
    "    pos2 = random.randint(0, len(wordsList) - 1)\n",
    "    while pos1 == pos2:\n",
    "        pos2 = random.randint(0, len(wordsList) - 1)   \n",
    "\n",
    "    #Return both indexes\n",
    "    return pos1, pos2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d79e90-4f18-420d-8a71-51cf9b88f46c",
   "metadata": {},
   "source": [
    "Esta funcion selecciona dos palabras no vacías del texto original y las intercambia. Repite este proceso tantas veces como la función alculateModifications(text) lo indique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b5c866c6-4871-4e58-9db9-3a1ed10d3b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function that selects two non stop words of a text and swaps them. Does this alculateModifications(text) times\n",
    "def wordSwap(text):\n",
    "    #Remove the quotation marks\n",
    "    text = removeQuotes(text)\n",
    "\n",
    "    #Get the list of words of the text\n",
    "    wordsList = text.split()\n",
    "    \n",
    "    #Remove the spanish stop words from the text\n",
    "    withoutStopWordsTextList = revomeSpanishStopWords(text).split()\n",
    "    \n",
    "    # Check if there are more than one none stop words to perform swapping\n",
    "    if len(withoutStopWordsTextList) > 1:\n",
    "        # Loop for the number of modifications calculated for the text\n",
    "        for i in range(calculateModifications(text)):\n",
    "            # Get two diferent random indexes\n",
    "            pos1 , pos2 = getRandomIndexes(withoutStopWordsTextList)\n",
    "\n",
    "            #Update the indexes to match the original text\n",
    "            pos1 = wordsList.index(withoutStopWordsTextList[pos1])\n",
    "            pos2 = wordsList.index(withoutStopWordsTextList[pos2])\n",
    "            \n",
    "            # Swap the words at the two random positions\n",
    "            aux = wordsList[pos1]\n",
    "            wordsList[pos1] = wordsList[pos2]\n",
    "            wordsList[pos2] = aux\n",
    "\n",
    "    \n",
    "    # Return the modified line with quotes added\n",
    "    return addQuotes(' '.join(wordsList))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0ea91c-eaea-487d-a22d-19807fae2432",
   "metadata": {},
   "source": [
    "Función que realiza el intercambio aleatorio a todos los elementos de una lista de textos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a5ece6d8-3259-4517-bcb7-5759783edc82",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random swap\n",
    "def randomSwap(textList, targetPath):\n",
    "    #Open the file\n",
    "    targetFile = open(targetPath, \"w\", encoding = \"utf-8\")\n",
    "\n",
    "    newList = []\n",
    "    for text in textList:\n",
    "        newText = wordSwap(text)\n",
    "        newList.append(newText)\n",
    "        targetFile.write(newText + \"\\n\")\n",
    "\n",
    "    #Close the file\n",
    "    targetFile.close()\n",
    "    \n",
    "    return newList"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0365c26-e77d-439a-8017-e1437b734392",
   "metadata": {},
   "source": [
    "Se va a realizar este proceso tres veces para generar más textos distintos de los que se posteriormente se elegirán los mejores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "25e42176-1ab7-40b5-9157-f5aac0a6c388",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the original data\n",
    "validOriginalPath = \"/home/ibon/Documentos/GitHub/TFG/1. Data/4. Labeled Reviews/2. Without Emojis/ValidReviews.txt\"\n",
    "invalidOriginalPath = \"/home/ibon/Documentos/GitHub/TFG/1. Data/4. Labeled Reviews/2. Without Emojis/InvalidReviews.txt\"\n",
    "\n",
    "validOriginal = importFromTxtToList(validOriginalPath)\n",
    "invalidOriginal = importFromTxtToList(invalidOriginalPath) \n",
    "\n",
    "#Apply the synonym replacement 3 times to both datasets\n",
    "for i in range(3):\n",
    "    validPath = f\"5. Random Swap/1. All Augmented Data/validRandomSwapReviews{i + 1}.txt\"\n",
    "    invalidPath = f\"5. Random Swap/1. All Augmented Data/invalidRandomSwapReviews{i + 1}.txt\"\n",
    "\n",
    "    \n",
    "    randomSwap(validOriginal, validPath)\n",
    "    randomSwap(invalidOriginal, invalidPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ab8f82-48b3-434b-86e8-bd112fb91533",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Análisis del intercambio aleatorio y selección de los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc91d420-094a-46fb-a96d-793ad35c8025",
   "metadata": {},
   "source": [
    "Importar los datos originales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "673a0d06-0f2e-4ea9-ba2b-8c1c43c4e852",
   "metadata": {},
   "outputs": [],
   "source": [
    "validOriginalPath = \"/home/ibon/Documentos/GitHub/TFG/1. Data/4. Labeled Reviews/2. Without Emojis/ValidReviews.txt\"\n",
    "invalidOriginalPath = \"/home/ibon/Documentos/GitHub/TFG/1. Data/4. Labeled Reviews/2. Without Emojis/InvalidReviews.txt\"\n",
    "\n",
    "validOriginal = importFromTxtToList(validOriginalPath)\n",
    "invalidOriginal = importFromTxtToList(invalidOriginalPath) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd32f0a-8087-4f8f-8481-66496f5e888b",
   "metadata": {},
   "source": [
    "Importar los datasets generados en el intercambio aleatorio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bdecae3e-bc1a-4e7a-9ccc-3e8b065b5fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import all the data \n",
    "validRandomSwapList = []\n",
    "invalidRandomSwapList = []\n",
    "for i in range(3):\n",
    "    validPath = f\"5. Random Swap/1. All Augmented Data/validRandomSwapReviews{i + 1}.txt\"\n",
    "    invalidPath = f\"5. Random Swap/1. All Augmented Data/invalidRandomSwapReviews{i + 1}.txt\"\n",
    "\n",
    "    validRandomSwapList.append(importFromTxtToList(validPath))\n",
    "    invalidRandomSwapList.append(importFromTxtToList(invalidPath))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b00d5e-0728-43a2-ae1e-8a0e67905317",
   "metadata": {},
   "source": [
    "Realizar el análisis y la selección de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "89d280cf-d939-4a88-a954-ab255e67eb8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "validWithOriginalPath = '5. Random Swap/2. Augmented Data/ ValidRandomSwapWithOriginal.csv'\n",
    "validAugmentedPath = '5. Random Swap/2. Augmented Data/ValidRandomSwapData.txt'\n",
    "invalidWithOriginalPath = '5. Random Swap/2. Augmented Data/InvalidRandomSwapWithOriginal.csv'\n",
    "invalidAugmentedPath = '5. Random Swap/2. Augmented Data/InvalidRandomSwapData.txt'\n",
    "\n",
    "infoValid = processAugmentation(validOriginal, validRandomSwapList, validWithOriginalPath, validAugmentedPath)\n",
    "infoValidDF = pd.DataFrame(infoValid)\n",
    "infoInvalid = processAugmentation(invalidOriginal, invalidRandomSwapList, invalidWithOriginalPath, invalidAugmentedPath)\n",
    "infoInvalidDF = pd.DataFrame(infoInvalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3e55b3f8-2e9e-4179-9bd3-221084899593",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>originalText</th>\n",
       "      <th>bestAugmentation</th>\n",
       "      <th>bestAugmentedDataSemanticSimilarity</th>\n",
       "      <th>bestAugmentedDataLexicalSimiliratity</th>\n",
       "      <th>augmented1</th>\n",
       "      <th>semanticSimilarity1</th>\n",
       "      <th>lexicalSimilarity1</th>\n",
       "      <th>augmented2</th>\n",
       "      <th>semanticSimilarity2</th>\n",
       "      <th>lexicalSimilarity2</th>\n",
       "      <th>augmented3</th>\n",
       "      <th>semanticSimilarity3</th>\n",
       "      <th>lexicalSimilarity3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"Tiene fácil acceso para las personas con movi...</td>\n",
       "      <td>\"acceso fácil Tiene para las personas con movi...</td>\n",
       "      <td>0.993650</td>\n",
       "      <td>1.0</td>\n",
       "      <td>\"Tiene fácil acceso para las personas con movi...</td>\n",
       "      <td>0.986013</td>\n",
       "      <td>1.0</td>\n",
       "      <td>\"Tiene fácil acceso para las personas con movi...</td>\n",
       "      <td>0.989630</td>\n",
       "      <td>1.0</td>\n",
       "      <td>\"acceso fácil Tiene para las personas con movi...</td>\n",
       "      <td>0.993650</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"Espero que hayan mejorais\"</td>\n",
       "      <td>\"Espero que mejorais hayan\"</td>\n",
       "      <td>0.992640</td>\n",
       "      <td>1.0</td>\n",
       "      <td>\"hayan que Espero mejorais\"</td>\n",
       "      <td>0.989534</td>\n",
       "      <td>1.0</td>\n",
       "      <td>\"Espero que mejorais hayan\"</td>\n",
       "      <td>0.992640</td>\n",
       "      <td>1.0</td>\n",
       "      <td>\"mejorais que hayan Espero\"</td>\n",
       "      <td>0.991634</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"La estación es antigua, aparte de tener una s...</td>\n",
       "      <td>\"La estación es antigua, aparte de tener una n...</td>\n",
       "      <td>0.995518</td>\n",
       "      <td>1.0</td>\n",
       "      <td>\"La estación es antigua, aparte de tener una n...</td>\n",
       "      <td>0.995518</td>\n",
       "      <td>1.0</td>\n",
       "      <td>\"La estación es antigua, aparte de habilitada ...</td>\n",
       "      <td>0.993437</td>\n",
       "      <td>1.0</td>\n",
       "      <td>\"La estación es antigua, pasillo de tener una ...</td>\n",
       "      <td>0.982857</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"Bien\"</td>\n",
       "      <td>\"Bien\"</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>\"Bien\"</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>\"Bien\"</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>\"Bien\"</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"Bonito comodo\"</td>\n",
       "      <td>\"comodo Bonito\"</td>\n",
       "      <td>0.988781</td>\n",
       "      <td>1.0</td>\n",
       "      <td>\"comodo Bonito\"</td>\n",
       "      <td>0.988781</td>\n",
       "      <td>1.0</td>\n",
       "      <td>\"comodo Bonito\"</td>\n",
       "      <td>0.988781</td>\n",
       "      <td>1.0</td>\n",
       "      <td>\"comodo Bonito\"</td>\n",
       "      <td>0.988781</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        originalText  \\\n",
       "0  \"Tiene fácil acceso para las personas con movi...   \n",
       "1                        \"Espero que hayan mejorais\"   \n",
       "2  \"La estación es antigua, aparte de tener una s...   \n",
       "3                                             \"Bien\"   \n",
       "4                                    \"Bonito comodo\"   \n",
       "\n",
       "                                    bestAugmentation  \\\n",
       "0  \"acceso fácil Tiene para las personas con movi...   \n",
       "1                        \"Espero que mejorais hayan\"   \n",
       "2  \"La estación es antigua, aparte de tener una n...   \n",
       "3                                             \"Bien\"   \n",
       "4                                    \"comodo Bonito\"   \n",
       "\n",
       "   bestAugmentedDataSemanticSimilarity  bestAugmentedDataLexicalSimiliratity  \\\n",
       "0                             0.993650                                   1.0   \n",
       "1                             0.992640                                   1.0   \n",
       "2                             0.995518                                   1.0   \n",
       "3                             1.000000                                   1.0   \n",
       "4                             0.988781                                   1.0   \n",
       "\n",
       "                                          augmented1  semanticSimilarity1  \\\n",
       "0  \"Tiene fácil acceso para las personas con movi...             0.986013   \n",
       "1                        \"hayan que Espero mejorais\"             0.989534   \n",
       "2  \"La estación es antigua, aparte de tener una n...             0.995518   \n",
       "3                                             \"Bien\"             1.000000   \n",
       "4                                    \"comodo Bonito\"             0.988781   \n",
       "\n",
       "   lexicalSimilarity1                                         augmented2  \\\n",
       "0                 1.0  \"Tiene fácil acceso para las personas con movi...   \n",
       "1                 1.0                        \"Espero que mejorais hayan\"   \n",
       "2                 1.0  \"La estación es antigua, aparte de habilitada ...   \n",
       "3                 1.0                                             \"Bien\"   \n",
       "4                 1.0                                    \"comodo Bonito\"   \n",
       "\n",
       "   semanticSimilarity2  lexicalSimilarity2  \\\n",
       "0             0.989630                 1.0   \n",
       "1             0.992640                 1.0   \n",
       "2             0.993437                 1.0   \n",
       "3             1.000000                 1.0   \n",
       "4             0.988781                 1.0   \n",
       "\n",
       "                                          augmented3  semanticSimilarity3  \\\n",
       "0  \"acceso fácil Tiene para las personas con movi...             0.993650   \n",
       "1                        \"mejorais que hayan Espero\"             0.991634   \n",
       "2  \"La estación es antigua, pasillo de tener una ...             0.982857   \n",
       "3                                             \"Bien\"             1.000000   \n",
       "4                                    \"comodo Bonito\"             0.988781   \n",
       "\n",
       "   lexicalSimilarity3  \n",
       "0                 1.0  \n",
       "1                 1.0  \n",
       "2                 1.0  \n",
       "3                 1.0  \n",
       "4                 1.0  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infoValidDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e4b74276-cf01-4c1a-8f60-d6177a774b0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>originalText</th>\n",
       "      <th>bestAugmentation</th>\n",
       "      <th>bestAugmentedDataSemanticSimilarity</th>\n",
       "      <th>bestAugmentedDataLexicalSimiliratity</th>\n",
       "      <th>augmented1</th>\n",
       "      <th>semanticSimilarity1</th>\n",
       "      <th>lexicalSimilarity1</th>\n",
       "      <th>augmented2</th>\n",
       "      <th>semanticSimilarity2</th>\n",
       "      <th>lexicalSimilarity2</th>\n",
       "      <th>augmented3</th>\n",
       "      <th>semanticSimilarity3</th>\n",
       "      <th>lexicalSimilarity3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"He vivido 35 años en el barrio y reconozco qu...</td>\n",
       "      <td>\"He gente 35 años en el ha y reconozco que el ...</td>\n",
       "      <td>0.986286</td>\n",
       "      <td>1.0</td>\n",
       "      <td>\"He gente 35 años en el ha y reconozco que el ...</td>\n",
       "      <td>0.986286</td>\n",
       "      <td>1.0</td>\n",
       "      <td>\"barrio vivido 35 años en el He y reconozco qu...</td>\n",
       "      <td>0.976794</td>\n",
       "      <td>1.0</td>\n",
       "      <td>\"He zonas mejoró años en el barrio y reconozco...</td>\n",
       "      <td>0.970291</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"localización con muchos bares interesantes\"</td>\n",
       "      <td>\"interesantes con muchos bares localización\"</td>\n",
       "      <td>0.991607</td>\n",
       "      <td>1.0</td>\n",
       "      <td>\"interesantes con muchos bares localización\"</td>\n",
       "      <td>0.991607</td>\n",
       "      <td>1.0</td>\n",
       "      <td>\"bares con muchos localización interesantes\"</td>\n",
       "      <td>0.985325</td>\n",
       "      <td>1.0</td>\n",
       "      <td>\"interesantes con muchos bares localización\"</td>\n",
       "      <td>0.991607</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"…\"</td>\n",
       "      <td>\"…\"</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>\"…\"</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>\"…\"</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>\"…\"</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"Muy rica comida..\"</td>\n",
       "      <td>\"rica Muy comida..\"</td>\n",
       "      <td>0.989455</td>\n",
       "      <td>1.0</td>\n",
       "      <td>\"Muy comida.. rica\"</td>\n",
       "      <td>0.980956</td>\n",
       "      <td>1.0</td>\n",
       "      <td>\"rica Muy comida..\"</td>\n",
       "      <td>0.989455</td>\n",
       "      <td>1.0</td>\n",
       "      <td>\"comida.. rica Muy\"</td>\n",
       "      <td>0.964634</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"Estación del.metro\"</td>\n",
       "      <td>\"del.metro Estación\"</td>\n",
       "      <td>0.988279</td>\n",
       "      <td>1.0</td>\n",
       "      <td>\"del.metro Estación\"</td>\n",
       "      <td>0.988279</td>\n",
       "      <td>1.0</td>\n",
       "      <td>\"del.metro Estación\"</td>\n",
       "      <td>0.988279</td>\n",
       "      <td>1.0</td>\n",
       "      <td>\"del.metro Estación\"</td>\n",
       "      <td>0.988279</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        originalText  \\\n",
       "0  \"He vivido 35 años en el barrio y reconozco qu...   \n",
       "1       \"localización con muchos bares interesantes\"   \n",
       "2                                                \"…\"   \n",
       "3                                \"Muy rica comida..\"   \n",
       "4                               \"Estación del.metro\"   \n",
       "\n",
       "                                    bestAugmentation  \\\n",
       "0  \"He gente 35 años en el ha y reconozco que el ...   \n",
       "1       \"interesantes con muchos bares localización\"   \n",
       "2                                                \"…\"   \n",
       "3                                \"rica Muy comida..\"   \n",
       "4                               \"del.metro Estación\"   \n",
       "\n",
       "   bestAugmentedDataSemanticSimilarity  bestAugmentedDataLexicalSimiliratity  \\\n",
       "0                             0.986286                                   1.0   \n",
       "1                             0.991607                                   1.0   \n",
       "2                             1.000000                                   0.0   \n",
       "3                             0.989455                                   1.0   \n",
       "4                             0.988279                                   1.0   \n",
       "\n",
       "                                          augmented1  semanticSimilarity1  \\\n",
       "0  \"He gente 35 años en el ha y reconozco que el ...             0.986286   \n",
       "1       \"interesantes con muchos bares localización\"             0.991607   \n",
       "2                                                \"…\"             1.000000   \n",
       "3                                \"Muy comida.. rica\"             0.980956   \n",
       "4                               \"del.metro Estación\"             0.988279   \n",
       "\n",
       "   lexicalSimilarity1                                         augmented2  \\\n",
       "0                 1.0  \"barrio vivido 35 años en el He y reconozco qu...   \n",
       "1                 1.0       \"bares con muchos localización interesantes\"   \n",
       "2                 0.0                                                \"…\"   \n",
       "3                 1.0                                \"rica Muy comida..\"   \n",
       "4                 1.0                               \"del.metro Estación\"   \n",
       "\n",
       "   semanticSimilarity2  lexicalSimilarity2  \\\n",
       "0             0.976794                 1.0   \n",
       "1             0.985325                 1.0   \n",
       "2             1.000000                 0.0   \n",
       "3             0.989455                 1.0   \n",
       "4             0.988279                 1.0   \n",
       "\n",
       "                                          augmented3  semanticSimilarity3  \\\n",
       "0  \"He zonas mejoró años en el barrio y reconozco...             0.970291   \n",
       "1       \"interesantes con muchos bares localización\"             0.991607   \n",
       "2                                                \"…\"             1.000000   \n",
       "3                                \"comida.. rica Muy\"             0.964634   \n",
       "4                               \"del.metro Estación\"             0.988279   \n",
       "\n",
       "   lexicalSimilarity3  \n",
       "0                 1.0  \n",
       "1                 1.0  \n",
       "2                 0.0  \n",
       "3                 1.0  \n",
       "4                 1.0  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infoInvalidDF.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f5a87f-6a1c-4e4a-9033-bbe8ea9b47b4",
   "metadata": {},
   "source": [
    "En este caso, la similitud semántica varía ligeramente, pero la similitud léxica permanece intacta, ya que no se introducen nuevas palabras ni se eliminan palabras existentes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9c96e2-13e1-4b78-8f33-066e78ae243b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Eliminación aleatoria\n",
    "Este método consiste en eliminar una palabra elegida aleatoriamente en el texto que no sea una palabra vacía."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3a7041-ff2d-4015-9606-e10cf37a2b9e",
   "metadata": {},
   "source": [
    "Función que escoge una palabra no vacía de un texto y la elimina"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6cc15602-0bde-4f58-b094-0ee841eb59d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deleteWord(text):\n",
    "    #Remove the quotation marks\n",
    "    text = removeQuotes(text)\n",
    "\n",
    "    #Get the list of words of the text\n",
    "    wordsList = text.split()\n",
    "\n",
    "    #Remove the spanish stop words from the text\n",
    "    withoutStopWordsTextList = revomeSpanishStopWords(text).split()\n",
    "    \n",
    "    # Calculate and performs the number of deletions based on the line's content\n",
    "    for i in range(calculateModifications(text)):\n",
    "        # Check if there are more than one word to delete from\n",
    "        if len(withoutStopWordsTextList) > 1:\n",
    "            # Generate a random index to select a word for deletion\n",
    "            pos = random.randint(0, len(withoutStopWordsTextList) - 1)\n",
    "    \n",
    "            #Update the index to the original list (with stop words)\n",
    "            indx = wordsList.index(withoutStopWordsTextList[pos])\n",
    "    \n",
    "            #Remove the chosen word\n",
    "            wordsList.pop(indx)\n",
    "            withoutStopWordsTextList.pop(pos)\n",
    "            \n",
    "    # Join the remaining words into a string, add quotes, and return the result\n",
    "    return addQuotes(' '.join(wordsList))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24b8a6f-7e52-4a83-9c36-7c7587fa4256",
   "metadata": {},
   "source": [
    "Función que realiza la eliminación aleatoria a todos los elementos de una lista de textos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "238fcd47-7d97-4703-baef-3c58b69afbd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomDeletion(textList, targetPath):\n",
    "    #Open the file\n",
    "    targetFile = open(targetPath, \"w\", encoding = 'utf-8')\n",
    "    \n",
    "    newList = []\n",
    "    for text in textList:\n",
    "        newText = deleteWord(text)\n",
    "        newList.append(newText)\n",
    "        targetFile.write(newText + \"\\n\")\n",
    "\n",
    "    #Close the file\n",
    "    targetFile.close()\n",
    "    \n",
    "    return newList"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a1beca-d375-423f-b73b-55850e5ea8a5",
   "metadata": {},
   "source": [
    "Se va a realizar este proceso tres veces para generar más textos distintos de los que se posteriormente se elegirán los mejores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9dccc3e6-605f-42ff-9876-4429ec89c5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the original data\n",
    "validOriginalPath = \"/home/ibon/Documentos/GitHub/TFG/1. Data/4. Labeled Reviews/2. Without Emojis/ValidReviews.txt\"\n",
    "invalidOriginalPath = \"/home/ibon/Documentos/GitHub/TFG/1. Data/4. Labeled Reviews/2. Without Emojis/InvalidReviews.txt\"\n",
    "\n",
    "validOriginal = importFromTxtToList(validOriginalPath)\n",
    "invalidOriginal = importFromTxtToList(invalidOriginalPath) \n",
    "\n",
    "#Apply the synonym replacement 3 times to both datasets\n",
    "for i in range(3):\n",
    "    validPath = f\"6. Random Deletion/1. All Augmented Data/validRandomDeletionReviews{i + 1}.txt\"\n",
    "    invalidPath = f\"6. Random Deletion/1. All Augmented Data/invalidRandomDeletionReviews{i + 1}.txt\"\n",
    "\n",
    "    \n",
    "    randomDeletion(validOriginal, validPath)\n",
    "    randomDeletion(invalidOriginal, invalidPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee1953ed-beb3-4e83-9692-20666eff56cd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Análisis de la eliminación aleatoria y selección de los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c678ea0-134d-4a5f-9d3c-1ca2e388f834",
   "metadata": {},
   "source": [
    "Importar los datos originales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1e83b81d-4fd4-4a00-abb2-e4bc9d701ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "validOriginalPath = \"/home/ibon/Documentos/GitHub/TFG/1. Data/4. Labeled Reviews/2. Without Emojis/ValidReviews.txt\"\n",
    "invalidOriginalPath = \"/home/ibon/Documentos/GitHub/TFG/1. Data/4. Labeled Reviews/2. Without Emojis/InvalidReviews.txt\"\n",
    "\n",
    "validOriginal = importFromTxtToList(validOriginalPath)\n",
    "invalidOriginal = importFromTxtToList(invalidOriginalPath) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac25cb8f-2ddf-48be-9107-a7d05fba9692",
   "metadata": {},
   "source": [
    "Importar los datasets generados en la eliminacion aleatoria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "105ad61d-4adf-474c-8549-ff80b99ca0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import all the data \n",
    "validRandomDeletionList = []\n",
    "invalidRandomDeletionList = []\n",
    "for i in range(3):\n",
    "    validPath = f\"6. Random Deletion/1. All Augmented Data/validRandomDeletionReviews{i + 1}.txt\"\n",
    "    invalidPath = f\"6. Random Deletion/1. All Augmented Data/invalidRandomDeletionReviews{i + 1}.txt\"\n",
    "\n",
    "    validRandomDeletionList.append(importFromTxtToList(validPath))\n",
    "    invalidRandomDeletionList.append(importFromTxtToList(invalidPath))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41cbbf28-4bb7-42e0-abd1-6eddf6523b81",
   "metadata": {},
   "source": [
    "Realizar el análisis y la selección de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8d4c4bfc-0338-45d3-b01e-1d78aab627e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "validWithOriginalPath = '6. Random Deletion/2. Augmented Data/ ValidRandomDeletionWithOriginal.csv'\n",
    "validAugmentedPath = '6. Random Deletion/2. Augmented Data/ValidRandomDeletionData.txt'\n",
    "invalidWithOriginalPath = '6. Random Deletion/2. Augmented Data/InvalidRandomDeletionWithOriginal.csv'\n",
    "invalidAugmentedPath = '6. Random Deletion/2. Augmented Data/InvalidRandomDeletionData.txt'\n",
    "\n",
    "infoValid = processAugmentation(validOriginal, validRandomDeletionList, validWithOriginalPath, validAugmentedPath)\n",
    "infoValidDF = pd.DataFrame(infoValid)\n",
    "infoInvalid = processAugmentation(invalidOriginal, invalidRandomDeletionList, invalidWithOriginalPath, invalidAugmentedPath)\n",
    "infoInvalidDF = pd.DataFrame(infoInvalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "61a6547c-38b4-4111-b890-1a9223d409e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>originalText</th>\n",
       "      <th>bestAugmentation</th>\n",
       "      <th>bestAugmentedDataSemanticSimilarity</th>\n",
       "      <th>bestAugmentedDataLexicalSimiliratity</th>\n",
       "      <th>augmented1</th>\n",
       "      <th>semanticSimilarity1</th>\n",
       "      <th>lexicalSimilarity1</th>\n",
       "      <th>augmented2</th>\n",
       "      <th>semanticSimilarity2</th>\n",
       "      <th>lexicalSimilarity2</th>\n",
       "      <th>augmented3</th>\n",
       "      <th>semanticSimilarity3</th>\n",
       "      <th>lexicalSimilarity3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"Tiene fácil acceso para las personas con movi...</td>\n",
       "      <td>\"Tiene fácil acceso para las personas con movi...</td>\n",
       "      <td>0.976980</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>\"Tiene fácil acceso para las personas con movi...</td>\n",
       "      <td>0.945306</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>\"Tiene fácil acceso para las personas con movi...</td>\n",
       "      <td>0.976980</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>\"Tiene fácil acceso para las con movilidad una...</td>\n",
       "      <td>0.951568</td>\n",
       "      <td>0.866667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"Espero que hayan mejorais\"</td>\n",
       "      <td>\"que hayan mejorais\"</td>\n",
       "      <td>0.915591</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>\"que hayan mejorais\"</td>\n",
       "      <td>0.915591</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>\"Espero que mejorais\"</td>\n",
       "      <td>0.900617</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>\"que hayan mejorais\"</td>\n",
       "      <td>0.915591</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"La estación es antigua, aparte de tener una s...</td>\n",
       "      <td>\"La estación es antigua, aparte de tener una s...</td>\n",
       "      <td>0.989588</td>\n",
       "      <td>0.863636</td>\n",
       "      <td>\"La estación es antigua, aparte de tener una s...</td>\n",
       "      <td>0.989588</td>\n",
       "      <td>0.863636</td>\n",
       "      <td>\"La estación es antigua, aparte de una sola sa...</td>\n",
       "      <td>0.987141</td>\n",
       "      <td>0.863636</td>\n",
       "      <td>\"La estación es antigua, aparte de una sola sa...</td>\n",
       "      <td>0.944223</td>\n",
       "      <td>0.909091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"Bien\"</td>\n",
       "      <td>\"Bien\"</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>\"Bien\"</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>\"Bien\"</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>\"Bien\"</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"Bonito comodo\"</td>\n",
       "      <td>\"comodo\"</td>\n",
       "      <td>0.800290</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>\"Bonito\"</td>\n",
       "      <td>0.791561</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>\"comodo\"</td>\n",
       "      <td>0.800290</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>\"Bonito\"</td>\n",
       "      <td>0.791561</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        originalText  \\\n",
       "0  \"Tiene fácil acceso para las personas con movi...   \n",
       "1                        \"Espero que hayan mejorais\"   \n",
       "2  \"La estación es antigua, aparte de tener una s...   \n",
       "3                                             \"Bien\"   \n",
       "4                                    \"Bonito comodo\"   \n",
       "\n",
       "                                    bestAugmentation  \\\n",
       "0  \"Tiene fácil acceso para las personas con movi...   \n",
       "1                               \"que hayan mejorais\"   \n",
       "2  \"La estación es antigua, aparte de tener una s...   \n",
       "3                                             \"Bien\"   \n",
       "4                                           \"comodo\"   \n",
       "\n",
       "   bestAugmentedDataSemanticSimilarity  bestAugmentedDataLexicalSimiliratity  \\\n",
       "0                             0.976980                              0.866667   \n",
       "1                             0.915591                              0.666667   \n",
       "2                             0.989588                              0.863636   \n",
       "3                             1.000000                              1.000000   \n",
       "4                             0.800290                              0.500000   \n",
       "\n",
       "                                          augmented1  semanticSimilarity1  \\\n",
       "0  \"Tiene fácil acceso para las personas con movi...             0.945306   \n",
       "1                               \"que hayan mejorais\"             0.915591   \n",
       "2  \"La estación es antigua, aparte de tener una s...             0.989588   \n",
       "3                                             \"Bien\"             1.000000   \n",
       "4                                           \"Bonito\"             0.791561   \n",
       "\n",
       "   lexicalSimilarity1                                         augmented2  \\\n",
       "0            0.866667  \"Tiene fácil acceso para las personas con movi...   \n",
       "1            0.666667                              \"Espero que mejorais\"   \n",
       "2            0.863636  \"La estación es antigua, aparte de una sola sa...   \n",
       "3            1.000000                                             \"Bien\"   \n",
       "4            0.500000                                           \"comodo\"   \n",
       "\n",
       "   semanticSimilarity2  lexicalSimilarity2  \\\n",
       "0             0.976980            0.866667   \n",
       "1             0.900617            0.666667   \n",
       "2             0.987141            0.863636   \n",
       "3             1.000000            1.000000   \n",
       "4             0.800290            0.500000   \n",
       "\n",
       "                                          augmented3  semanticSimilarity3  \\\n",
       "0  \"Tiene fácil acceso para las con movilidad una...             0.951568   \n",
       "1                               \"que hayan mejorais\"             0.915591   \n",
       "2  \"La estación es antigua, aparte de una sola sa...             0.944223   \n",
       "3                                             \"Bien\"             1.000000   \n",
       "4                                           \"Bonito\"             0.791561   \n",
       "\n",
       "   lexicalSimilarity3  \n",
       "0            0.866667  \n",
       "1            0.666667  \n",
       "2            0.909091  \n",
       "3            1.000000  \n",
       "4            0.500000  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infoValidDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bef9b4fa-a0b8-4eb6-a03d-24d9ece921d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>originalText</th>\n",
       "      <th>bestAugmentation</th>\n",
       "      <th>bestAugmentedDataSemanticSimilarity</th>\n",
       "      <th>bestAugmentedDataLexicalSimiliratity</th>\n",
       "      <th>augmented1</th>\n",
       "      <th>semanticSimilarity1</th>\n",
       "      <th>lexicalSimilarity1</th>\n",
       "      <th>augmented2</th>\n",
       "      <th>semanticSimilarity2</th>\n",
       "      <th>lexicalSimilarity2</th>\n",
       "      <th>augmented3</th>\n",
       "      <th>semanticSimilarity3</th>\n",
       "      <th>lexicalSimilarity3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"He vivido 35 años en el barrio y reconozco qu...</td>\n",
       "      <td>\"He 35 años en el barrio y que el metro nos di...</td>\n",
       "      <td>0.955221</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>\"He vivido 35 años en el y reconozco que el me...</td>\n",
       "      <td>0.966433</td>\n",
       "      <td>0.863636</td>\n",
       "      <td>\"He vivido 35 años en el barrio y reconozco qu...</td>\n",
       "      <td>0.948992</td>\n",
       "      <td>0.863636</td>\n",
       "      <td>\"He 35 años en el barrio y que el metro nos di...</td>\n",
       "      <td>0.955221</td>\n",
       "      <td>0.818182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"localización con muchos bares interesantes\"</td>\n",
       "      <td>\"localización con muchos interesantes\"</td>\n",
       "      <td>0.846651</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>\"localización con muchos interesantes\"</td>\n",
       "      <td>0.846651</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>\"con muchos bares interesantes\"</td>\n",
       "      <td>0.734324</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>\"con muchos bares interesantes\"</td>\n",
       "      <td>0.734324</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"…\"</td>\n",
       "      <td>\"…\"</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>\"…\"</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>\"…\"</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>\"…\"</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"Muy rica comida..\"</td>\n",
       "      <td>\"Muy comida..\"</td>\n",
       "      <td>0.739944</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>\"Muy comida..\"</td>\n",
       "      <td>0.739944</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>\"Muy comida..\"</td>\n",
       "      <td>0.739944</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>\"Muy comida..\"</td>\n",
       "      <td>0.739944</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"Estación del.metro\"</td>\n",
       "      <td>\"del.metro\"</td>\n",
       "      <td>0.768227</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>\"Estación\"</td>\n",
       "      <td>0.710799</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>\"Estación\"</td>\n",
       "      <td>0.710799</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>\"del.metro\"</td>\n",
       "      <td>0.768227</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        originalText  \\\n",
       "0  \"He vivido 35 años en el barrio y reconozco qu...   \n",
       "1       \"localización con muchos bares interesantes\"   \n",
       "2                                                \"…\"   \n",
       "3                                \"Muy rica comida..\"   \n",
       "4                               \"Estación del.metro\"   \n",
       "\n",
       "                                    bestAugmentation  \\\n",
       "0  \"He 35 años en el barrio y que el metro nos di...   \n",
       "1             \"localización con muchos interesantes\"   \n",
       "2                                                \"…\"   \n",
       "3                                     \"Muy comida..\"   \n",
       "4                                        \"del.metro\"   \n",
       "\n",
       "   bestAugmentedDataSemanticSimilarity  bestAugmentedDataLexicalSimiliratity  \\\n",
       "0                             0.955221                              0.818182   \n",
       "1                             0.846651                              0.666667   \n",
       "2                             1.000000                              0.000000   \n",
       "3                             0.739944                              0.666667   \n",
       "4                             0.768227                              0.500000   \n",
       "\n",
       "                                          augmented1  semanticSimilarity1  \\\n",
       "0  \"He vivido 35 años en el y reconozco que el me...             0.966433   \n",
       "1             \"localización con muchos interesantes\"             0.846651   \n",
       "2                                                \"…\"             1.000000   \n",
       "3                                     \"Muy comida..\"             0.739944   \n",
       "4                                         \"Estación\"             0.710799   \n",
       "\n",
       "   lexicalSimilarity1                                         augmented2  \\\n",
       "0            0.863636  \"He vivido 35 años en el barrio y reconozco qu...   \n",
       "1            0.666667                    \"con muchos bares interesantes\"   \n",
       "2            0.000000                                                \"…\"   \n",
       "3            0.666667                                     \"Muy comida..\"   \n",
       "4            0.500000                                         \"Estación\"   \n",
       "\n",
       "   semanticSimilarity2  lexicalSimilarity2  \\\n",
       "0             0.948992            0.863636   \n",
       "1             0.734324            0.666667   \n",
       "2             1.000000            0.000000   \n",
       "3             0.739944            0.666667   \n",
       "4             0.710799            0.500000   \n",
       "\n",
       "                                          augmented3  semanticSimilarity3  \\\n",
       "0  \"He 35 años en el barrio y que el metro nos di...             0.955221   \n",
       "1                    \"con muchos bares interesantes\"             0.734324   \n",
       "2                                                \"…\"             1.000000   \n",
       "3                                     \"Muy comida..\"             0.739944   \n",
       "4                                        \"del.metro\"             0.768227   \n",
       "\n",
       "   lexicalSimilarity3  \n",
       "0            0.818182  \n",
       "1            0.666667  \n",
       "2            0.000000  \n",
       "3            0.666667  \n",
       "4            0.500000  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infoInvalidDF.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc57ade-e5d1-434b-bfa5-4c9e3479e4a4",
   "metadata": {},
   "source": [
    "Éste método es el que más variación genera de los anteriores tres."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b374395c-4c76-4e99-8dbe-38c9086eb863",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Combinación de reemplazo por sinónimos, inserción, intercambio y eliminacion \n",
    "Para intentar que haya la mayor variación posibles, manteniendo la semántica de los textos, se van a combinar los cuatro anteriores métodos para evitar un posible overfiting cuando se entrene al modelo con estos datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c6297a59-2139-4aa2-8f9c-229fd9c2cdc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#performs all EDA transformations\n",
    "def mixedEDAMethods(textList, prob, targetFolder, fileName, numVersion):\n",
    "    targetPath = targetFolder + \"/\" + fileName + str(numVersion) + \".txt\"\n",
    "    \n",
    "    newList = synonymReplacement(textList, prob, targetFolder + \"/1. Intermidiate Augmentation/\" + fileName + str(numVersion) + \"Synonym.txt\")\n",
    "    newList = randomInsertion(newList, targetFolder + \"/1. Intermidiate Augmentation/\" + fileName + str(numVersion) + \"Insertion.txt\")\n",
    "    newList = randomSwap(newList, targetFolder + \"/1. Intermidiate Augmentation/\" + fileName + str(numVersion) + \"Swap.txt\")\n",
    "    newList = randomDeletion(newList, targetPath)\n",
    "    \n",
    "    return newList"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67633a2e-b2a9-4415-96d6-eeafd9a72b6b",
   "metadata": {},
   "source": [
    "Se aplica éste método tres veces para generar más datos y despúes escoger los maś convenientes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "509fb018-860b-4d1f-9a27-4d4759283fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the original data\n",
    "validOriginalPath = \"/home/ibon/Documentos/GitHub/TFG/1. Data/4. Labeled Reviews/2. Without Emojis/ValidReviews.txt\"\n",
    "invalidOriginalPath = \"/home/ibon/Documentos/GitHub/TFG/1. Data/4. Labeled Reviews/2. Without Emojis/InvalidReviews.txt\"\n",
    "\n",
    "validOriginal = importFromTxtToList(validOriginalPath)\n",
    "invalidOriginal = importFromTxtToList(invalidOriginalPath) \n",
    "\n",
    "#Apply the synonym replacement 3 times to both datasets\n",
    "targetFolder = \"7. Mixed EDA/1. All Augmented Data\"\n",
    "for i in range(3):\n",
    "    prob = 0.25 + 2 * i / 10\n",
    "    \n",
    "    mixedEDAMethods(validOriginal, prob, targetFolder, \"validMixedEDAReviews\", i + 1)\n",
    "    mixedEDAMethods(invalidOriginal, prob, targetFolder, \"invalidMixedEDAReviews\", i + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c205ef54-5734-4365-99f6-8a1879b19dd3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Análisis de la combinación de los métodos y selección de los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb262dc-d4a3-4cc1-8969-671fc3a0a435",
   "metadata": {},
   "source": [
    "Importar los datos originales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1a447a13-7633-4141-8a99-7c031ba5b965",
   "metadata": {},
   "outputs": [],
   "source": [
    "validOriginalPath = \"/home/ibon/Documentos/GitHub/TFG/1. Data/4. Labeled Reviews/2. Without Emojis/ValidReviews.txt\"\n",
    "invalidOriginalPath = \"/home/ibon/Documentos/GitHub/TFG/1. Data/4. Labeled Reviews/2. Without Emojis/InvalidReviews.txt\"\n",
    "\n",
    "validOriginal = importFromTxtToList(validOriginalPath)\n",
    "invalidOriginal = importFromTxtToList(invalidOriginalPath) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29772b11-5391-48f4-81ce-15a1cecb8d5e",
   "metadata": {},
   "source": [
    "Importar los datasets generados en la eliminacion aleatoria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e0b18a26-da82-429f-b193-55dcc6d28e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import all the data \n",
    "validMixedEDAList = []\n",
    "invalidMixedEDAList = []\n",
    "for i in range(3):\n",
    "    validPath = f\"7. Mixed EDA/1. All Augmented Data/validMixedEDAReviews{i + 1}.txt\"\n",
    "    invalidPath = f\"7. Mixed EDA/1. All Augmented Data/invalidMixedEDAReviews{i + 1}.txt\"\n",
    "\n",
    "    validMixedEDAList.append(importFromTxtToList(validPath))\n",
    "    invalidMixedEDAList.append(importFromTxtToList(invalidPath))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994ce5c1-846c-48d4-94ff-4744398c2ca7",
   "metadata": {},
   "source": [
    "Realizar el análisis y la selección de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "87f2c572-71fa-4625-aeba-acf8323aae4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "validWithOriginalPath = '7. Mixed EDA/2. Augmented Data/ ValidMixedEDAWithOriginal.csv'\n",
    "validAugmentedPath = '7. Mixed EDA/2. Augmented Data/ValidMixedEDAData.txt'\n",
    "invalidWithOriginalPath = '7. Mixed EDA/2. Augmented Data/InvalidMixedEDAWithOriginal.csv'\n",
    "invalidAugmentedPath = '7. Mixed EDA/2. Augmented Data/InvalidMixedEDAData.txt'\n",
    "\n",
    "infoValid = processAugmentation(validOriginal, validMixedEDAList, validWithOriginalPath, validAugmentedPath)\n",
    "infoValidDF = pd.DataFrame(infoValid)\n",
    "infoInvalid = processAugmentation(invalidOriginal, invalidMixedEDAList, invalidWithOriginalPath, invalidAugmentedPath)\n",
    "infoInvalidDF = pd.DataFrame(infoInvalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2c41b7b7-8667-4524-84b0-a45cc710d913",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>originalText</th>\n",
       "      <th>bestAugmentation</th>\n",
       "      <th>bestAugmentedDataSemanticSimilarity</th>\n",
       "      <th>bestAugmentedDataLexicalSimiliratity</th>\n",
       "      <th>augmented1</th>\n",
       "      <th>semanticSimilarity1</th>\n",
       "      <th>lexicalSimilarity1</th>\n",
       "      <th>augmented2</th>\n",
       "      <th>semanticSimilarity2</th>\n",
       "      <th>lexicalSimilarity2</th>\n",
       "      <th>augmented3</th>\n",
       "      <th>semanticSimilarity3</th>\n",
       "      <th>lexicalSimilarity3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"Tiene fácil acceso para las personas con movi...</td>\n",
       "      <td>\"Tiene fácil movilidad para las salida pueblo ...</td>\n",
       "      <td>0.909179</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>\"tiene fácil acceso para las personas con movi...</td>\n",
       "      <td>0.958459</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>\"Tiene fácil movilidad para las salida pueblo ...</td>\n",
       "      <td>0.909179</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>\"Tiene del entrada para fácil las personas con...</td>\n",
       "      <td>0.849175</td>\n",
       "      <td>0.647059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"Espero que hayan mejorais\"</td>\n",
       "      <td>\"espero \"Espero que mejorais\"\"</td>\n",
       "      <td>0.858184</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>\"mejorais que hayan Espero\"</td>\n",
       "      <td>0.991634</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>\"espero \"Espero que mejorais\"\"</td>\n",
       "      <td>0.858184</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>\"que Espero hayan mejorais\"</td>\n",
       "      <td>0.994041</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"La estación es antigua, aparte de tener una s...</td>\n",
       "      <td>\"La (que antigua, a un lado de dar a subterran...</td>\n",
       "      <td>0.788468</td>\n",
       "      <td>0.483871</td>\n",
       "      <td>\"\"La estación sola pertenece aparte de ademas ...</td>\n",
       "      <td>0.858311</td>\n",
       "      <td>0.954545</td>\n",
       "      <td>\"\"La estación metro\" pueblo aparte de tener un...</td>\n",
       "      <td>0.878832</td>\n",
       "      <td>0.760000</td>\n",
       "      <td>\"La (que antigua, a un lado de dar a subterran...</td>\n",
       "      <td>0.788468</td>\n",
       "      <td>0.483871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"Bien\"</td>\n",
       "      <td>\"bien\"</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>\"bien\"</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>\"bien\"</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>\"\"Bien\"\"</td>\n",
       "      <td>0.963228</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"Bonito comodo\"</td>\n",
       "      <td>\"comodo comodo\"\"</td>\n",
       "      <td>0.803378</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>\"\"Bonito comodo\"\"</td>\n",
       "      <td>0.986783</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>\"\"Bonito comodo\"</td>\n",
       "      <td>0.991887</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>\"comodo comodo\"\"</td>\n",
       "      <td>0.803378</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        originalText  \\\n",
       "0  \"Tiene fácil acceso para las personas con movi...   \n",
       "1                        \"Espero que hayan mejorais\"   \n",
       "2  \"La estación es antigua, aparte de tener una s...   \n",
       "3                                             \"Bien\"   \n",
       "4                                    \"Bonito comodo\"   \n",
       "\n",
       "                                    bestAugmentation  \\\n",
       "0  \"Tiene fácil movilidad para las salida pueblo ...   \n",
       "1                     \"espero \"Espero que mejorais\"\"   \n",
       "2  \"La (que antigua, a un lado de dar a subterran...   \n",
       "3                                             \"bien\"   \n",
       "4                                   \"comodo comodo\"\"   \n",
       "\n",
       "   bestAugmentedDataSemanticSimilarity  bestAugmentedDataLexicalSimiliratity  \\\n",
       "0                             0.909179                              0.705882   \n",
       "1                             0.858184                              0.666667   \n",
       "2                             0.788468                              0.483871   \n",
       "3                             1.000000                              1.000000   \n",
       "4                             0.803378                              0.500000   \n",
       "\n",
       "                                          augmented1  semanticSimilarity1  \\\n",
       "0  \"tiene fácil acceso para las personas con movi...             0.958459   \n",
       "1                        \"mejorais que hayan Espero\"             0.991634   \n",
       "2  \"\"La estación sola pertenece aparte de ademas ...             0.858311   \n",
       "3                                             \"bien\"             1.000000   \n",
       "4                                  \"\"Bonito comodo\"\"             0.986783   \n",
       "\n",
       "   lexicalSimilarity1                                         augmented2  \\\n",
       "0            0.875000  \"Tiene fácil movilidad para las salida pueblo ...   \n",
       "1            1.000000                     \"espero \"Espero que mejorais\"\"   \n",
       "2            0.954545  \"\"La estación metro\" pueblo aparte de tener un...   \n",
       "3            1.000000                                             \"bien\"   \n",
       "4            1.000000                                   \"\"Bonito comodo\"   \n",
       "\n",
       "   semanticSimilarity2  lexicalSimilarity2  \\\n",
       "0             0.909179            0.705882   \n",
       "1             0.858184            0.666667   \n",
       "2             0.878832            0.760000   \n",
       "3             1.000000            1.000000   \n",
       "4             0.991887            1.000000   \n",
       "\n",
       "                                          augmented3  semanticSimilarity3  \\\n",
       "0  \"Tiene del entrada para fácil las personas con...             0.849175   \n",
       "1                        \"que Espero hayan mejorais\"             0.994041   \n",
       "2  \"La (que antigua, a un lado de dar a subterran...             0.788468   \n",
       "3                                           \"\"Bien\"\"             0.963228   \n",
       "4                                   \"comodo comodo\"\"             0.803378   \n",
       "\n",
       "   lexicalSimilarity3  \n",
       "0            0.647059  \n",
       "1            1.000000  \n",
       "2            0.483871  \n",
       "3            1.000000  \n",
       "4            0.500000  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infoValidDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d9c4a348-7e0d-4890-b257-86664bae6c27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>originalText</th>\n",
       "      <th>bestAugmentation</th>\n",
       "      <th>bestAugmentedDataSemanticSimilarity</th>\n",
       "      <th>bestAugmentedDataLexicalSimiliratity</th>\n",
       "      <th>augmented1</th>\n",
       "      <th>semanticSimilarity1</th>\n",
       "      <th>lexicalSimilarity1</th>\n",
       "      <th>augmented2</th>\n",
       "      <th>semanticSimilarity2</th>\n",
       "      <th>lexicalSimilarity2</th>\n",
       "      <th>augmented3</th>\n",
       "      <th>semanticSimilarity3</th>\n",
       "      <th>lexicalSimilarity3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"He vivido 35 años en el barrio y reconozco qu...</td>\n",
       "      <td>\"barrio desaparecido. 35 mucho tiempo en el pr...</td>\n",
       "      <td>0.897766</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>\"He vivido 35 años en el barrio desaparecido y...</td>\n",
       "      <td>0.919991</td>\n",
       "      <td>0.720000</td>\n",
       "      <td>\"tren 35 tiempo zonas en el mejoró y reconozco...</td>\n",
       "      <td>0.851737</td>\n",
       "      <td>0.720000</td>\n",
       "      <td>\"barrio desaparecido. 35 mucho tiempo en el pr...</td>\n",
       "      <td>0.897766</td>\n",
       "      <td>0.692308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"localización con muchos bares interesantes\"</td>\n",
       "      <td>\"localizacion bares con muchos \"localización\"</td>\n",
       "      <td>0.917731</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>\"interesantes con muchos localización interesa...</td>\n",
       "      <td>0.798462</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>\"bares con muchos localización interesantes\"</td>\n",
       "      <td>0.985325</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>\"localizacion bares con muchos \"localización\"</td>\n",
       "      <td>0.917731</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"…\"</td>\n",
       "      <td>\"…\"</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>\"…\"</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>\"…\"</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>\"…\"</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"Muy rica comida..\"</td>\n",
       "      <td>\"rica rica Muy\"</td>\n",
       "      <td>0.872533</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>\"Muy comida.. rica\"</td>\n",
       "      <td>0.980956</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>\"rica rica Muy\"</td>\n",
       "      <td>0.872533</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>\"\"Muy comida..\" rica\"</td>\n",
       "      <td>0.942068</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"Estación del.metro\"</td>\n",
       "      <td>\"Estación del.metro\"</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>\"Estación del.metro\"</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>\"del.metro Estación\"</td>\n",
       "      <td>0.988279</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>\"Estación del.metro\"</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        originalText  \\\n",
       "0  \"He vivido 35 años en el barrio y reconozco qu...   \n",
       "1       \"localización con muchos bares interesantes\"   \n",
       "2                                                \"…\"   \n",
       "3                                \"Muy rica comida..\"   \n",
       "4                               \"Estación del.metro\"   \n",
       "\n",
       "                                    bestAugmentation  \\\n",
       "0  \"barrio desaparecido. 35 mucho tiempo en el pr...   \n",
       "1      \"localizacion bares con muchos \"localización\"   \n",
       "2                                                \"…\"   \n",
       "3                                    \"rica rica Muy\"   \n",
       "4                               \"Estación del.metro\"   \n",
       "\n",
       "   bestAugmentedDataSemanticSimilarity  bestAugmentedDataLexicalSimiliratity  \\\n",
       "0                             0.897766                              0.692308   \n",
       "1                             0.917731                              0.666667   \n",
       "2                             1.000000                              0.000000   \n",
       "3                             0.872533                              0.666667   \n",
       "4                             1.000000                              1.000000   \n",
       "\n",
       "                                          augmented1  semanticSimilarity1  \\\n",
       "0  \"He vivido 35 años en el barrio desaparecido y...             0.919991   \n",
       "1  \"interesantes con muchos localización interesa...             0.798462   \n",
       "2                                                \"…\"             1.000000   \n",
       "3                                \"Muy comida.. rica\"             0.980956   \n",
       "4                               \"Estación del.metro\"             1.000000   \n",
       "\n",
       "   lexicalSimilarity1                                         augmented2  \\\n",
       "0            0.720000  \"tren 35 tiempo zonas en el mejoró y reconozco...   \n",
       "1            0.666667       \"bares con muchos localización interesantes\"   \n",
       "2            0.000000                                                \"…\"   \n",
       "3            1.000000                                    \"rica rica Muy\"   \n",
       "4            1.000000                               \"del.metro Estación\"   \n",
       "\n",
       "   semanticSimilarity2  lexicalSimilarity2  \\\n",
       "0             0.851737            0.720000   \n",
       "1             0.985325            1.000000   \n",
       "2             1.000000            0.000000   \n",
       "3             0.872533            0.666667   \n",
       "4             0.988279            1.000000   \n",
       "\n",
       "                                          augmented3  semanticSimilarity3  \\\n",
       "0  \"barrio desaparecido. 35 mucho tiempo en el pr...             0.897766   \n",
       "1      \"localizacion bares con muchos \"localización\"             0.917731   \n",
       "2                                                \"…\"             1.000000   \n",
       "3                              \"\"Muy comida..\" rica\"             0.942068   \n",
       "4                               \"Estación del.metro\"             1.000000   \n",
       "\n",
       "   lexicalSimilarity3  \n",
       "0            0.692308  \n",
       "1            0.666667  \n",
       "2            0.000000  \n",
       "3            1.000000  \n",
       "4            1.000000  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infoInvalidDF.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0b77fc-3e6b-4243-9bec-2b80e43eb8ff",
   "metadata": {},
   "source": [
    "Como se puede ver, los resultados generados son mucho más prometedores ya que genrean más variabilidad léxica pero mantienen bastante alta la similitud semántica."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60075ddb-5bec-430c-b98a-0f47115569e3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Albumentation\n",
    "Consiste en cambiar el orden de las frases de un texto y eliminar las repetidas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88180db0-8a89-48d9-97a2-00bfde609281",
   "metadata": {},
   "source": [
    "Función que obtiene las frases de un texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c9b1bae7-1671-484f-8e60-b505c0880ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getUniqueSentences(text):\n",
    "    # Remove quotes from the input line to ensure clean processing\n",
    "    newText = removeQuotes(text)\n",
    "    # Split the cleaned line into sentences using '.' as the delimiter and create a set comprehension to ensure unique sentences\n",
    "    sentencesSet = {sentence.strip() + \".\" for sentence in newText.split('.') if sentence.strip()}\n",
    "    # Return the set of unique sentences\n",
    "    \n",
    "    return sentencesSet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5cebdc-87bf-442c-b1bd-4596fd74270c",
   "metadata": {},
   "source": [
    "Función que cambia el orden de las frases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "da81cdc8-9f8f-4417-be3e-93c1261f6bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mixSentences(sentencesSet):\n",
    "    # Convert the input set of sentences into a list for shuffling\n",
    "    newList = list(sentencesSet)\n",
    "    # Shuffle the list in place to randomize the order of sentences\n",
    "    random.shuffle(newList)\n",
    "    # Join the shuffled sentences into a single string, adding quotes around it\n",
    "    return addQuotes(' '.join(newList))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9594a3-8ca5-4452-bf86-471fc4f7b025",
   "metadata": {},
   "source": [
    "Función que realiza la \"albumentation\" a todos lo elementos de una lista de textos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b9d20a6f-2aa0-49d5-a4b7-7dbc9f3437af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NLP albumentation method\n",
    "def albumentation(textList, targetPath):\n",
    "    #Open the target file\n",
    "    targetFile = open(targetPath, \"w\", encoding = 'utf-8')\n",
    "    \n",
    "    newList = []\n",
    "    for text in textList:\n",
    "        newText = mixSentences(getUniqueSentences(text))\n",
    "        newList.append(newText)\n",
    "        targetFile.write(newText + \"\\n\")\n",
    "\n",
    "    #Close the file\n",
    "    targetFile.close()\n",
    "    \n",
    "    return newList"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c296b1c-3dab-4ff2-8650-690c104afcc4",
   "metadata": {},
   "source": [
    "Se va a realizar este proceso tres veces para tener más datos y luego poder escoger los mejores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0fa6a53d-1d1b-414a-ae74-4f9237e996db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the original data\n",
    "validOriginalPath = \"/home/ibon/Documentos/GitHub/TFG/1. Data/4. Labeled Reviews/2. Without Emojis/ValidReviews.txt\"\n",
    "invalidOriginalPath = \"/home/ibon/Documentos/GitHub/TFG/1. Data/4. Labeled Reviews/2. Without Emojis/InvalidReviews.txt\"\n",
    "\n",
    "validOriginal = importFromTxtToList(validOriginalPath)\n",
    "invalidOriginal = importFromTxtToList(invalidOriginalPath) \n",
    "\n",
    "#Apply the synonym replacement 3 times to both datasets\n",
    "for i in range(3):\n",
    "    validPath = f\"8. Albumentation/1. All Augmented Data/validAlbumentationReviews{i + 1}.txt\"\n",
    "    invalidPath = f\"8. Albumentation/1. All Augmented Data/invalidAlbumentationReviews{i + 1}.txt\"\n",
    "\n",
    "    \n",
    "    albumentation(validOriginal, validPath)\n",
    "    albumentation(invalidOriginal, invalidPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7d2cb8-f762-4b5b-881b-f96bd94cbfc4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Análisis del \"albumentation\" y selección de los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3223603a-a7d8-462b-83bd-10480ea8e5c6",
   "metadata": {},
   "source": [
    "Importar los datos originales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cd004472-183c-48bb-ae0c-eb920d5de75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "validOriginalPath = \"/home/ibon/Documentos/GitHub/TFG/1. Data/4. Labeled Reviews/2. Without Emojis/ValidReviews.txt\"\n",
    "invalidOriginalPath = \"/home/ibon/Documentos/GitHub/TFG/1. Data/4. Labeled Reviews/2. Without Emojis/InvalidReviews.txt\"\n",
    "\n",
    "validOriginal = importFromTxtToList(validOriginalPath)\n",
    "invalidOriginal = importFromTxtToList(invalidOriginalPath) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a46d406-607c-4aa3-8496-b57415d869a6",
   "metadata": {},
   "source": [
    "Importar los datasets generados en el intercambio aleatorio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e9891032-8cd6-4362-abef-50c881ef8cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import all the data \n",
    "validAlbumentationList = []\n",
    "invalidAlbumentationList = []\n",
    "for i in range(3):\n",
    "    validPath = f\"8. Albumentation/1. All Augmented Data/validAlbumentationReviews{i + 1}.txt\"\n",
    "    invalidPath = f\"8. Albumentation/1. All Augmented Data/invalidAlbumentationReviews{i + 1}.txt\"\n",
    "\n",
    "    validAlbumentationList.append(importFromTxtToList(validPath))\n",
    "    invalidAlbumentationList.append(importFromTxtToList(invalidPath))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6bc1826-14b4-4c0a-813c-1b81ab7fa041",
   "metadata": {},
   "source": [
    "Realizar el análisis y la selección de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1aaa878b-cdf3-41ac-bb11-1d37460139a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "validWithOriginalPath = '8. Albumentation/2. Augmented Data/ ValidAlbumentationWithOriginal.csv'\n",
    "validAugmentedPath = '8. Albumentation/2. Augmented Data/ValidAlbumentationData.txt'\n",
    "invalidWithOriginalPath = '8. Albumentation/2. Augmented Data/InvalidAlbumentationWithOriginal.csv'\n",
    "invalidAugmentedPath = '8. Albumentation/2. Augmented Data/InvalidAlbumentationData.txt'\n",
    "\n",
    "infoValid = processAugmentation(validOriginal, validAlbumentationList, validWithOriginalPath, validAugmentedPath)\n",
    "infoValidDF = pd.DataFrame(infoValid)\n",
    "infoInvalid = processAugmentation(invalidOriginal, invalidAlbumentationList, invalidWithOriginalPath, invalidAugmentedPath)\n",
    "infoInvalidDF = pd.DataFrame(infoInvalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "718749de-5e1b-4063-83a1-338eadf04305",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>originalText</th>\n",
       "      <th>bestAugmentation</th>\n",
       "      <th>bestAugmentedDataSemanticSimilarity</th>\n",
       "      <th>bestAugmentedDataLexicalSimiliratity</th>\n",
       "      <th>augmented1</th>\n",
       "      <th>semanticSimilarity1</th>\n",
       "      <th>lexicalSimilarity1</th>\n",
       "      <th>augmented2</th>\n",
       "      <th>semanticSimilarity2</th>\n",
       "      <th>lexicalSimilarity2</th>\n",
       "      <th>augmented3</th>\n",
       "      <th>semanticSimilarity3</th>\n",
       "      <th>lexicalSimilarity3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"Tiene fácil acceso para las personas con movi...</td>\n",
       "      <td>\"Tiene fácil acceso para las personas con movi...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>\"Tiene fácil acceso para las personas con movi...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>\"Tiene fácil acceso para las personas con movi...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>\"Tiene fácil acceso para las personas con movi...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"Espero que hayan mejorais\"</td>\n",
       "      <td>\"Espero que hayan mejorais.\"</td>\n",
       "      <td>0.978180</td>\n",
       "      <td>1.0</td>\n",
       "      <td>\"Espero que hayan mejorais.\"</td>\n",
       "      <td>0.978180</td>\n",
       "      <td>1.0</td>\n",
       "      <td>\"Espero que hayan mejorais.\"</td>\n",
       "      <td>0.978180</td>\n",
       "      <td>1.0</td>\n",
       "      <td>\"Espero que hayan mejorais.\"</td>\n",
       "      <td>0.978180</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"La estación es antigua, aparte de tener una s...</td>\n",
       "      <td>\"La estación es antigua, aparte de tener una s...</td>\n",
       "      <td>0.998383</td>\n",
       "      <td>1.0</td>\n",
       "      <td>\"Además comunica por un pasillo subterráneo (q...</td>\n",
       "      <td>0.941534</td>\n",
       "      <td>1.0</td>\n",
       "      <td>\"La estación es antigua, aparte de tener una s...</td>\n",
       "      <td>0.998383</td>\n",
       "      <td>1.0</td>\n",
       "      <td>\"Además comunica por un pasillo subterráneo (q...</td>\n",
       "      <td>0.941534</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"Bien\"</td>\n",
       "      <td>\"Bien.\"</td>\n",
       "      <td>0.909274</td>\n",
       "      <td>1.0</td>\n",
       "      <td>\"Bien.\"</td>\n",
       "      <td>0.909274</td>\n",
       "      <td>1.0</td>\n",
       "      <td>\"Bien.\"</td>\n",
       "      <td>0.909274</td>\n",
       "      <td>1.0</td>\n",
       "      <td>\"Bien.\"</td>\n",
       "      <td>0.909274</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"Bonito comodo\"</td>\n",
       "      <td>\"Bonito comodo.\"</td>\n",
       "      <td>0.956156</td>\n",
       "      <td>1.0</td>\n",
       "      <td>\"Bonito comodo.\"</td>\n",
       "      <td>0.956156</td>\n",
       "      <td>1.0</td>\n",
       "      <td>\"Bonito comodo.\"</td>\n",
       "      <td>0.956156</td>\n",
       "      <td>1.0</td>\n",
       "      <td>\"Bonito comodo.\"</td>\n",
       "      <td>0.956156</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        originalText  \\\n",
       "0  \"Tiene fácil acceso para las personas con movi...   \n",
       "1                        \"Espero que hayan mejorais\"   \n",
       "2  \"La estación es antigua, aparte de tener una s...   \n",
       "3                                             \"Bien\"   \n",
       "4                                    \"Bonito comodo\"   \n",
       "\n",
       "                                    bestAugmentation  \\\n",
       "0  \"Tiene fácil acceso para las personas con movi...   \n",
       "1                       \"Espero que hayan mejorais.\"   \n",
       "2  \"La estación es antigua, aparte de tener una s...   \n",
       "3                                            \"Bien.\"   \n",
       "4                                   \"Bonito comodo.\"   \n",
       "\n",
       "   bestAugmentedDataSemanticSimilarity  bestAugmentedDataLexicalSimiliratity  \\\n",
       "0                             1.000000                                   1.0   \n",
       "1                             0.978180                                   1.0   \n",
       "2                             0.998383                                   1.0   \n",
       "3                             0.909274                                   1.0   \n",
       "4                             0.956156                                   1.0   \n",
       "\n",
       "                                          augmented1  semanticSimilarity1  \\\n",
       "0  \"Tiene fácil acceso para las personas con movi...             1.000000   \n",
       "1                       \"Espero que hayan mejorais.\"             0.978180   \n",
       "2  \"Además comunica por un pasillo subterráneo (q...             0.941534   \n",
       "3                                            \"Bien.\"             0.909274   \n",
       "4                                   \"Bonito comodo.\"             0.956156   \n",
       "\n",
       "   lexicalSimilarity1                                         augmented2  \\\n",
       "0                 1.0  \"Tiene fácil acceso para las personas con movi...   \n",
       "1                 1.0                       \"Espero que hayan mejorais.\"   \n",
       "2                 1.0  \"La estación es antigua, aparte de tener una s...   \n",
       "3                 1.0                                            \"Bien.\"   \n",
       "4                 1.0                                   \"Bonito comodo.\"   \n",
       "\n",
       "   semanticSimilarity2  lexicalSimilarity2  \\\n",
       "0             1.000000                 1.0   \n",
       "1             0.978180                 1.0   \n",
       "2             0.998383                 1.0   \n",
       "3             0.909274                 1.0   \n",
       "4             0.956156                 1.0   \n",
       "\n",
       "                                          augmented3  semanticSimilarity3  \\\n",
       "0  \"Tiene fácil acceso para las personas con movi...             1.000000   \n",
       "1                       \"Espero que hayan mejorais.\"             0.978180   \n",
       "2  \"Además comunica por un pasillo subterráneo (q...             0.941534   \n",
       "3                                            \"Bien.\"             0.909274   \n",
       "4                                   \"Bonito comodo.\"             0.956156   \n",
       "\n",
       "   lexicalSimilarity3  \n",
       "0                 1.0  \n",
       "1                 1.0  \n",
       "2                 1.0  \n",
       "3                 1.0  \n",
       "4                 1.0  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infoValidDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f15ddf63-4b1f-4f18-a7c9-eb28c3ae1fcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>originalText</th>\n",
       "      <th>bestAugmentation</th>\n",
       "      <th>bestAugmentedDataSemanticSimilarity</th>\n",
       "      <th>bestAugmentedDataLexicalSimiliratity</th>\n",
       "      <th>augmented1</th>\n",
       "      <th>semanticSimilarity1</th>\n",
       "      <th>lexicalSimilarity1</th>\n",
       "      <th>augmented2</th>\n",
       "      <th>semanticSimilarity2</th>\n",
       "      <th>lexicalSimilarity2</th>\n",
       "      <th>augmented3</th>\n",
       "      <th>semanticSimilarity3</th>\n",
       "      <th>lexicalSimilarity3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"He vivido 35 años en el barrio y reconozco qu...</td>\n",
       "      <td>\"He vivido 35 años en el barrio y reconozco qu...</td>\n",
       "      <td>0.995105</td>\n",
       "      <td>1.00</td>\n",
       "      <td>\"Una pena. La gente joven se ha ido a otras zo...</td>\n",
       "      <td>0.966767</td>\n",
       "      <td>1.00</td>\n",
       "      <td>\"He vivido 35 años en el barrio y reconozco qu...</td>\n",
       "      <td>0.995105</td>\n",
       "      <td>1.00</td>\n",
       "      <td>\"Desgraciadamente el barrio que conocí práctic...</td>\n",
       "      <td>0.959858</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"localización con muchos bares interesantes\"</td>\n",
       "      <td>\"localización con muchos bares interesantes.\"</td>\n",
       "      <td>0.972995</td>\n",
       "      <td>1.00</td>\n",
       "      <td>\"localización con muchos bares interesantes.\"</td>\n",
       "      <td>0.972995</td>\n",
       "      <td>1.00</td>\n",
       "      <td>\"localización con muchos bares interesantes.\"</td>\n",
       "      <td>0.972995</td>\n",
       "      <td>1.00</td>\n",
       "      <td>\"localización con muchos bares interesantes.\"</td>\n",
       "      <td>0.972995</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"…\"</td>\n",
       "      <td>\"….\"</td>\n",
       "      <td>0.911401</td>\n",
       "      <td>0.00</td>\n",
       "      <td>\"….\"</td>\n",
       "      <td>0.911401</td>\n",
       "      <td>0.00</td>\n",
       "      <td>\"….\"</td>\n",
       "      <td>0.911401</td>\n",
       "      <td>0.00</td>\n",
       "      <td>\"….\"</td>\n",
       "      <td>0.911401</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"Muy rica comida..\"</td>\n",
       "      <td>\"Muy rica comida.\"</td>\n",
       "      <td>0.995217</td>\n",
       "      <td>1.00</td>\n",
       "      <td>\"Muy rica comida.\"</td>\n",
       "      <td>0.995217</td>\n",
       "      <td>1.00</td>\n",
       "      <td>\"Muy rica comida.\"</td>\n",
       "      <td>0.995217</td>\n",
       "      <td>1.00</td>\n",
       "      <td>\"Muy rica comida.\"</td>\n",
       "      <td>0.995217</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"Estación del.metro\"</td>\n",
       "      <td>\"Estación del. metro.\"</td>\n",
       "      <td>0.987054</td>\n",
       "      <td>0.25</td>\n",
       "      <td>\"Estación del. metro.\"</td>\n",
       "      <td>0.987054</td>\n",
       "      <td>0.25</td>\n",
       "      <td>\"metro. Estación del.\"</td>\n",
       "      <td>0.973336</td>\n",
       "      <td>0.25</td>\n",
       "      <td>\"metro. Estación del.\"</td>\n",
       "      <td>0.973336</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        originalText  \\\n",
       "0  \"He vivido 35 años en el barrio y reconozco qu...   \n",
       "1       \"localización con muchos bares interesantes\"   \n",
       "2                                                \"…\"   \n",
       "3                                \"Muy rica comida..\"   \n",
       "4                               \"Estación del.metro\"   \n",
       "\n",
       "                                    bestAugmentation  \\\n",
       "0  \"He vivido 35 años en el barrio y reconozco qu...   \n",
       "1      \"localización con muchos bares interesantes.\"   \n",
       "2                                               \"….\"   \n",
       "3                                 \"Muy rica comida.\"   \n",
       "4                             \"Estación del. metro.\"   \n",
       "\n",
       "   bestAugmentedDataSemanticSimilarity  bestAugmentedDataLexicalSimiliratity  \\\n",
       "0                             0.995105                                  1.00   \n",
       "1                             0.972995                                  1.00   \n",
       "2                             0.911401                                  0.00   \n",
       "3                             0.995217                                  1.00   \n",
       "4                             0.987054                                  0.25   \n",
       "\n",
       "                                          augmented1  semanticSimilarity1  \\\n",
       "0  \"Una pena. La gente joven se ha ido a otras zo...             0.966767   \n",
       "1      \"localización con muchos bares interesantes.\"             0.972995   \n",
       "2                                               \"….\"             0.911401   \n",
       "3                                 \"Muy rica comida.\"             0.995217   \n",
       "4                             \"Estación del. metro.\"             0.987054   \n",
       "\n",
       "   lexicalSimilarity1                                         augmented2  \\\n",
       "0                1.00  \"He vivido 35 años en el barrio y reconozco qu...   \n",
       "1                1.00      \"localización con muchos bares interesantes.\"   \n",
       "2                0.00                                               \"….\"   \n",
       "3                1.00                                 \"Muy rica comida.\"   \n",
       "4                0.25                             \"metro. Estación del.\"   \n",
       "\n",
       "   semanticSimilarity2  lexicalSimilarity2  \\\n",
       "0             0.995105                1.00   \n",
       "1             0.972995                1.00   \n",
       "2             0.911401                0.00   \n",
       "3             0.995217                1.00   \n",
       "4             0.973336                0.25   \n",
       "\n",
       "                                          augmented3  semanticSimilarity3  \\\n",
       "0  \"Desgraciadamente el barrio que conocí práctic...             0.959858   \n",
       "1      \"localización con muchos bares interesantes.\"             0.972995   \n",
       "2                                               \"….\"             0.911401   \n",
       "3                                 \"Muy rica comida.\"             0.995217   \n",
       "4                             \"metro. Estación del.\"             0.973336   \n",
       "\n",
       "   lexicalSimilarity3  \n",
       "0                1.00  \n",
       "1                1.00  \n",
       "2                0.00  \n",
       "3                1.00  \n",
       "4                0.25  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infoInvalidDF.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8662052-876d-407b-9db9-570d9f50d44f",
   "metadata": {},
   "source": [
    "En este caso, la similitud semántica varía ligeramente, pero la similitud léxica permanece intacta, ya que no se introducen nuevas palabras ni se eliminan palabras existentes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0435858-3d91-4c26-a0c3-ddd562b2f189",
   "metadata": {},
   "source": [
    "# Aumento de datos con modelos preentrenados\n",
    "Para realizar esta sección del aumento de datos se va a seguir el artículo \"Dara Augmentation Using Pre-trained Transformer Models\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32776d58-350a-4f0e-8338-99a923e7ac05",
   "metadata": {},
   "source": [
    "## Imports necesarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a180ef1d-4397-41b9-8671-e41d2189a835",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import DatasetDict, Dataset, load_dataset, concatenate_datasets\n",
    "import random\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments, DataCollatorForLanguageModeling, T5ForConditionalGeneration, T5Tokenizer, Seq2SeqTrainer, Seq2SeqTrainingArguments, DataCollatorForSeq2Seq "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a450b091-5e0a-4175-a700-9ae4ea0ab2ab",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## GPT-2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2210ffd3-07b4-43b1-b3e3-5a41be7d64c7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Funciones que se van a utilizar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6dc65e9-c5a9-45d1-8fb2-1688c854d0b7",
   "metadata": {},
   "source": [
    "Separación del conjunto de datos en: train, test y validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "facd5799-9d02-4e7e-bc0f-78597bedbacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainTestValidationSplit(dataset, trainPctg = 0.8):\n",
    "    datasetSplit = dataset.train_test_split(test_size = 1 - trainPctg, seed = 54)\n",
    "    validationTestSplit = datasetSplit[\"test\"].train_test_split(test_size = 0.5, seed = 54)\n",
    "    \n",
    "    return DatasetDict({\n",
    "        \"train\" : datasetSplit[\"train\"],\n",
    "        \"test\" : validationTestSplit[\"test\"],\n",
    "        \"validation\" : validationTestSplit[\"train\"]\n",
    "        })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6770b298-849c-4a8f-9964-3891a3f5c301",
   "metadata": {},
   "source": [
    "Importar datos de un fichero (donde los datos no tienen etiquetas) a una lista de diccionarios de la forma: \"texto\" : texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81ebefd8-df8f-4f7a-b23e-191870f8c790",
   "metadata": {},
   "outputs": [],
   "source": [
    "def importFromTxtToDictList(source):\n",
    "    dictList = []\n",
    "    with open(source, 'r', encoding=\"utf-8\") as file:\n",
    "        for line in file: \n",
    "            dictList.append({\"text\" : line.strip()})\n",
    "            \n",
    "    return dictList"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a52cab1-67a9-4e0d-a671-9ef393051308",
   "metadata": {},
   "source": [
    "Importar datos con etiquetas, donde las etiquetas están dadas por la carpeta de origen. Se genera una lista de diccionarios de la forma: \"texto\" : label + texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b457facd-03bf-4e06-8a57-c82c1b602ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def importFromTxtToDictListWithLabel(source, label):\n",
    "    dictList = []\n",
    "    \n",
    "    #Open and store evety line in the file in a list\n",
    "    with open(source, \"r\", encoding = 'utf-8') as dataFile:\n",
    "        for line in dataFile:\n",
    "            dictList.append({\"text\" : label + \" : \" + line.strip()})\n",
    "\n",
    "    return dictList"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7deab60d-bb4d-44f4-a313-c30cff4bb158",
   "metadata": {},
   "source": [
    "Importar datos de dos ficheros (donde los datos no tienen etiquetas) a una lista de diccionarios de la forma: \"original\" : texto , \"parafrasis\" : texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78866bd8-a643-47cd-995b-32c75bcb4822",
   "metadata": {},
   "outputs": [],
   "source": [
    "def importParaphraseFromTxtToDictList(originalPath, paraphrasePath):\n",
    "    dictList = []\n",
    "    \n",
    "    #As both files have the same number of lines, they can be iterated simultaneously\n",
    "    with open(originalPath, 'r', encoding = 'utf-8') as originalFile, open(paraphrasePath, 'r', encoding = 'utf-8') as paraphraseFile:\n",
    "        for originalLine, paraphraseLine in zip(originalFile, paraphraseFile): \n",
    "            dictList.append({\"original\" : originalLine.strip(), \"paraphrase\" : paraphraseLine.strip()})\n",
    "            \n",
    "    return dictList"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a6eda8-2165-4b1a-904c-e9937573c0ea",
   "metadata": {},
   "source": [
    "Importar datos de dos ficheros (donde los datos no tienen etiquetas) a una lista de diccionarios de la forma: \"original\" : label + texto , \"parafrasis\" : label + texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7525af30-8022-4283-a098-5903c87d69fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def importParaphraseFromTxtToDictListWithLabel(originalPath, paraphrasePath, label):\n",
    "    dictList = []\n",
    "    \n",
    "    #As both files have the same number of lines, they can be iterated simultaneously\n",
    "    with open(originalPath, 'r', encoding = 'utf-8') as originalFile, open(paraphrasePath, 'r', encoding = 'utf-8') as paraphraseFile:\n",
    "        for originalLine, paraphraseLine in zip(originalFile, paraphraseFile): \n",
    "            dictList.append({\"original\" : label + originalLine.strip(), \"paraphrase\" : label + paraphraseLine.strip()})\n",
    "            \n",
    "    return dictList"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b964738-2a4b-4acd-be87-bdfe68b65388",
   "metadata": {},
   "source": [
    "Tokenización de los datos donde solo se tiene en cuenta la clave text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e1c3f690-437c-4a9b-8870-8240e877d331",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizeFunction(examples, tokenizer):\n",
    "    return tokenizer(examples[\"text\"], truncation = True, padding = \"max_length\", max_length = 512)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0b4e4d-a125-468b-be43-21137b38295d",
   "metadata": {},
   "source": [
    "Tokenización de datos para paráfrasis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4545162d-2d65-4d35-9501-7c4ec2717760",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizeFunctionParaphrases(examples, tokenizer):\n",
    "    return tokenizer(\n",
    "        f\"<start> Original: {examples[\"original\"]} <sep> Paráfrasis: {examples[\"paraphrase\"]} <end>\", \n",
    "        truncation = True, padding = \"max_length\", max_length = 512)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9b4977-7364-4b95-8b08-3f5833515ec3",
   "metadata": {},
   "source": [
    "Entrenamiento de un modelo GPT2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "799935c0-7f19-42d8-acbf-a6b0eb08527a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainModel(savingPath, modelName, version, tokenizeFunction, dataDict, aditionalTokens = None, trainEpochs = 3, lr = 5e-5, freezePctg = 0):\n",
    "    #Download the pretrained model and the tokenizer\n",
    "    model = AutoModelForCausalLM.from_pretrained(modelName)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(modelName)\n",
    "    \n",
    "    #Check if the tokenizer has a padding token\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.add_special_tokens({'pad_token': '<|pad|>'})\n",
    "        model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "    #if necessary add more tokens\n",
    "    if aditionalTokens != None:\n",
    "        tokenizer.add_special_tokens(aditionalTokens)\n",
    "        model.resize_token_embeddings(len(tokenizer))\n",
    "        \n",
    "    tokenizedDataDict = dataDict.map(lambda x: tokenizeFunction(x, tokenizer), batched = False, load_from_cache_file = False)\n",
    "\n",
    "    #If necessary freeze the layers that do not have to be trained\n",
    "    if freezePctg > 0:\n",
    "        for name, param in model.named_parameters():\n",
    "            if 'transformer.h.' in name:  # GPT2 has blocks called 'transformer.h.X'\n",
    "                layer_num = int(name.split('.')[2])\n",
    "                if layer_num < model.config.n_layer * freezePctg:  #Freeze 80 % of the layers\n",
    "                    param.requires_grad = False\n",
    "\n",
    "    # Define training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir = savingPath + \"/\" + modelName.replace(\"/\", \"%\"),\n",
    "        overwrite_output_dir = True,\n",
    "        num_train_epochs = trainEpochs,\n",
    "        per_device_train_batch_size = 8,\n",
    "        logging_steps = 10,\n",
    "        eval_strategy = \"epoch\",\n",
    "        save_strategy = \"no\",\n",
    "        fp16 = True,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        weight_decay=0.01,                \n",
    "        warmup_steps=500,  \n",
    "        learning_rate = lr,\n",
    "        logging_dir = savingPath + \"/\" + modelName.replace(\"/\", \"%\"),\n",
    "    )\n",
    "\n",
    "    dataCollator = DataCollatorForLanguageModeling(\n",
    "        tokenizer = tokenizer,\n",
    "        mlm = False,  # Is not a mask lenguage model\n",
    "    )\n",
    "\n",
    "    #Generates the labels automatically so that the model can predict the next token\n",
    "    trainer = Trainer(\n",
    "        model = model,\n",
    "        args = training_args,\n",
    "        train_dataset = tokenizedDataDict[\"train\"],\n",
    "        eval_dataset = tokenizedDataDict[\"validation\"],\n",
    "        data_collator = dataCollator,\n",
    "    )\n",
    "\n",
    "    #Train the model\n",
    "    torch.cuda.empty_cache()\n",
    "    trainer.train()\n",
    "\n",
    "    #Save the model\n",
    "    tokenizer.save_pretrained(savingPath + \"/\" + modelName.replace(\"/\", \"%\") + \"v\" + str(version))\n",
    "    model.save_pretrained(savingPath + \"/\" + modelName.replace(\"/\", \"%\") + \"v\" + str(version))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba3536d-5598-4c48-b3a6-51dcb2f85a06",
   "metadata": {},
   "source": [
    "Generación de texto dado un modelo GPT2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "89e4d7e2-ea32-49fd-a245-5048766ebfef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateText(modelPath, text):\n",
    "    #Load the model\n",
    "    model = AutoModelForCausalLM.from_pretrained(modelPath)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(modelPath)\n",
    "\n",
    "    #Check if the tokenizer has a padding token\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.add_special_tokens({'pad_token': '<|pad|>'})\n",
    "        model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "    #Tokenize the input text\n",
    "    inputIds = tokenizer.encode(text, return_tensors = \"pt\")\n",
    "    inputLength = inputIds.shape[-1]\n",
    "\n",
    "    #Generate the text\n",
    "    output = model.generate(inputIds, max_new_tokens = inputLength, num_return_sequences = 1, \n",
    "                            pad_token_id = tokenizer.pad_token_id, eos_token_id = tokenizer.eos_token_id, \n",
    "                            temperature = 0.7, top_k = 50, top_p = 0.9, repetition_penalty = 2.0,\n",
    "                            no_repeat_ngram_size = 3,)\n",
    "\n",
    "    #Decode the text\n",
    "    generatedText = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "    return generatedText"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b7aba1b-546b-4383-a70c-a03230e56bca",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Primer enfoque\n",
    "Se van a hacer el refinamiento del modelo preentrenado GPT-2. El modelo es una versión ligera de GPT2 devido a las limitaciones de HW.\n",
    "\n",
    "El propósito es adaptar el modelo a nuestro conjunto de datos y que sea capaz de generar una frase nueva semánticamente similar a la frase de entrada pero léxicamente distinta."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f687342-de46-4cf0-a719-fb094e94904e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Preparación de los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72cbce5b-7a34-4a19-842b-a136e82db731",
   "metadata": {},
   "source": [
    "Se importan los datos sin etiquetas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b9778118-9f5f-418b-bd3c-45aa40085eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/home/ibon/Documentos/GitHub/TFG/1. Data/4. Labeled Reviews/2. Without Emojis/AllUnlabeledReviews.txt\"\n",
    "\n",
    "#Each element is a dictionary with the key text and the text as the value\n",
    "dataDictList = importFromTxtToDictList(path)\n",
    "#The list of dictionaries is converted to a dataset\n",
    "dataset = Dataset.from_list(dataDictList)\n",
    "\n",
    "#Split the data\n",
    "dataDict = trainTestValidationSplit(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ef673400-22ed-4f0b-86cd-70d0aa220f05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 2587\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 324\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 323\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(dataDict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fcd8818-7f68-4760-96cb-eb14e759d2ca",
   "metadata": {},
   "source": [
    "Se va a generar un segundo conjunto de datos siguiendo el artículo: Data Augmentation Using Pre-trained Transformer Models. Se van a anteponer las etiquetas de las clases a los ejemplos de las clases. Es decir, los datos que se van a proporcionar al modelo tendrán la siguiente forma: etiqueta : texto.\n",
    "\n",
    "Originalmente, las etiquetas del cojunto de datos eran v (valida) y n (no válida). Sin embargo, para proporcionar más contexto semántico al modelo, se van a extender a valida y noValida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ecb7f9d2-ace5-49a7-a57b-d234ed330324",
   "metadata": {},
   "outputs": [],
   "source": [
    "validOriginalPath = \"/home/ibon/Documentos/GitHub/TFG/1. Data/4. Labeled Reviews/2. Without Emojis/ValidReviews.txt\"\n",
    "invalidOriginalPath = \"/home/ibon/Documentos/GitHub/TFG/1. Data/4. Labeled Reviews/2. Without Emojis/InvalidReviews.txt\"\n",
    "\n",
    "validReviewsLabeledDictList = importFromTxtToDictListWithLabel(validOriginalPath, \"valida\")\n",
    "invalidReviewsLabeledDictList = importFromTxtToDictListWithLabel(invalidOriginalPath, \"noValida\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cbe761b3-562e-428c-84d0-d956e3d15d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Each element is a dictionary with the key text and the text as the value\n",
    "dataLabeledDictList = validReviewsLabeledDictList + invalidReviewsLabeledDictList\n",
    "random.shuffle(dataLabeledDictList)\n",
    "\n",
    "#The list of dictionaries is converted to a dataset\n",
    "labeledDataset = Dataset.from_list(dataLabeledDictList)\n",
    "\n",
    "#Split the data\n",
    "labeledDataDict = trainTestValidationSplit(labeledDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5ac912d1-1557-4e97-b71b-e69f8c1dc643",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 2587\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 324\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 323\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(labeledDataDict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8f256f-9cab-40a8-8ba4-112e66c23357",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Entrenamiento de los modelos\n",
    "Se van a entrenar varios modelos con distintas características para luego poder compararlos entre sí"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c697657c-2bf4-4de1-aa4a-4d95f8008d23",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n",
      "Map: 100%|██████████| 2587/2587 [00:00<00:00, 10062.28 examples/s]\n",
      "Map: 100%|██████████| 324/324 [00:00<00:00, 9121.11 examples/s]\n",
      "Map: 100%|██████████| 323/323 [00:00<00:00, 9594.35 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='243' max='243' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [243/243 04:57, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>18.704100</td>\n",
       "      <td>4.752882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>17.701900</td>\n",
       "      <td>4.576190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>17.280200</td>\n",
       "      <td>4.401620</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 2587/2587 [00:00<00:00, 9547.95 examples/s]\n",
      "Map: 100%|██████████| 324/324 [00:00<00:00, 8677.48 examples/s]\n",
      "Map: 100%|██████████| 323/323 [00:00<00:00, 8722.32 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='486' max='486' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [486/486 09:56, Epoch 6/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>18.703700</td>\n",
       "      <td>4.753025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>17.702000</td>\n",
       "      <td>4.576076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>17.279400</td>\n",
       "      <td>4.401365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>16.816200</td>\n",
       "      <td>4.259889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>15.998100</td>\n",
       "      <td>4.146826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>15.587200</td>\n",
       "      <td>4.070008</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 2587/2587 [00:00<00:00, 9488.50 examples/s]\n",
      "Map: 100%|██████████| 324/324 [00:00<00:00, 8794.00 examples/s]\n",
      "Map: 100%|██████████| 323/323 [00:00<00:00, 8908.85 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='243' max='243' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [243/243 05:12, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>18.416500</td>\n",
       "      <td>4.656086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>16.855100</td>\n",
       "      <td>4.327623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>16.225000</td>\n",
       "      <td>4.114830</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 2587/2587 [00:00<00:00, 9889.17 examples/s] \n",
      "Map: 100%|██████████| 324/324 [00:00<00:00, 8640.57 examples/s]\n",
      "Map: 100%|██████████| 323/323 [00:00<00:00, 9144.52 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='486' max='486' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [486/486 10:26, Epoch 6/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>18.416700</td>\n",
       "      <td>4.655893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>16.855300</td>\n",
       "      <td>4.327700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>16.225300</td>\n",
       "      <td>4.114779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>15.769900</td>\n",
       "      <td>4.009274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>15.062500</td>\n",
       "      <td>3.952060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>14.589300</td>\n",
       "      <td>3.918522</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 2587/2587 [00:00<00:00, 9553.05 examples/s]\n",
      "Map: 100%|██████████| 324/324 [00:00<00:00, 8523.67 examples/s]\n",
      "Map: 100%|██████████| 323/323 [00:00<00:00, 8950.46 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='243' max='243' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [243/243 05:34, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>17.897000</td>\n",
       "      <td>4.489497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>16.002000</td>\n",
       "      <td>4.087089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>15.524200</td>\n",
       "      <td>3.963442</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 2587/2587 [00:00<00:00, 9443.52 examples/s]\n",
      "Map: 100%|██████████| 324/324 [00:00<00:00, 8499.31 examples/s]\n",
      "Map: 100%|██████████| 323/323 [00:00<00:00, 8512.69 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='486' max='486' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [486/486 11:10, Epoch 6/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>17.897500</td>\n",
       "      <td>4.489340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>16.001500</td>\n",
       "      <td>4.087051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>15.523800</td>\n",
       "      <td>3.963464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>15.153800</td>\n",
       "      <td>3.903233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>14.366200</td>\n",
       "      <td>3.873145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>13.727400</td>\n",
       "      <td>3.865109</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 2587/2587 [00:00<00:00, 9517.79 examples/s] \n",
      "Map: 100%|██████████| 324/324 [00:00<00:00, 7863.27 examples/s]\n",
      "Map: 100%|██████████| 323/323 [00:00<00:00, 8595.49 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='243' max='243' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [243/243 04:57, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>18.418700</td>\n",
       "      <td>4.706368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>16.400300</td>\n",
       "      <td>4.033558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>14.936700</td>\n",
       "      <td>3.736642</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 2587/2587 [00:00<00:00, 9312.07 examples/s]\n",
      "Map: 100%|██████████| 324/324 [00:00<00:00, 7886.13 examples/s]\n",
      "Map: 100%|██████████| 323/323 [00:00<00:00, 8225.37 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='486' max='486' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [486/486 09:56, Epoch 6/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>18.418300</td>\n",
       "      <td>4.706165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>16.400100</td>\n",
       "      <td>4.033539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>14.936500</td>\n",
       "      <td>3.736605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>14.048200</td>\n",
       "      <td>3.570991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>13.265700</td>\n",
       "      <td>3.478842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>12.701900</td>\n",
       "      <td>3.434006</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 2587/2587 [00:00<00:00, 9090.88 examples/s]\n",
      "Map: 100%|██████████| 324/324 [00:00<00:00, 7854.41 examples/s]\n",
      "Map: 100%|██████████| 323/323 [00:00<00:00, 8433.15 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='243' max='243' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [243/243 05:12, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>17.319700</td>\n",
       "      <td>4.353747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>14.889400</td>\n",
       "      <td>3.646231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>13.825600</td>\n",
       "      <td>3.468897</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 2587/2587 [00:00<00:00, 9045.61 examples/s]\n",
      "Map: 100%|██████████| 324/324 [00:00<00:00, 7742.54 examples/s]\n",
      "Map: 100%|██████████| 323/323 [00:00<00:00, 8441.72 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='486' max='486' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [486/486 10:26, Epoch 6/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>17.319600</td>\n",
       "      <td>4.353586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>14.889600</td>\n",
       "      <td>3.646158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>13.825400</td>\n",
       "      <td>3.469061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>13.259400</td>\n",
       "      <td>3.405213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>12.564400</td>\n",
       "      <td>3.367789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>12.000400</td>\n",
       "      <td>3.349270</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 2587/2587 [00:00<00:00, 9078.06 examples/s]\n",
      "Map: 100%|██████████| 324/324 [00:00<00:00, 7207.89 examples/s]\n",
      "Map: 100%|██████████| 323/323 [00:00<00:00, 7921.46 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='243' max='243' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [243/243 05:34, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>15.405300</td>\n",
       "      <td>3.800304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>14.120800</td>\n",
       "      <td>3.473247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>13.443000</td>\n",
       "      <td>3.396134</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 2587/2587 [00:00<00:00, 8859.61 examples/s]\n",
      "Map: 100%|██████████| 324/324 [00:00<00:00, 7605.90 examples/s]\n",
      "Map: 100%|██████████| 323/323 [00:00<00:00, 8153.20 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='486' max='486' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [486/486 11:10, Epoch 6/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>15.405000</td>\n",
       "      <td>3.800235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>14.120900</td>\n",
       "      <td>3.473225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>13.443600</td>\n",
       "      <td>3.396276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>12.833800</td>\n",
       "      <td>3.348695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>12.098500</td>\n",
       "      <td>3.325533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>11.417000</td>\n",
       "      <td>3.328719</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "savingPath = \"/home/ibon/Documentos/1. Models/GPT2/1. Generation\"\n",
    "modelName = \"mrm8488/spanish-gpt2\"\n",
    "\n",
    "#Possible characteristics of the models\n",
    "dataDictList = [dataDict, labeledDataDict]\n",
    "freezePctgList = [0.8, 0.5, 0]\n",
    "trainEpochsList = [3, 6]\n",
    "\n",
    "#Version control\n",
    "version = 0\n",
    "\n",
    "for i, dataDict in enumerate(dataDictList):\n",
    "    for freezePctg in freezePctgList:\n",
    "        for trainEpochs in trainEpochsList:\n",
    "            trainModel(savingPath, modelName, version, tokenizeFunction, dataDict, trainEpochs = trainEpochs, freezePctg = freezePctg)\n",
    "\n",
    "            with open(savingPath + \"/VersionControlGPT2Generation.csv\", \"a\") as file:\n",
    "                data = \"unlabeled \\n\" if i == 0 else \"labeled \\n\"\n",
    "\n",
    "                #Update the version control\n",
    "                file.write(modelName.replace(\"/\", \"%\") + \"v\" + str(version) + \",\" + str(version) + \",\" + str(freezePctg) + \",\" + str(trainEpochs) + \",\" + data)\n",
    "\n",
    "            version += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6faad526-5ec9-48ef-9488-1977d3bebd97",
   "metadata": {},
   "source": [
    "#### Probar los modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7accdd75-7e83-40a8-af07-e699046da08a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ibon/Programas/anaconda3/envs/TFG/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/ibon/Programas/anaconda3/envs/TFG/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo: mrm8488%spanish-gpt2v0\n",
      "\n",
      "Texto: Parafrasea: suele haber demasiada gente en el metro, pero no hay nadie que se ocupe de los trenes\n",
      "\n",
      "Modelo: mrm8488%spanish-gpt2v1\n",
      "\n",
      "Texto: Parafrasea: suele haber demasiada gente en el metro, pero hay que tener cuidado de no perder la calma\n",
      "\n",
      "Modelo: mrm8488%spanish-gpt2v2\n",
      "\n",
      "Texto: Parafrasea: suele haber demasiada gente en el metro, pero no hay que olvidar la estación de metro.\n",
      "\n",
      "Modelo: mrm8488%spanish-gpt2v3\n",
      "\n",
      "Texto: Parafrasea: suele haber demasiada gente en el metro, por lo que es una estación muy transitada.\n",
      "\n",
      "Modelo: mrm8488%spanish-gpt2v4\n",
      "\n",
      "Texto: Parafrasea: suele haber demasiada gente en el metro, y no hay suficientes autobuses para todos los pasajeros.\n",
      "\n",
      "Modelo: mrm8488%spanish-gpt2v5\n",
      "\n",
      "Texto: Parafrasea: suele haber demasiada gente en el metro, por lo que hay que ir con cuidado.\"\n",
      "\n",
      "Modelo: mrm8488%spanish-gpt2v6\n",
      "\n",
      "Texto: Parafrasea: suele haber demasiada gente en el metro, pero no hay nadie que se quede con la boca\n",
      "\n",
      "Modelo: mrm8488%spanish-gpt2v7\n",
      "\n",
      "Texto: Parafrasea: suele haber demasiada gente en el metro.\"No hay suficientes personas para hacer un buen trabajo\n",
      "\n",
      "Modelo: mrm8488%spanish-gpt2v8\n",
      "\n",
      "Texto: Parafrasea: suele haber demasiada gente en el metro, y no hay suficientes autobuses para todos los pasajeros.\n",
      "\n",
      "Modelo: mrm8488%spanish-gpt2v9\n",
      "\n",
      "Texto: Parafrasea: suele haber demasiada gente en el metro, y no hay suficientes autobuses para todos los pasajeros.\n",
      "\n",
      "Modelo: mrm8488%spanish-gpt2v10\n",
      "\n",
      "Texto: Parafrasea: suele haber demasiada gente en el metro, y no hay suficientes autobuses para todos los pasajeros.\n",
      "\n",
      "Modelo: mrm8488%spanish-gpt2v11\n",
      "\n",
      "Texto: Parafrasea: suele haber demasiada gente en el metro, y no hay suficientes autobuses para todos los pasajeros.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "savingPath = \"/home/ibon/Documentos/1. Models/GPT2/1. Generation\"\n",
    "text = \"Parafrasea: suele haber demasiada gente en el metro\"\n",
    "\n",
    "#For every generated model\n",
    "with open(savingPath + \"/VersionControlGPT2Generation.csv\", \"r\") as file:\n",
    "    #Ignore the first line of the file\n",
    "    next(file)\n",
    "\n",
    "    for line in file:\n",
    "        elemsList = line.split(\",\")\n",
    "        \n",
    "        modelName = elemsList[0]\n",
    "        modelPath = savingPath + \"/\" + modelName\n",
    "\n",
    "        #Generate the text\n",
    "        generatedText = generateText(modelPath, text)\n",
    "\n",
    "        print(\"Modelo: \" +  modelName + \"\\n\")\n",
    "        print(\"Texto: \" + generatedText + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501acb59-441d-4d41-a9e0-6830ba92108d",
   "metadata": {},
   "source": [
    "Ninguno de los modelos es capaz de parafrasear, todos completan la frase de entrada. Esto se debe a que no se ha entrenado el modelo para tal tarea. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5701cff-7edb-4c8c-a732-a64e4cd498f2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Segundo Enfoque\n",
    "En este caso, se van a entrenar los modelos de forma explícita para el parafraseo. Es decir, los datos de entrenamiento se van a organizar en pares de frases: original - parafraseo.\n",
    "\n",
    "Dado que hemos aplicado técnicas de aumento de datos, disponemos de bastantes pares de frases. \n",
    "\n",
    "Se han aplicado 3 variaciones de la retrotraducción, por lo que solo con esto, se disponen de tres paraes de frases por cada elemento del conjunto."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef42d7b8-3991-45b1-b4c4-5d0d53134415",
   "metadata": {},
   "source": [
    "#### Preparación de los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2d0c8d-fbfa-4a88-9f3a-a1f5198dd2ac",
   "metadata": {},
   "source": [
    "Se importan los datos de las 3 variaciones de la retrotraducción sin tener en cuenta las etiquetas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ac2d775a-69f5-4845-8dd5-5b4496fe18f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "invalidOriginalPath = \"/home/ibon/Documentos/GitHub/TFG/1. Data/4. Labeled Reviews/2. Without Emojis/InvalidReviews.txt\"\n",
    "validOriginalPath = \"/home/ibon/Documentos/GitHub/TFG/1. Data/4. Labeled Reviews/2. Without Emojis/ValidReviews.txt\"\n",
    "\n",
    "backtranslationFolder = \"/home/ibon/Documentos/GitHub/TFG/2. Review Classifier/1. Data Augmentation/1. Back Translation/1. Google Translator\"\n",
    "invalidBacktranslationPath1 = backtranslationFolder + \"/InvalidReviewsTranslationsEsEnEnEs.txt\"\n",
    "invalidBacktranslationPath2 = backtranslationFolder + \"/InvalidReviewsTranslationsEsFrFrJaJaRuRuEs.txt\"\n",
    "invalidBacktranslationPath3 = backtranslationFolder + \"/InvalidReviewsTranslationsEsJaJaEs.txt\"\n",
    "validBacktranslationPath1 = backtranslationFolder + \"/ValidReviewsTranslationsEsEnEnEs.txt\"\n",
    "validBacktranslationPath2 = backtranslationFolder + \"/ValidReviewsTranslationsEsFrFrJaJaRuRuEs.txt\"\n",
    "validBacktranslationPath3 = backtranslationFolder + \"/ValidReviewsTranslationsEsJaJaEs.txt\"\n",
    "\n",
    "invalidParaphrasesDictList1 = importParaphraseFromTxtToDictList(invalidOriginalPath, invalidBacktranslationPath1)\n",
    "invalidParaphrasesDictList2 = importParaphraseFromTxtToDictList(invalidOriginalPath, invalidBacktranslationPath2)\n",
    "invalidParaphrasesDictList3 = importParaphraseFromTxtToDictList(invalidOriginalPath, invalidBacktranslationPath3)\n",
    "validParaphrasesDictList1 = importParaphraseFromTxtToDictList(validOriginalPath, validBacktranslationPath1)\n",
    "validParaphrasesDictList2 = importParaphraseFromTxtToDictList(validOriginalPath, validBacktranslationPath2)\n",
    "validParaphrasesDictList3 = importParaphraseFromTxtToDictList(validOriginalPath, validBacktranslationPath3)\n",
    "\n",
    "#Join all the lists\n",
    "paraphrasesDictList = invalidParaphrasesDictList1 + invalidParaphrasesDictList2 + invalidParaphrasesDictList3 + validParaphrasesDictList1 + validParaphrasesDictList2 + validParaphrasesDictList3\n",
    "#Shuffle the lists\n",
    "random.shuffle(paraphrasesDictList)\n",
    "\n",
    "#Convert it to a Hugging Face dataset\n",
    "paraphrasesDataset = Dataset.from_list(paraphrasesDictList)\n",
    "\n",
    "#Split the data\n",
    "paraphrasesDataDict = trainTestValidationSplit(paraphrasesDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0acae1c7-fd93-4757-81c8-01ddae7e55bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['original', 'paraphrase'],\n",
      "        num_rows: 7761\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['original', 'paraphrase'],\n",
      "        num_rows: 971\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['original', 'paraphrase'],\n",
      "        num_rows: 970\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(paraphrasesDataDict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aef9aa4-d721-4d9a-99e1-ec2d1da909af",
   "metadata": {},
   "source": [
    "Se importan los datos de las 3 variaciones de la retrotraducción teniendo en cuenta las etiquetas. \n",
    "\n",
    "Originalmente, las etiquetas del cojunto de datos eran v (valida) y n (no válida). Sin embargo, para proporcionar más contexto semántico al modelo, se van a extender a valida y noValida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1c854eb0-f21a-4080-9770-f1867e66f82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "invalidOriginalPath = \"/home/ibon/Documentos/GitHub/TFG/1. Data/4. Labeled Reviews/2. Without Emojis/InvalidReviews.txt\"\n",
    "validOriginalPath = \"/home/ibon/Documentos/GitHub/TFG/1. Data/4. Labeled Reviews/2. Without Emojis/ValidReviews.txt\"\n",
    "\n",
    "backtranslationFolder = \"/home/ibon/Documentos/GitHub/TFG/2. Review Classifier/1. Data Augmentation/1. Back Translation/1. Google Translator\"\n",
    "invalidBacktranslationPath1 = backtranslationFolder + \"/InvalidReviewsTranslationsEsEnEnEs.txt\"\n",
    "invalidBacktranslationPath2 = backtranslationFolder + \"/InvalidReviewsTranslationsEsFrFrJaJaRuRuEs.txt\"\n",
    "invalidBacktranslationPath3 = backtranslationFolder + \"/InvalidReviewsTranslationsEsJaJaEs.txt\"\n",
    "validBacktranslationPath1 = backtranslationFolder + \"/ValidReviewsTranslationsEsEnEnEs.txt\"\n",
    "validBacktranslationPath2 = backtranslationFolder + \"/ValidReviewsTranslationsEsFrFrJaJaRuRuEs.txt\"\n",
    "validBacktranslationPath3 = backtranslationFolder + \"/ValidReviewsTranslationsEsJaJaEs.txt\"\n",
    "\n",
    "invalidLabeledParaphrasesDictList1 = importParaphraseFromTxtToDictListWithLabel(invalidOriginalPath, invalidBacktranslationPath1, \"noValida\")\n",
    "invalidLabeledParaphrasesDictList2 = importParaphraseFromTxtToDictListWithLabel(invalidOriginalPath, invalidBacktranslationPath2, \"noValida\")\n",
    "invalidLabeledParaphrasesDictList3 = importParaphraseFromTxtToDictListWithLabel(invalidOriginalPath, invalidBacktranslationPath3, \"noValida\")\n",
    "validLabeledParaphrasesDictList1 = importParaphraseFromTxtToDictListWithLabel(validOriginalPath, validBacktranslationPath1, \"valida\")\n",
    "validLabeledParaphrasesDictList2 = importParaphraseFromTxtToDictListWithLabel(validOriginalPath, validBacktranslationPath2, \"valida\")\n",
    "validLabeledParaphrasesDictList3 = importParaphraseFromTxtToDictListWithLabel(validOriginalPath, validBacktranslationPath3, \"valida\")\n",
    "\n",
    "#Join all the lists\n",
    "labeledParaphrasesDictList = invalidLabeledParaphrasesDictList1 + invalidLabeledParaphrasesDictList2 + invalidLabeledParaphrasesDictList3 + validLabeledParaphrasesDictList1 + validLabeledParaphrasesDictList2 + validLabeledParaphrasesDictList3\n",
    "#Shuffle the lists\n",
    "random.shuffle(labeledParaphrasesDictList)\n",
    "\n",
    "#Convert it to a Hugging Face dataset\n",
    "labeledParaphrasesDataset = Dataset.from_list(labeledParaphrasesDictList)\n",
    "\n",
    "#Split the data\n",
    "labeledParaphrasesDataDict = trainTestValidationSplit(labeledParaphrasesDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a2960f96-3bc1-4695-bb24-8c94dd9fa1b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['original', 'paraphrase'],\n",
      "        num_rows: 7761\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['original', 'paraphrase'],\n",
      "        num_rows: 971\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['original', 'paraphrase'],\n",
      "        num_rows: 970\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(labeledParaphrasesDataDict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b64fd5-b37c-4c0e-97a7-71a929360d43",
   "metadata": {},
   "source": [
    "#### Entrenamiento de los modelos\n",
    "Se van a entrenar varios modelos con distintas características para luego poder compararlos entre sí"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c05eecb7-44c0-4ed9-925c-ad4fec6baf05",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n",
      "Map: 100%|██████████| 7761/7761 [00:01<00:00, 3986.18 examples/s]\n",
      "Map: 100%|██████████| 971/971 [00:00<00:00, 3932.73 examples/s]\n",
      "Map: 100%|██████████| 970/970 [00:00<00:00, 3918.43 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='726' max='726' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [726/726 14:17, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>12.078500</td>\n",
       "      <td>3.017042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>10.306700</td>\n",
       "      <td>2.605485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>9.327900</td>\n",
       "      <td>2.290071</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 7761/7761 [00:01<00:00, 3972.11 examples/s]\n",
      "Map: 100%|██████████| 971/971 [00:00<00:00, 3993.00 examples/s]\n",
      "Map: 100%|██████████| 970/970 [00:00<00:00, 3981.81 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1452' max='1452' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1452/1452 28:39, Epoch 5/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>12.078500</td>\n",
       "      <td>3.016943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>10.307600</td>\n",
       "      <td>2.605726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>9.101600</td>\n",
       "      <td>2.228823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>8.621400</td>\n",
       "      <td>2.174544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>8.181700</td>\n",
       "      <td>2.151142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>7.977200</td>\n",
       "      <td>2.143585</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 7761/7761 [00:01<00:00, 3945.48 examples/s]\n",
      "Map: 100%|██████████| 971/971 [00:00<00:00, 3897.22 examples/s]\n",
      "Map: 100%|██████████| 970/970 [00:00<00:00, 3883.51 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='726' max='726' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [726/726 15:03, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>10.955200</td>\n",
       "      <td>2.753172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>9.708000</td>\n",
       "      <td>2.452509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>8.582900</td>\n",
       "      <td>2.128568</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 7761/7761 [00:01<00:00, 3934.46 examples/s]\n",
      "Map: 100%|██████████| 971/971 [00:00<00:00, 3986.33 examples/s]\n",
      "Map: 100%|██████████| 970/970 [00:00<00:00, 4022.08 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1452' max='1452' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1452/1452 30:08, Epoch 5/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>10.955200</td>\n",
       "      <td>2.753142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>9.710700</td>\n",
       "      <td>2.453190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>8.413400</td>\n",
       "      <td>2.082076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>7.815300</td>\n",
       "      <td>2.022706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>7.308500</td>\n",
       "      <td>1.994706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>7.066000</td>\n",
       "      <td>1.986546</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 7761/7761 [00:01<00:00, 3986.17 examples/s]\n",
      "Map: 100%|██████████| 971/971 [00:00<00:00, 3955.17 examples/s]\n",
      "Map: 100%|██████████| 970/970 [00:00<00:00, 3957.56 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='726' max='726' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [726/726 16:09, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>10.488100</td>\n",
       "      <td>2.635873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>9.100400</td>\n",
       "      <td>2.265548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>7.977300</td>\n",
       "      <td>2.008270</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 7761/7761 [00:01<00:00, 3932.96 examples/s]\n",
      "Map: 100%|██████████| 971/971 [00:00<00:00, 3893.24 examples/s]\n",
      "Map: 100%|██████████| 970/970 [00:00<00:00, 3903.44 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1452' max='1452' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1452/1452 32:20, Epoch 5/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>10.483400</td>\n",
       "      <td>2.634787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>9.084100</td>\n",
       "      <td>2.259301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>7.820500</td>\n",
       "      <td>1.962439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>7.041800</td>\n",
       "      <td>1.889323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>6.418600</td>\n",
       "      <td>1.850762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>5.995400</td>\n",
       "      <td>1.838795</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 7761/7761 [00:01<00:00, 3973.31 examples/s]\n",
      "Map: 100%|██████████| 971/971 [00:00<00:00, 3722.36 examples/s]\n",
      "Map: 100%|██████████| 970/970 [00:00<00:00, 4013.15 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='726' max='726' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [726/726 14:19, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>11.720500</td>\n",
       "      <td>2.889414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>9.871200</td>\n",
       "      <td>2.410148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>8.668300</td>\n",
       "      <td>2.120625</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 7761/7761 [00:01<00:00, 3904.31 examples/s]\n",
      "Map: 100%|██████████| 971/971 [00:00<00:00, 3581.51 examples/s]\n",
      "Map: 100%|██████████| 970/970 [00:00<00:00, 3865.23 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1452' max='1452' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1452/1452 28:39, Epoch 5/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>11.718400</td>\n",
       "      <td>2.888865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>9.870200</td>\n",
       "      <td>2.409793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>8.463400</td>\n",
       "      <td>2.065821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>7.835800</td>\n",
       "      <td>2.017475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>7.691500</td>\n",
       "      <td>1.993318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>7.784100</td>\n",
       "      <td>1.986082</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 7761/7761 [00:01<00:00, 3934.54 examples/s]\n",
      "Map: 100%|██████████| 971/971 [00:00<00:00, 3755.71 examples/s]\n",
      "Map: 100%|██████████| 970/970 [00:00<00:00, 4009.45 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='726' max='726' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [726/726 15:03, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>10.393600</td>\n",
       "      <td>2.564446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>9.321100</td>\n",
       "      <td>2.276402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>8.022400</td>\n",
       "      <td>1.968417</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 7761/7761 [00:02<00:00, 3880.05 examples/s]\n",
      "Map: 100%|██████████| 971/971 [00:00<00:00, 3701.67 examples/s]\n",
      "Map: 100%|██████████| 970/970 [00:00<00:00, 3940.34 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1452' max='1452' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1452/1452 30:07, Epoch 5/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>10.393600</td>\n",
       "      <td>2.564410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>9.321600</td>\n",
       "      <td>2.276447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>7.846500</td>\n",
       "      <td>1.921246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>7.093000</td>\n",
       "      <td>1.861821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>6.862300</td>\n",
       "      <td>1.834509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>6.919800</td>\n",
       "      <td>1.826032</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 7761/7761 [00:01<00:00, 3889.18 examples/s]\n",
      "Map: 100%|██████████| 971/971 [00:00<00:00, 3612.87 examples/s]\n",
      "Map: 100%|██████████| 970/970 [00:00<00:00, 3863.29 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='726' max='726' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [726/726 16:09, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>9.867100</td>\n",
       "      <td>2.440304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>8.709900</td>\n",
       "      <td>2.118285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>7.448900</td>\n",
       "      <td>1.846492</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 7761/7761 [00:01<00:00, 3929.15 examples/s]\n",
      "Map: 100%|██████████| 971/971 [00:00<00:00, 3729.27 examples/s]\n",
      "Map: 100%|██████████| 970/970 [00:00<00:00, 3935.85 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1452' max='1452' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1452/1452 32:20, Epoch 5/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>9.865300</td>\n",
       "      <td>2.439883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>8.708000</td>\n",
       "      <td>2.117530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>7.292000</td>\n",
       "      <td>1.804644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>6.356300</td>\n",
       "      <td>1.734845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5.996300</td>\n",
       "      <td>1.700825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>5.971100</td>\n",
       "      <td>1.691810</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "savingPath = \"/home/ibon/Documentos/1. Models/GPT2/2. Paraphrasing\"\n",
    "modelName = \"mrm8488/spanish-gpt2\"\n",
    "\n",
    "#Generate the aditional tokens for the separation of the original and paraphrased texts\n",
    "aditionalTokens = {'additional_special_tokens': ['<start>', '<sep>', '<end>']}\n",
    "\n",
    "#Possible characteristics of the models\n",
    "dataDictList = [paraphrasesDataDict, labeledParaphrasesDataDict]\n",
    "freezePctgList = [0.8, 0.5, 0]\n",
    "trainEpochsList = [3, 6]\n",
    "\n",
    "#Version control\n",
    "version = 0\n",
    "\n",
    "for i, dataDict in enumerate(dataDictList):\n",
    "    for freezePctg in freezePctgList:\n",
    "        for trainEpochs in trainEpochsList:\n",
    "            trainModel(savingPath, modelName, version, tokenizeFunctionParaphrases, dataDict, aditionalTokens = aditionalTokens, trainEpochs = trainEpochs, freezePctg = freezePctg)\n",
    "\n",
    "            with open(savingPath + \"/VersionControlGPT2Paraphrasing.csv\", \"a\") as file:\n",
    "                data = \"unlabeled \\n\" if i == 0 else \"labeled \\n\"\n",
    "\n",
    "                #Update the version control\n",
    "                file.write(modelName.replace(\"/\", \"%\") + \"v\" + str(version) + \",\" + str(version) + \",\" + str(freezePctg) + \",\" + str(trainEpochs) + \",\" + data)\n",
    "\n",
    "            version += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd9c5a4-b0e2-4d84-8de7-080317bfd86d",
   "metadata": {},
   "source": [
    "#### Probar los modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fd4aceb2-d3ec-4de9-955f-61c11f7099cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ibon/Programas/anaconda3/envs/TFG/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/ibon/Programas/anaconda3/envs/TFG/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo: mrm8488%spanish-gpt2v0\n",
      "\n",
      "Texto:  Original: El metro es rapido  Paráfrasis:  Parámonos en el Metro de Madrid, cerca del aeropuerto. En\n",
      "\n",
      "Modelo: mrm8488%spanish-gpt2v0\n",
      "\n",
      "Texto:  Original: El metro es rapido  Paráfrasis:  Parámonos en el Metro de Madrid, cerca del aeropuerto. En\n",
      "\n",
      "Modelo: mrm8488%spanish-gpt2v1\n",
      "\n",
      "Texto:  Original: El metro es rapido  Paráfrasis:  Parámonos en la estación de Metro. No hay trenes directos desde\n",
      "\n",
      "Modelo: mrm8488%spanish-gpt2v2\n",
      "\n",
      "Texto:  Original: El metro es rapido  Paráfrasis:  Parágrafo: La línea de metro está en construcción. Es una\n",
      "\n",
      "Modelo: mrm8488%spanish-gpt2v3\n",
      "\n",
      "Texto:  Original: El metro es rapido  Paráfrasis:  Parágrafo: La línea de metro no tiene cobertura. Es una\n",
      "\n",
      "Modelo: mrm8488%spanish-gpt2v4\n",
      "\n",
      "Texto:  Original: El metro es rapido  Paráfrasis:  que el metro no tiene paradas en la estación de tren. Es una pena\n",
      "\n",
      "Modelo: mrm8488%spanish-gpt2v5\n",
      "\n",
      "Texto:  Original: El metro es rapido  Paráfrasis:  que el metro no tiene paradas en las vías. Es una estación de metro\n",
      "\n",
      "Modelo: mrm8488%spanish-gpt2v6\n",
      "\n",
      "Texto:  Original: El metro es rapido  Paráfrasis:  Paráfarás: Aparecerá en pantalla una estación de metro.\n",
      "\n",
      "Modelo: mrm8488%spanish-gpt2v7\n",
      "\n",
      "Texto:  Original: El metro es rapido  Paráfrasis:  Parámonos en el Metro de Madrid. Es una estación muy transi\n",
      "\n",
      "Modelo: mrm8488%spanish-gpt2v8\n",
      "\n",
      "Texto:  Original: El metro es rapido  Paráfrasis:  Parágrafo: La línea de metro rápido está en construcción. Es\n",
      "\n",
      "Modelo: mrm8488%spanish-gpt2v9\n",
      "\n",
      "Texto:  Original: El metro es rapido  Paráfrasis:  Parás: el metro va rápido. Es una pena que no\n",
      "\n",
      "Modelo: mrm8488%spanish-gpt2v10\n",
      "\n",
      "Texto:  Original: El metro es rapido  Paráfrasis:  Hoods son rápidos, no hay prisa. Es un tren de cercanías que\n",
      "\n",
      "Modelo: mrm8488%spanish-gpt2v11\n",
      "\n",
      "Texto:  Original: El metro es rapido  Paráfrasis:  Paráspedes: el metro es rápido. ITALI\n",
      "\n"
     ]
    }
   ],
   "source": [
    "savingPath = \"/home/ibon/Documentos/1. Models/GPT2/2. Paraphrasing\"\n",
    "text = \"<start> Original: El metro es rapido <sep> Paráfrasis: \"\n",
    "\n",
    "#For every generated model\n",
    "with open(savingPath + \"/VersionControlGPT2Paraphrasing.csv\", \"r\") as file:\n",
    "    #Ignore the first line of the file\n",
    "    next(file)\n",
    "\n",
    "    for line in file:\n",
    "        elemsList = line.split(\",\")\n",
    "        \n",
    "        modelName = elemsList[0]\n",
    "        modelPath = savingPath + \"/\" + modelName\n",
    "\n",
    "        #Generate the text\n",
    "        generatedText = generateText(modelPath, text)\n",
    "\n",
    "        print(\"Modelo: \" +  modelName + \"\\n\")\n",
    "        print(\"Texto: \" + generatedText + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb29636-4b22-47ad-94bd-faf8b511bcf1",
   "metadata": {},
   "source": [
    "Como se puede ver, los resultados siguen siendo muy malos. El modelo no genera la paráfrasis y muchas veces genera texto incoherente. Esto puede deberse a varias razones: el formato y la calidad de los datos de entrenamientos no son lo suficinetemente buenos o cuantiosos, los hiperparámetros de generación no son óptimos o la opción más probable: el modelo base es inadecuado (GPT2 no fue diseñado para paráfrasis)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47be6fba-408d-4b85-aaea-24dfcc00e2dc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## T5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e2bade-c626-46d1-b8a3-cfa4dc30a9d6",
   "metadata": {},
   "source": [
    "Dado que GPT2 no está optimizado para parafraser, sino que se usa para generar texto, se va a usar un modelo text2text llamado t5 y desarrollado por Google.\n",
    "\n",
    "Como las versiones de t5 normales proporcionadas por Google no estan pensadas para usarse en castellano, se va a usar una versión mejorada y con más idiomas (incluido el castellano), llamado flan-t5, que además está entrenado para realizar más tareas sin necesidad de hacer fine-tuning (aunque esto mejora el rendimiento)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf220a6-1fb5-4706-aeb9-8d8417c387b5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Funciones que se van a utilizar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b960f30e-281a-45d5-94ff-e7eb2361df37",
   "metadata": {},
   "source": [
    "Separación del conjunto de datos en: train, test y validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a27e16f-fdde-4d63-b660-b2a45e82a46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainTestValidationSplit(dataset, trainPctg = 0.8):\n",
    "    datasetSplit = dataset.train_test_split(test_size = 1 - trainPctg, seed = 54)\n",
    "    validationTestSplit = datasetSplit[\"test\"].train_test_split(test_size = 0.5, seed = 54)\n",
    "    \n",
    "    return DatasetDict({\n",
    "        \"train\" : datasetSplit[\"train\"],\n",
    "        \"test\" : validationTestSplit[\"test\"],\n",
    "        \"validation\" : validationTestSplit[\"train\"]\n",
    "        })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17325f9-2121-43d0-b4b9-f4265a76a330",
   "metadata": {},
   "source": [
    "Importar datos de dos ficheros (donde los datos no tienen etiquetas) a una lista de diccionarios de la forma: \"original\" : texto , \"parafrasis\" : texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2475129c-9f3a-472d-98be-e8ea961b7199",
   "metadata": {},
   "outputs": [],
   "source": [
    "def importParaphraseFromTxtToDictList(originalPath, paraphrasePath):\n",
    "    dictList = []\n",
    "    \n",
    "    #As both files have the same number of lines, they can be iterated simultaneously\n",
    "    with open(originalPath, 'r', encoding = 'utf-8') as originalFile, open(paraphrasePath, 'r', encoding = 'utf-8') as paraphraseFile:\n",
    "        for originalLine, paraphraseLine in zip(originalFile, paraphraseFile): \n",
    "            dictList.append({\"sentence1\" : originalLine.strip(), \"sentence2\" : paraphraseLine.strip()})\n",
    "            \n",
    "    return dictList"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f397d0-3cd7-47d6-abdd-360746e5bebf",
   "metadata": {},
   "source": [
    "Importación y filtrado de PAWSX (dataset de Google), de forma que se obtienen las frases en castellano y únicamente los datos que son paráfrasis (etiqueta a 1). Además, se van a eliminar las columnas de id y label, ya que no serán necesarias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "feaa9fbc-0862-41da-9212-da912f917088",
   "metadata": {},
   "outputs": [],
   "source": [
    "def processPAWSX():\n",
    "    #Import the dataset with the spanish phrases\n",
    "    datasetDict = load_dataset('paws-x', 'es')\n",
    "\n",
    "    #Create an empty dataset to save the paraphrases\n",
    "    newDatasetDict = DatasetDict()\n",
    "\n",
    "    #Filter the paraphrases\n",
    "    for key, valueDataset in datasetDict.items():\n",
    "        paraphraseDataset = valueDataset.filter(lambda example: example['label'] == 1)\n",
    "\n",
    "        paraphraseDataset = paraphraseDataset.remove_columns([\"id\", \"label\"])\n",
    "    \n",
    "        newDatasetDict[key] = paraphraseDataset\n",
    "    \n",
    "    return newDatasetDict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686308d4-38f2-40ad-a4a2-ae0bdf1ee596",
   "metadata": {},
   "source": [
    "Tokenización y preprocesamiento de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66c2d532-fc88-4d64-86a3-543a320ca77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizeFunctionFlanT5(examples, tokenizer):\n",
    "    #Add the instruction to all the sentecnces for the model to understand what it has to do\n",
    "    inputs = [f\"Reescribe esta frase: {text}\" for text in examples[\"sentence1\"]]\n",
    "\n",
    "    #Indicate that the targets are the paraphrased sentences\n",
    "    targets = examples[\"sentence2\"]\n",
    "\n",
    "    #Tokenize the input\n",
    "    model_inputs = tokenizer(inputs, max_length = 512, truncation = True, padding = \"max_length\")\n",
    "\n",
    "    #Tokenize the output\n",
    "    labels = tokenizer(targets, max_length = 512, truncation = True, padding = \"max_length\").input_ids\n",
    "\n",
    "    #Add the expected output to the input of the model\n",
    "    model_inputs[\"labels\"] = labels\n",
    "    \n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d6347c-a052-478d-833d-888e0b903984",
   "metadata": {},
   "source": [
    "Entrenamiento para un modelo flan-t5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3fc9537b-ffb6-4e26-8120-08893d61fb42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainFlanT5Model(savingPath, modelName, version, tokenizeFunction, dataDict, aditionalTokens = None, trainEpochs = 3, lr = 5e-5, freezePctg = 0):\n",
    "    #Download the pretrained model and the tokenizer\n",
    "    model = T5ForConditionalGeneration.from_pretrained(modelName)\n",
    "    tokenizer = T5Tokenizer.from_pretrained(modelName)\n",
    "    \n",
    "    #Check if the tokenizer has a padding token\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.add_special_tokens({'pad_token': '<|pad|>'})\n",
    "        model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "    #if necessary add more tokens\n",
    "    if aditionalTokens != None:\n",
    "        tokenizer.add_special_tokens(aditionalTokens)\n",
    "        model.resize_token_embeddings(len(tokenizer))\n",
    "        \n",
    "    tokenizedDataDict = dataDict.map(lambda x: tokenizeFunction(x, tokenizer), batched = True, load_from_cache_file = False)\n",
    "\n",
    "    #Remove the sentece1 and sentence2 columns\n",
    "    newTokenizedDataDict = DatasetDict()\n",
    "    \n",
    "    for key in tokenizedDataDict.keys():\n",
    "        newTokenizedData = tokenizedDataDict[key].remove_columns([\"sentence1\", \"sentence2\"])\n",
    "    \n",
    "        newTokenizedDataDict[key] = newTokenizedData\n",
    "\n",
    "    tokenizedDataDict = newTokenizedDataDict\n",
    "\n",
    "    #If necessary freeze the layers that do not have to be trained\n",
    "    if freezePctg > 0:\n",
    "        #Number of encoder and decoder layers\n",
    "        numEncoderLayers = len(model.encoder.block)\n",
    "        numDecoderLayers = len(model.decoder.block)\n",
    "\n",
    "        #Compute how many layers have to be frozen\n",
    "        numFreezeEncoder = int(freezePctg * numEncoderLayers)  \n",
    "        numFreezeDencoder = int(freezePctg * numDecoderLayers)  \n",
    "        \n",
    "        # Congelar las capas del encoder\n",
    "        for i, layer in enumerate(model.encoder.block):\n",
    "            if i < numFreezeEncoder:\n",
    "                for param in layer.parameters():\n",
    "                    param.requires_grad = False\n",
    "        \n",
    "        # Congelar las capas del decoder\n",
    "        for i, layer in enumerate(model.decoder.block):\n",
    "            if i < numFreezeDencoder:\n",
    "                for param in layer.parameters():\n",
    "                    param.requires_grad = False\n",
    "\n",
    "\n",
    "    # Define training arguments\n",
    "    training_args = Seq2SeqTrainingArguments(\n",
    "        output_dir = savingPath + \"/\" + modelName.replace(\"/\", \"%\"),\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        learning_rate = lr,\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=8,\n",
    "        num_train_epochs = trainEpochs,\n",
    "        weight_decay=0.01,\n",
    "        save_strategy = \"no\",\n",
    "        predict_with_generate = True,\n",
    "        logging_dir = savingPath + \"/\" + modelName.replace(\"/\", \"%\"),\n",
    "        logging_steps = 10,\n",
    "        do_train=True,\n",
    "        do_eval=True, \n",
    "        fp16 = False,\n",
    "        overwrite_output_dir = True,\n",
    "        gradient_accumulation_steps = 4,             \n",
    "        warmup_steps = 500, \n",
    "    )\n",
    "\n",
    "    dataCollator = DataCollatorForSeq2Seq(\n",
    "        tokenizer = tokenizer,\n",
    "        model = model,\n",
    "        padding = True,\n",
    "    )\n",
    "\n",
    "    #Generates the labels automatically so that the model can predict the next token\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model = model,\n",
    "        args = training_args,\n",
    "        train_dataset = tokenizedDataDict[\"train\"],\n",
    "        eval_dataset = tokenizedDataDict[\"validation\"],\n",
    "        data_collator = dataCollator,\n",
    "    )\n",
    "\n",
    "    #Train the model\n",
    "    torch.cuda.empty_cache()\n",
    "    trainer.train()\n",
    "\n",
    "    #Save the model\n",
    "    tokenizer.save_pretrained(savingPath + \"/\" + modelName.replace(\"/\", \"%\") + \"v\" + str(version))\n",
    "    model.save_pretrained(savingPath + \"/\" + modelName.replace(\"/\", \"%\") + \"v\" + str(version))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05532bd8-bf60-4784-9cd2-1edb12626327",
   "metadata": {},
   "source": [
    "Generación de texto dado un modelo flan t5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dd5d8ff4-33be-45fa-b0f3-6a5e49e9f90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateTextFlanT5(modelPath, text):\n",
    "    #Load the model\n",
    "    model = T5ForConditionalGeneration.from_pretrained(modelPath)\n",
    "    tokenizer = T5Tokenizer.from_pretrained(modelPath)\n",
    "\n",
    "    #Check if the tokenizer has a padding token\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.add_special_tokens({'pad_token': '<|pad|>'})\n",
    "        model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "    #Tokenize the input text\n",
    "    inputIds = tokenizer(text, return_tensors = \"pt\").input_ids\n",
    "    inputLength = inputIds.shape[-1]\n",
    "\n",
    "    #Generate the text\n",
    "    outputs = model.generate(inputIds, num_return_sequences = 5, max_length = inputLength, do_sample = True, num_beams = 5, early_stopping = True, repetition_penalty = 1.5, no_repeat_ngram_size = 2, top_k = 50, top_p = 0.95, temperature = 1.5,)\n",
    "\n",
    "    #Decode the text\n",
    "    generatedText = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    return generatedText"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f9fb27-265a-439d-8950-cd6c32baf983",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Preparación de los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62d8b47-c035-4e72-b46e-6f9152168719",
   "metadata": {},
   "source": [
    "##### PAWSX original"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483a2b75-4836-4eb1-9a94-3f56a19af0f2",
   "metadata": {},
   "source": [
    "Para entrenar un modelo que tenga una precisión considerable, nos hemos dado cuenta de que los datos de los que disponemos son insuficientes. Consecuentemente, vamos a hacer uso del dataset PAWS-X de Google, donde es la versión extendida de PAWS en 7 idiomas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "54be0eec-d1cc-4293-84fb-447f652d0b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the dataset with the spanish phrases\n",
    "datasetDict2 = load_dataset('paws-x', 'es')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "baf4b076-00a7-480e-93b9-4b7e78e7f219",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'sentence1', 'sentence2', 'label'],\n",
      "        num_rows: 49401\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'sentence1', 'sentence2', 'label'],\n",
      "        num_rows: 2000\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['id', 'sentence1', 'sentence2', 'label'],\n",
      "        num_rows: 2000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(datasetDict2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ca26a1f-c629-46a3-8770-4ecbbeb446e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 11, 'sentence1': 'Kabir Suman grabó varios álbumes con el nombre de Suman Chattopaddhyay o Suman Chatterjee entre 1992 y 1999.', 'sentence2': 'Suman Chatterjee, grabó varios álbumes entre 1992 y 1999 con el nombre de Suman Chattopaddhyay o Kabir Suman.', 'label': 0}\n"
     ]
    }
   ],
   "source": [
    "print(datasetDict2[\"train\"][10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b430bcc6-217e-408a-8c41-2052011c6b62",
   "metadata": {},
   "source": [
    "##### PAWSX filtrado\n",
    "Dado que solo queremos hacer uso del modelo en castellano, únicamente nos quedaremos con las frases en este idioma. Además, el conjunto de datos original contiene pares de frases que son paráfrasis y que no lo son. En nuestro caso, únicamente nos interesan las primeras, por lo que también se filtrarán."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3d7c6f1b-782f-4511-a84f-38aa116f410e",
   "metadata": {},
   "outputs": [],
   "source": [
    "PAWSXDataDict = processPAWSX()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a9bb2d-242e-4f12-a306-f245b99568fa",
   "metadata": {},
   "source": [
    "Estructura del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d2eeea12-cd4e-4182-bb39-ba5a56e8a792",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['sentence1', 'sentence2'],\n",
      "        num_rows: 21829\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['sentence1', 'sentence2'],\n",
      "        num_rows: 907\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['sentence1', 'sentence2'],\n",
      "        num_rows: 847\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(PAWSXDataDict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0aa9686-5af8-4b68-b113-25518bddf617",
   "metadata": {},
   "source": [
    "Diez ejemplos no consecutivos para cerciorarnos de que todos los datos son paráfrasis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d47c9c52-74d3-4331-8dac-6db41cc51396",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sentence1': 'El distrito electoral se encuentra en la bahía de Swansea, situada en la margen derecha del río Afan, cerca de su desembocadura en el sur de Gales.', 'sentence2': 'El distrito electoral se encuentra en Swansea Bay, en la orilla derecha del río Afan, cerca de su desembocadura en el sur de Gales.'}\n",
      "{'sentence1': \"Los `` Zapateros '' cayeron al 16º en 1991: 92, antes de caer en el 20º puesto en 1992: 93 bajo Phil Chard.\", 'sentence2': \"Los `` Zapateros '' cayeron al 16º en 1991: 92, antes de caer en el 20º puesto en 1992: 93 bajo Phil Chard.\"}\n",
      "{'sentence1': 'La ley albanesa está codificada y basada en la ley francesa.', 'sentence2': 'La ley albanesa está codificada y se basa en la ley francesa.'}\n",
      "{'sentence1': 'Nació en Usera, España (Madrid) el 18 de abril de 1976.', 'sentence2': 'Nació el 18 de abril de 1976 en Usera, España (Madrid).'}\n",
      "{'sentence1': 'Los pequeños blenios marinos blenioides (\"Ecsenius australianus\") son peces australianos del género \"Ecsenius\".', 'sentence2': \"Los pequeños Blennny de Blennioids marinos (`` Ecsenius australianus '') son peces australianos del género '' Ecsenius ''.\"}\n",
      "{'sentence1': \"El 24 de julio de 2013, John Hodgman apareció en el podcast Maximum Fun `` Judge Brother Ali '' como '' Expert Witness ''.\", 'sentence2': \"El 24 de julio de 2013, John Hodgman apareció en el podcast Maximum Fun `` Judge Brother Ali '' como un `` Expert Witness ''.\"}\n",
      "{'sentence1': 'El padre de Ted y Tom fue Steve.', 'sentence2': 'Steve fue el padre de Ted y Tom.'}\n",
      "{'sentence1': 'Danielle, Nadine y Mark tienen sus fotos visibles en la vitrina de la escuela.', 'sentence2': 'Mark, Nadine y Danielle han visto sus fotos en el caso de la escuela.'}\n",
      "{'sentence1': 'Cuando las empresas de agua y alcantarillado se privatizaron en Irlanda del Norte en 1989, estos servicios siguieron siendo públicos en Inglaterra, Gales y Escocia.', 'sentence2': 'Cuando las empresas de agua y alcantarillado se privatizaron en Irlanda del Norte en 1989, estos servicios siguieron siendo públicos en Inglaterra, Gales y Escocia.'}\n",
      "{'sentence1': \"Los únicos valores de `` n '' a 600000, para los cuales hay más primos impares pitagóricos que no pitagóricos, son, por ejemplo, 26861 y 26862.\", 'sentence2': \"Por ejemplo, los únicos valores de `` n '' hasta 600000 para los cuales hay más números primos impares pitagóricos que no pitagóricos son 26861 y 26862.\"}\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(PAWSXDataDict[\"train\"][1050 + i * 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029c564a-1f84-4909-8968-06448a0cdd4d",
   "metadata": {},
   "source": [
    "##### Dataset propio\n",
    "Dado que hemos aplicado técnicas de aumento de datos, disponemos pares de frases (paráfrasis) adaptadas a nuestro dominio (reseñas de estaciones de metro). \n",
    "\n",
    "Se han aplicado 3 variaciones de la retrotraducción, por lo que solo con esto, se disponen de tres paraes de frases por cada elemento del conjunto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "18a78634-7876-4cb9-b956-599eee0512e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "invalidOriginalPath = \"/home/ibon/Documentos/GitHub/TFG/1. Data/4. Labeled Reviews/2. Without Emojis/InvalidReviews.txt\"\n",
    "validOriginalPath = \"/home/ibon/Documentos/GitHub/TFG/1. Data/4. Labeled Reviews/2. Without Emojis/ValidReviews.txt\"\n",
    "\n",
    "backtranslationFolder = \"/home/ibon/Documentos/GitHub/TFG/2. Review Classifier/1. Data Augmentation/1. Back Translation/1. Google Translator\"\n",
    "invalidBacktranslationPath1 = backtranslationFolder + \"/InvalidReviewsTranslationsEsEnEnEs.txt\"\n",
    "invalidBacktranslationPath2 = backtranslationFolder + \"/InvalidReviewsTranslationsEsFrFrJaJaRuRuEs.txt\"\n",
    "invalidBacktranslationPath3 = backtranslationFolder + \"/InvalidReviewsTranslationsEsJaJaEs.txt\"\n",
    "validBacktranslationPath1 = backtranslationFolder + \"/ValidReviewsTranslationsEsEnEnEs.txt\"\n",
    "validBacktranslationPath2 = backtranslationFolder + \"/ValidReviewsTranslationsEsFrFrJaJaRuRuEs.txt\"\n",
    "validBacktranslationPath3 = backtranslationFolder + \"/ValidReviewsTranslationsEsJaJaEs.txt\"\n",
    "\n",
    "invalidParaphrasesDictList1 = importParaphraseFromTxtToDictList(invalidOriginalPath, invalidBacktranslationPath1)\n",
    "invalidParaphrasesDictList2 = importParaphraseFromTxtToDictList(invalidOriginalPath, invalidBacktranslationPath2)\n",
    "invalidParaphrasesDictList3 = importParaphraseFromTxtToDictList(invalidOriginalPath, invalidBacktranslationPath3)\n",
    "validParaphrasesDictList1 = importParaphraseFromTxtToDictList(validOriginalPath, validBacktranslationPath1)\n",
    "validParaphrasesDictList2 = importParaphraseFromTxtToDictList(validOriginalPath, validBacktranslationPath2)\n",
    "validParaphrasesDictList3 = importParaphraseFromTxtToDictList(validOriginalPath, validBacktranslationPath3)\n",
    "\n",
    "#Join all the lists\n",
    "backtranslationDictList = invalidParaphrasesDictList1 + invalidParaphrasesDictList2 + invalidParaphrasesDictList3 + validParaphrasesDictList1 + validParaphrasesDictList2 + validParaphrasesDictList3\n",
    "#Shuffle the lists\n",
    "random.shuffle(backtranslationDictList)\n",
    "\n",
    "#Convert it to a Hugging Face dataset\n",
    "backtranslationDataset = Dataset.from_list(backtranslationDictList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1590efcd-3d23-42e9-8d5a-ca8f9ab53cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the data\n",
    "backtranslationDataDict = trainTestValidationSplit(backtranslationDataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d7ec25-d62b-43f0-914d-075924bbd7b4",
   "metadata": {},
   "source": [
    "##### Unión del PAWSX filtrado y el dataset propio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b9dd295b-ee02-480c-b70b-8bdcbf6e51d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Concatenate the datasets\n",
    "unionDatasetDict = DatasetDict()\n",
    "\n",
    "for key in PAWSXDataDict.keys():\n",
    "    newDataset = concatenate_datasets([PAWSXDataDict[key], backtranslationDataDict[key]])\n",
    "\n",
    "    unionDatasetDict[key] = newDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5f657e7d-1042-410c-8547-12d535dc0dc7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['sentence1', 'sentence2'],\n",
      "        num_rows: 29590\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['sentence1', 'sentence2'],\n",
      "        num_rows: 1878\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['sentence1', 'sentence2'],\n",
      "        num_rows: 1817\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(unionDatasetDict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243e2bee-1784-43ef-9e16-b2a1d8557b18",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Entrenamiento de los modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6ec044c8-246f-4389-b7c3-f5968bbbabb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Map: 100%|██████████| 7761/7761 [00:01<00:00, 4257.45 examples/s]\n",
      "Map: 100%|██████████| 971/971 [00:00<00:00, 4225.38 examples/s]\n",
      "Map: 100%|██████████| 970/970 [00:00<00:00, 4214.91 examples/s]\n",
      "/home/ibon/Programas/anaconda3/envs/TFG/lib/python3.12/site-packages/transformers/training_args.py:1559: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1452' max='1452' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1452/1452 41:01, Epoch 5/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>78.753500</td>\n",
       "      <td>14.595844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.974500</td>\n",
       "      <td>0.411336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.704100</td>\n",
       "      <td>0.161291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.558500</td>\n",
       "      <td>0.155704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.754800</td>\n",
       "      <td>0.153961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.734000</td>\n",
       "      <td>0.153505</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 7761/7761 [00:01<00:00, 4290.72 examples/s]\n",
      "Map: 100%|██████████| 971/971 [00:00<00:00, 4158.41 examples/s]\n",
      "Map: 100%|██████████| 970/970 [00:00<00:00, 4143.68 examples/s]\n",
      "/home/ibon/Programas/anaconda3/envs/TFG/lib/python3.12/site-packages/transformers/training_args.py:1559: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1452' max='1452' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1452/1452 46:52, Epoch 5/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>20.500100</td>\n",
       "      <td>4.170770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.728500</td>\n",
       "      <td>0.234523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.619900</td>\n",
       "      <td>0.149062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.492800</td>\n",
       "      <td>0.143550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.664000</td>\n",
       "      <td>0.141230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.645800</td>\n",
       "      <td>0.140847</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 21829/21829 [00:05<00:00, 4345.25 examples/s]\n",
      "Map: 100%|██████████| 907/907 [00:00<00:00, 4140.91 examples/s]\n",
      "Map: 100%|██████████| 847/847 [00:00<00:00, 4408.93 examples/s]\n",
      "/home/ibon/Programas/anaconda3/envs/TFG/lib/python3.12/site-packages/transformers/training_args.py:1559: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4092' max='4092' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4092/4092 1:52:02, Epoch 5/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.379700</td>\n",
       "      <td>0.104605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.303600</td>\n",
       "      <td>0.097423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.278200</td>\n",
       "      <td>0.096132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.289400</td>\n",
       "      <td>0.094946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.279400</td>\n",
       "      <td>0.094912</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 21829/21829 [00:04<00:00, 4380.34 examples/s]\n",
      "Map: 100%|██████████| 907/907 [00:00<00:00, 4323.39 examples/s]\n",
      "Map: 100%|██████████| 847/847 [00:00<00:00, 4359.34 examples/s]\n",
      "/home/ibon/Programas/anaconda3/envs/TFG/lib/python3.12/site-packages/transformers/training_args.py:1559: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4092' max='4092' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4092/4092 2:08:24, Epoch 5/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.314500</td>\n",
       "      <td>0.096967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.260000</td>\n",
       "      <td>0.091664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.240900</td>\n",
       "      <td>0.090016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.244000</td>\n",
       "      <td>0.088631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.235700</td>\n",
       "      <td>0.088624</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 29590/29590 [00:06<00:00, 4297.57 examples/s]\n",
      "Map: 100%|██████████| 1878/1878 [00:00<00:00, 4334.68 examples/s]\n",
      "Map: 100%|██████████| 1817/1817 [00:00<00:00, 4359.05 examples/s]\n",
      "/home/ibon/Programas/anaconda3/envs/TFG/lib/python3.12/site-packages/transformers/training_args.py:1559: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5544' max='5544' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5544/5544 2:33:06, Epoch 5/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.437000</td>\n",
       "      <td>0.134752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.363300</td>\n",
       "      <td>0.130644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.451000</td>\n",
       "      <td>0.127939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.357400</td>\n",
       "      <td>0.126981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.384200</td>\n",
       "      <td>0.126165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.357800</td>\n",
       "      <td>0.126157</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 29590/29590 [00:06<00:00, 4302.81 examples/s]\n",
      "Map: 100%|██████████| 1878/1878 [00:00<00:00, 4267.42 examples/s]\n",
      "Map: 100%|██████████| 1817/1817 [00:00<00:00, 4098.90 examples/s]\n",
      "/home/ibon/Programas/anaconda3/envs/TFG/lib/python3.12/site-packages/transformers/training_args.py:1559: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5544' max='5544' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5544/5544 2:55:56, Epoch 5/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.387900</td>\n",
       "      <td>0.125870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.321700</td>\n",
       "      <td>0.121482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.396300</td>\n",
       "      <td>0.118064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.309800</td>\n",
       "      <td>0.116897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.327600</td>\n",
       "      <td>0.115868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.308000</td>\n",
       "      <td>0.115681</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "savingPath = \"/home/ibon/Documentos/1. Models/FlanT5Small\"\n",
    "modelName = \"google/flan-t5-small\"\n",
    "\n",
    "#Possible characteristics of the models\n",
    "dataDictList = [backtranslationDataDict, PAWSXDataDict, unionDatasetDict]\n",
    "freezePctgList = [0.8, 0]\n",
    "trainEpochsList = [6]\n",
    "\n",
    "#Version control\n",
    "version = 0\n",
    "\n",
    "for i, dataDict in enumerate(dataDictList):\n",
    "    for freezePctg in freezePctgList:\n",
    "        for trainEpochs in trainEpochsList:\n",
    "            trainFlanT5Model(savingPath, modelName, version, tokenizeFunctionFlanT5, dataDict, aditionalTokens = None, trainEpochs = trainEpochs, freezePctg = freezePctg)\n",
    "\n",
    "            with open(savingPath + \"/VersionControlFlanT5Small.csv\", \"a\") as file:\n",
    "                data = \"unlabeled \\n\" if i == 0 else \"labeled \\n\"\n",
    "\n",
    "                #Update the version control\n",
    "                file.write(modelName.replace(\"/\", \"%\") + \"v\" + str(version) + \",\" + str(version) + \",\" + str(freezePctg) + \",\" + str(trainEpochs) + \",\" + data)\n",
    "\n",
    "            version += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42923f43-eb89-4b58-9bc9-29aa00c7a2d8",
   "metadata": {},
   "source": [
    "### Probar los modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "14e55a98-a918-46b6-8dcf-07fdaafde1e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo: google%flan-t5-smallv0\n",
      "\n",
      "Texto: “La estación no tiene escaleras mecanicos, pero las metros vienen enhora. No hay apta para invalidarios”.\n",
      "\n",
      "Modelo: google%flan-t5-smallv1\n",
      "\n",
      "Texto: La estación no tiene escaleras mecánicas, pero los metros vienen horas. No hay apta para invalidos\n",
      "\n",
      "Modelo: google%flan-t5-smallv2\n",
      "\n",
      "Texto: La estación no tiene escaleras mécaniques, pero los metros vienen hora. No se apta para invalidos.\n",
      "\n",
      "Modelo: google%flan-t5-smallv3\n",
      "\n",
      "Texto: La estación no tiene escaleras de forma mecanica, pero las metros vienen enhora. No és apta para invalidos.\n",
      "\n",
      "Modelo: google%flan-t5-smallv4\n",
      "\n",
      "Texto: La estación no tiene escaleras mécaniques, pero los metros vienen hora. No hay apta para invalidos.\n",
      "\n",
      "Modelo: google%flan-t5-smallv5\n",
      "\n",
      "Texto: La estación no tiene escaleras mecánicas, pero los metros vienen horas. No hay apta para invalidos\n",
      "\n"
     ]
    }
   ],
   "source": [
    "savingPath = \"/home/ibon/Documentos/1. Models/FlanT5Small\"\n",
    "text = \"Reescribe esta frase: La estación no tiene escaleras mecanicas, pero los metros vienen en hora. No es apta para invalidos\"\n",
    "\n",
    "#For every generated model\n",
    "with open(savingPath + \"/VersionControlFlanT5Small.csv\", \"r\") as file:\n",
    "    #Ignore the first line of the file\n",
    "    next(file)\n",
    "\n",
    "    for line in file:\n",
    "        elemsList = line.split(\",\")\n",
    "        \n",
    "        modelName = elemsList[0]\n",
    "        modelPath = savingPath + \"/\" + modelName\n",
    "\n",
    "        #Generate the text\n",
    "        generatedText = generateTextFlanT5(modelPath, text)\n",
    "\n",
    "        print(\"Modelo: \" +  modelName + \"\\n\")\n",
    "        print(\"Texto: \" + generatedText + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae24a95-bb09-4a98-be36-ab3b1d170fd4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Modificaciones\n",
    "\n",
    "Como se puede ver, los modelos generan salidas idénticas o salidas poco coherentes. Consecuentemente, se va aplicar otro enfoque en el entrenamiento: se va a añadir de forma más explícita la tarea que debe realizar el modelo y se van a introducir prompts negativos para poder distinguir entre salidas correctas e incorrectas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928e6159-ac1d-413e-9cdf-5e0aad3060dc",
   "metadata": {},
   "source": [
    "##### Añadir ejemplos negativos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ff407e6-8500-4b4c-965e-1e1d1b7a8bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def processPAWSXMod():\n",
    "    #Import the dataset with the spanish phrases\n",
    "    datasetDict = load_dataset('paws-x', 'es')\n",
    "\n",
    "    #Create an empty dataset to save the paraphrases\n",
    "    newDatasetDict = DatasetDict()\n",
    "\n",
    "    #Filter the paraphrases\n",
    "    for key, valueDataset in datasetDict.items():\n",
    "        paraphraseDataset = valueDataset.filter(lambda example: example['label'] == 1)\n",
    "\n",
    "        paraphraseDataset = paraphraseDataset.remove_columns([\"id\"])\n",
    "        paraphraseDataset = paraphraseDataset.rename_column(\"label\", \"isPositive\")\n",
    "    \n",
    "        newDatasetDict[key] = paraphraseDataset\n",
    "\n",
    "    #Add the negatives\n",
    "    for key, valueDataset in newDatasetDict.items():\n",
    "        negativeExamples = []\n",
    "        for value in valueDataset:\n",
    "            if random.random() <= 0.4:\n",
    "                negativeExamples.append({\n",
    "                    \"sentence1\" : value[\"sentence1\"],\n",
    "                    \"sentence2\" : value[\"sentence1\"],\n",
    "                    \"isPositive\" : 0\n",
    "                })\n",
    "        newValueDataset = Dataset.from_list(negativeExamples)\n",
    "        newValueDataset = newValueDataset.cast(valueDataset.features)\n",
    "        newDatasetDict[key] = concatenate_datasets([valueDataset, newValueDataset])\n",
    "    \n",
    "    return newDatasetDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bce9f5c3-466b-4b79-b634-05a6b4739121",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Casting the dataset: 100%|██████████| 8566/8566 [00:00<00:00, 2062243.60 examples/s]\n",
      "Casting the dataset: 100%|██████████| 387/387 [00:00<00:00, 289546.14 examples/s]\n",
      "Casting the dataset: 100%|██████████| 333/333 [00:00<00:00, 237252.12 examples/s]\n"
     ]
    }
   ],
   "source": [
    "PAWSXDataDictMod = processPAWSXMod()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7535a2ed-b833-4f13-8b94-89d7214f15fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['sentence1', 'sentence2', 'isPositive'],\n",
      "        num_rows: 30395\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['sentence1', 'sentence2', 'isPositive'],\n",
      "        num_rows: 1294\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['sentence1', 'sentence2', 'isPositive'],\n",
      "        num_rows: 1180\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(PAWSXDataDictMod)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56990d1c-4339-4572-b8a4-5d1dbeb3e411",
   "metadata": {},
   "source": [
    "##### Tokenización modificada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e7d3175b-fd12-496b-8fb3-543250adff5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizeFunctionFlanT5Mod(examples, tokenizer):\n",
    "    #Add the instruction to all the sentecnces for the model to understand what it has to do\n",
    "    inputs = [f\"Haz un parafraseo creativo de esta frase: {text}\" for text in examples[\"sentence1\"]]\n",
    "    isPositive = examples[\"isPositive\"]\n",
    "\n",
    "    #If isPositive is not a list\n",
    "    if isinstance(isPositive, int):\n",
    "        isPositive = [isPositive]\n",
    "\n",
    "    #Indicate that the targets are the paraphrased sentences\n",
    "    targets = examples[\"sentence2\"]\n",
    "\n",
    "    #Tokenize the input\n",
    "    modelInputs = tokenizer(inputs, max_length = 512, truncation = True, padding = \"max_length\")\n",
    "\n",
    "    #Tokenize the output\n",
    "    labels = tokenizer(targets, max_length = 512, truncation = True, padding = \"max_length\").input_ids\n",
    "\n",
    "    #Ajust the outputs depending on the label (penalize the negatives)\n",
    "    for i, elem in enumerate(isPositive):\n",
    "        if elem == 0:\n",
    "            labels[i] = [-100] + labels[i] #Penalize the output in the taining\n",
    "\n",
    "    #Add the expected output to the input of the model\n",
    "    modelInputs[\"labels\"] = labels\n",
    "    \n",
    "    return modelInputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f72e7d1-277d-4374-bd86-6fe050af077c",
   "metadata": {},
   "source": [
    "##### Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f3062fa7-316a-4133-a999-623b20948ab5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Map: 100%|██████████| 30395/30395 [00:07<00:00, 3876.79 examples/s]\n",
      "Map: 100%|██████████| 1294/1294 [00:00<00:00, 4230.15 examples/s]\n",
      "Map: 100%|██████████| 1180/1180 [00:00<00:00, 4294.11 examples/s]\n",
      "/home/ibon/Programas/anaconda3/envs/TFG/lib/python3.12/site-packages/transformers/training_args.py:1559: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5700' max='5700' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5700/5700 2:46:59, Epoch 6/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.234400</td>\n",
       "      <td>0.072752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.198400</td>\n",
       "      <td>0.070854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.214100</td>\n",
       "      <td>0.070051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.183200</td>\n",
       "      <td>0.069201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.220000</td>\n",
       "      <td>0.068537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.202800</td>\n",
       "      <td>0.068647</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 30395/30395 [00:07<00:00, 4205.43 examples/s]\n",
      "Map: 100%|██████████| 1294/1294 [00:00<00:00, 4212.04 examples/s]\n",
      "Map: 100%|██████████| 1180/1180 [00:00<00:00, 4344.64 examples/s]\n",
      "/home/ibon/Programas/anaconda3/envs/TFG/lib/python3.12/site-packages/transformers/training_args.py:1559: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5700' max='5700' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5700/5700 2:40:34, Epoch 6/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.252100</td>\n",
       "      <td>0.075531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.215400</td>\n",
       "      <td>0.073329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.234800</td>\n",
       "      <td>0.072382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.198900</td>\n",
       "      <td>0.071673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.241800</td>\n",
       "      <td>0.071354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.223500</td>\n",
       "      <td>0.071255</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "savingPath = \"/home/ibon/Documentos/1. Models/FlanT5Small\"\n",
    "modelName = \"google/flan-t5-small\"\n",
    "\n",
    "#Possible characteristics of the models\n",
    "dataDictList = [PAWSXDataDictMod]\n",
    "freezePctgList = [0.5,0.8]\n",
    "trainEpochsList = [6]\n",
    "\n",
    "#Version control\n",
    "version = 6\n",
    "\n",
    "for i, dataDict in enumerate(dataDictList):\n",
    "    for freezePctg in freezePctgList:\n",
    "        for trainEpochs in trainEpochsList:\n",
    "            trainFlanT5Model(savingPath, modelName, version, tokenizeFunctionFlanT5Mod, dataDict, aditionalTokens = None, trainEpochs = trainEpochs, freezePctg = freezePctg)\n",
    "\n",
    "            with open(savingPath + \"/VersionControlFlanT5Small.csv\", \"a\") as file:\n",
    "                data = \"unlabeled \\n\" if i == 0 else \"labeled \\n\"\n",
    "\n",
    "                #Update the version control\n",
    "                file.write(modelName.replace(\"/\", \"%\") + \"v\" + str(version) + \",\" + str(version) + \",\" + str(freezePctg) + \",\" + str(trainEpochs) + \",\" + data)\n",
    "\n",
    "            version += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82c2b43-b9be-499f-936c-d544d6984bf5",
   "metadata": {},
   "source": [
    "##### Probar los modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0385d4ce-7542-4a36-8fff-b7aeb9e51075",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateTextFlanT5(modelPath, text):\n",
    "    #Load the model\n",
    "    model = T5ForConditionalGeneration.from_pretrained(modelPath)\n",
    "    tokenizer = T5Tokenizer.from_pretrained(modelPath)\n",
    "\n",
    "    #Check if the tokenizer has a padding token\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.add_special_tokens({'pad_token': '<|pad|>'})\n",
    "        model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "    #Tokenize the input text\n",
    "    inputIds = tokenizer(text, return_tensors = \"pt\").input_ids\n",
    "    inputLength = inputIds.shape[-1]\n",
    "\n",
    "    #Generate the text\n",
    "    outputs = model.generate(inputIds, num_return_sequences = 5, max_length = inputLength, do_sample = True, num_beams = 5, early_stopping = True, repetition_penalty = 1.5, no_repeat_ngram_size = 2, top_k = 50, top_p = 0.95, temperature = 1.5,)\n",
    "\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a8940ec4-c2bb-423d-b7be-30fc7b02f433",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo: google%flan-t5-smallv0\n",
      "\n",
      "Texto: “Estación sucia, me gusta. Hay escaleras mecánicas por lo que está apta para inválidos.”\n",
      "\n",
      "Texto: “Estación sucia, me gusta. Hay escaleras mecánicas por lo que está apta para inválidos”.\n",
      "\n",
      "Texto: “Estación sucia, me gusta. Hay escaleras mecánicas por lo que está apta para inválidos.\n",
      "\n",
      "Texto: “Estación sucia, me gusta. Hay escaleras mecánicas por lo que está apta para inválidos”\n",
      "\n",
      "Texto: “Estación sucia, me gusta. Hay escaleras mecánicas por lo que está apta para inválidos\n",
      "\n",
      "Modelo: google%flan-t5-smallv1\n",
      "\n",
      "Texto: Aunque la estación está sucia, mi gusta. Hay escaleras mecánicas por lo que hay apta para inválidos.\n",
      "\n",
      "Texto: Aunque la estación está sucia, mi gusta. Hay escaleras mecánicas por lo que hay apta para inválidos\n",
      "\n",
      "Texto: Aunque la estación está sucia, mi gusta. Hay escaleras mecánicas por lo que hay apta para invisión.\n",
      "\n",
      "Texto: Aunque la estación está sucia, mi gusta. Hay escaleras mecánicas por lo que hay apta para inválidos...\n",
      "\n",
      "Texto: Aunque la estación está sucia, mi gusta. Hay escaleras mecánicas por lo que hay apta para involidos\n",
      "\n",
      "Modelo: google%flan-t5-smallv2\n",
      "\n",
      "Texto: Aunque la estación estás sucia, me gusta. Hay escaleras mecánicas por lo que é apta para inválidos.\n",
      "\n",
      "Texto: Aunque la estación estás sucia, me gusta. Hay escaleras mecánicas por lo que era apta para inválidos.\n",
      "\n",
      "Texto: Aunque la estación estás sucia, me gusta. Hay escaleras mecánicas por lo que é apta para inválidos\n",
      "\n",
      "Texto: Aunque la estación estás sucia, me gusta. Hay escaleras mecánicas por lo que é apta para invidálidos.\n",
      "\n",
      "Texto: Aunque la estación estás sucia, me gusta. Hay escaleras mecánicas por lo que é apta para inválidos:\n",
      "\n",
      "Modelo: google%flan-t5-smallv3\n",
      "\n",
      "Texto: Aunque la estación está sucia, me gusta. Hay escaleras mecánicas para inválidos por lo que se dificulta para insertar.\n",
      "\n",
      "Texto: Aunque la estación está sucia, me gusta. Hay escaleras mecánicas para inválidos por lo que se dificulta para insertarse.\n",
      "\n",
      "Texto: Aunque la estación está sucia, me gusta. Hay escaleras mecánicas para inválidos por lo que se dificulta para insertarse en lugares.\n",
      "\n",
      "Texto: Aunque la estación está sucia, me gusta. Hay escaleras mecánicas para inválidos por lo que se dificulta para insertarse en lugar.\n",
      "\n",
      "Texto: Aunque la estación está sucia, me gusta. Hay escaleras mecánicas para inválidos por lo que se dificulta para insertarse en lugares\n",
      "\n",
      "Modelo: google%flan-t5-smallv4\n",
      "\n",
      "Texto: Aunque la estación estás sucia, me gusta. Hay escaleras mecánicas por lo que hay apta para inválidos.\n",
      "\n",
      "Texto: Aunque la estación estás sucia, me gusta. Hay escaleras mecánicas por lo que era apta para inválidos.\n",
      "\n",
      "Texto: Aunque la estación estás sucia, me gusta. Hay escaleras mecánicas por lo que hay apta para inválidos\n",
      "\n",
      "Texto: Aunque la estación estás sucia, me gusta. Hay escaleras mecánicas por lo que era apta para inválidos\n",
      "\n",
      "Texto: Aunque la estación estás sucia, me gusta. Hay escaleras mecánicas por lo que era apta para involidos.\n",
      "\n",
      "Modelo: google%flan-t5-smallv5\n",
      "\n",
      "Texto: Aunque la estación estás sucia, me gusta. Hay escaleras mecánicas por lo que se apta para inválidos.\n",
      "\n",
      "Texto: Aunque la estación está sucia, me gusta. Hay escaleras mecánicas para inválidos.\n",
      "\n",
      "Texto: Aunque la estación estás sucia, me gusta. Hay escaleras mecánicas por lo que se apta para inválidos\n",
      "\n",
      "Texto: Aunque la estación estás sucia, me gusta. Hay escaleras mecánicas por lo que se apta para inválidos. Haz un parafraseo creativo destaca.\n",
      "\n",
      "Texto: Aunque la estación estás sucia, me gusta. Hay escaleras mecánicas por lo que se apta para inválidos. Haz un parafraseo creativo de este frase\n",
      "\n",
      "Modelo: google%flan-t5-smallv6\n",
      "\n",
      "Texto: Aunque la estación estás sucia, me gusta. Hay escaleras mecánicas por lo que se apta para inválidos.\n",
      "\n",
      "Texto: Aunque la estación estás sucia, me gusta. Hay escaleras mecánicas por lo que ésta para inválidos.\n",
      "\n",
      "Texto: Aunque la estación estás sucia, me gusta. Hay escaleras mecánicas por lo que se apta para inválidos\n",
      "\n",
      "Texto: Aunque la estación estás sucia, me gusta. Hay escaleras mecánicas por lo que ésta para inválidos\n",
      "\n",
      "Texto: Aunque la estación estás sucia, me gusta. Hay escaleras mecánicas por lo que se apta para Inválidos.\n",
      "\n",
      "Modelo: google%flan-t5-smallv7\n",
      "\n",
      "Texto: Aunque la estación estás sucia, me gusta. Hay escaleras mecánicas por lo que se apta para inválidos.\n",
      "\n",
      "Texto: Aunque la estación estás sucia, me gusta. Hay escaleras mecánicas por lo que é apta para inválidos.\n",
      "\n",
      "Texto: Aunque la estación estás sucia, me gusta. Hay escaleras mecánicas por lo que é apta para inválidos\n",
      "\n",
      "Texto: Aunque la estación estás sucia, me gusta. Hay escaleras mecánicas por lo que se apta para inválidos\n",
      "\n",
      "Texto: Aunque la estación estás sucia, me gusta. Hay escaleras mecánicas por lo que se apta para involidos.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "savingPath = \"/home/ibon/Documentos/1. Models/FlanT5Small\"\n",
    "text = \"Haz un parafraseo creativo de esta frase: Aunque la estación estás sucia, me gusta. Hay escaleras mecánicas por lo que es apta para inválidos\"\n",
    "\n",
    "#For every generated model\n",
    "with open(savingPath + \"/VersionControlFlanT5Small.csv\", \"r\") as file:\n",
    "    #Ignore the first line of the file\n",
    "    next(file)\n",
    "\n",
    "    for line in file:\n",
    "        elemsList = line.split(\",\")\n",
    "        \n",
    "        modelName = elemsList[0]\n",
    "        modelPath = savingPath + \"/\" + modelName\n",
    "\n",
    "        #Generate the text\n",
    "        outputs = generateTextFlanT5(modelPath, text)\n",
    "\n",
    "        print(\"Modelo: \" +  modelName + \"\\n\")\n",
    "        for output in outputs:\n",
    "            #Decode the text\n",
    "            generatedText = tokenizer.decode(output, skip_special_tokens=True)\n",
    "            print(\"Texto: \" + generatedText + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "53d101ac-d9ac-488b-8415-3cb6f0d379f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The station is clean but i do not like it. It does not have mechanical stairs.\n",
      "The railway station is clean but i do not like it because it does not have mechanical stairs.\n",
      "The station is clean but i do not like it. It does not have mechanical stairs.\n",
      "The train station is clean but i do not like it. It does not have mechanical stairs.\n",
      "The station is clean, but i do not like it because it does not have mechanical stairs.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Vamsi/T5_Paraphrase_Paws\")  \n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"Vamsi/T5_Paraphrase_Paws\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "sentence = \"The train station is clean, but i do not like it. it does not have mechanical stairs\"\n",
    "\n",
    "text =  \"paraphrase: \" + sentence + \" </s>\"\n",
    "\n",
    "encoding = tokenizer.encode_plus(text,pad_to_max_length=True, return_tensors=\"pt\")\n",
    "\n",
    "input_ids, attention_masks = encoding[\"input_ids\"].to(device), encoding[\"attention_mask\"].to(device)\n",
    "\n",
    "outputs = model.generate(\n",
    "    input_ids=input_ids, attention_mask=attention_masks,\n",
    "    max_length=256,\n",
    "    do_sample=True,\n",
    "    top_k=200,\n",
    "    top_p=0.95,\n",
    "    early_stopping=True,\n",
    "    num_return_sequences=5\n",
    ")\n",
    "\n",
    "for output in outputs:\n",
    "    line = tokenizer.decode(output, skip_special_tokens=True,clean_up_tokenization_spaces=True)\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55386aec-41c9-43ea-b5a4-1d651bed4a1d",
   "metadata": {},
   "source": [
    "## DeepSeek R1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60d0fb9-30ab-4ef5-856f-23a6dea84482",
   "metadata": {},
   "source": [
    "Debido a la imposibilidad de generar paráfrasis de calidad con los modelos GPT2 y T5, se optó por usar modelos de DeepSeek adaptados para la librería unsloth que permite reducir el consumo de vRAM durante el proceso de finetuning.\n",
    "\n",
    "El modelo que se va a usar va a ser el unsloth/DeepSeek-R1-Distill-Llama-8B-unsloth-bnb-4bit, que es una combinación de varias tecnologías desarrolladas por diferentes entidades, entre ellas DeepSeek y las técnicas de optimización de Unsloth."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47de4e6-5ba5-4d7c-8d35-739e325e692b",
   "metadata": {},
   "source": [
    "### Funciones que se van a utilizar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12da183-12b6-459f-a3b7-fffeccf4ea8b",
   "metadata": {},
   "source": [
    "Importar datos de dos ficheros (donde los datos no tienen etiquetas) a una lista de diccionarios de la forma: \"sentence1\" : original , \"sentence2\" : paráfrasis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "086173d7-1ef9-46a1-b23d-a7d58dea359b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def importParaphraseFromTxtToDictList(originalPath, paraphrasePath):\n",
    "    dictList = []\n",
    "    \n",
    "    #As both files have the same number of lines, they can be iterated simultaneously\n",
    "    with open(originalPath, 'r', encoding = 'utf-8') as originalFile, open(paraphrasePath, 'r', encoding = 'utf-8') as paraphraseFile:\n",
    "        for originalLine, paraphraseLine in zip(originalFile, paraphraseFile): \n",
    "            dictList.append({\"sentence1\" : originalLine.strip(), \"sentence2\" : paraphraseLine.strip()})\n",
    "            \n",
    "    return dictList"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78aa2e0-8598-48e0-9ea7-b26057cb5f00",
   "metadata": {},
   "source": [
    "Separación del conjunto de datos en: train, test y validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d8da393-3995-4ac2-b8ec-3969069b2d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainTestValidationSplit(dataset, trainPctg = 0.8):\n",
    "    datasetSplit = dataset.train_test_split(test_size = 1 - trainPctg, seed = 54)\n",
    "    validationTestSplit = datasetSplit[\"test\"].train_test_split(test_size = 0.5, seed = 54)\n",
    "    \n",
    "    return DatasetDict({\n",
    "        \"train\" : datasetSplit[\"train\"],\n",
    "        \"test\" : validationTestSplit[\"test\"],\n",
    "        \"validation\" : validationTestSplit[\"train\"]\n",
    "        })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3416eca6-c9db-46ba-9648-2ea2be2d3650",
   "metadata": {},
   "source": [
    "Importación y filtrado de PAWSX (dataset de Google), de forma que se obtienen las frases en castellano y únicamente los datos que son paráfrasis (etiqueta a 1). Además, se van a eliminar las columnas de id y label, ya que no serán necesarias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1378ab5a-6cf6-486b-85f4-071bc405dc5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def processPAWSX():\n",
    "    #Import the dataset with the spanish phrases\n",
    "    datasetDict = load_dataset('paws-x', 'es')\n",
    "\n",
    "    #Create an empty dataset to save the paraphrases\n",
    "    newDatasetDict = DatasetDict()\n",
    "\n",
    "    #Filter the paraphrases\n",
    "    for key, valueDataset in datasetDict.items():\n",
    "        paraphraseDataset = valueDataset.filter(lambda example: example['label'] == 1)\n",
    "\n",
    "        paraphraseDataset = paraphraseDataset.remove_columns([\"id\", \"label\"])\n",
    "    \n",
    "        newDatasetDict[key] = paraphraseDataset\n",
    "    \n",
    "    return newDatasetDict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74e774a-fb51-4ab4-96aa-57f7893c459b",
   "metadata": {},
   "source": [
    "Formateo del promt para optimizar la tarea del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b73dbc6-8033-4464-bdca-12542014d2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def formattingPrompts(examples, instruction):\n",
    "    inputs = examples[\"sentence1\"]\n",
    "    outputs = examples[\"sentence2\"]\n",
    "    texts = []\n",
    "    for inpt, output in zip(inputs, outputs):\n",
    "        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n",
    "        text = f\"### Instrucción:\\n{instruction}.\\n\\n### Entrada:\\n{inpt}\\n\\n### Respuesta:\\n{output}\"+ EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    return { \"text\" : texts, }\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6431aa09-2da7-49b3-9029-8cd51419b491",
   "metadata": {},
   "source": [
    "### Preparación de los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b8e3f27-448d-40ed-86f8-89af31ddfa7b",
   "metadata": {},
   "source": [
    "##### PAWSX filtrado\n",
    "Dado que solo queremos hacer uso del modelo en castellano, únicamente nos quedaremos con las frases en este idioma. Además, el conjunto de datos original contiene pares de frases que son paráfrasis y que no lo son. En nuestro caso, únicamente nos interesan las primeras, por lo que también se filtrarán."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5249aeee-0704-4c99-9b96-e157a8ea35d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "PAWSXDataDict = processPAWSX()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3e6676-17f0-4187-8250-0f975e279172",
   "metadata": {},
   "source": [
    "Estructura del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "31be53db-eb30-4c23-a236-d8491b7dc0a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['sentence1', 'sentence2'],\n",
      "        num_rows: 21829\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['sentence1', 'sentence2'],\n",
      "        num_rows: 907\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['sentence1', 'sentence2'],\n",
      "        num_rows: 847\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(PAWSXDataDict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceaa6fee-780f-489b-8422-a041134ea916",
   "metadata": {},
   "source": [
    "Diez ejemplos no consecutivos para cerciorarnos de que todos los datos son paráfrasis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cff64853-8f7c-47db-a8d5-ef87871728ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sentence1': 'El distrito electoral se encuentra en la bahía de Swansea, situada en la margen derecha del río Afan, cerca de su desembocadura en el sur de Gales.', 'sentence2': 'El distrito electoral se encuentra en Swansea Bay, en la orilla derecha del río Afan, cerca de su desembocadura en el sur de Gales.'}\n",
      "{'sentence1': \"Los `` Zapateros '' cayeron al 16º en 1991: 92, antes de caer en el 20º puesto en 1992: 93 bajo Phil Chard.\", 'sentence2': \"Los `` Zapateros '' cayeron al 16º en 1991: 92, antes de caer en el 20º puesto en 1992: 93 bajo Phil Chard.\"}\n",
      "{'sentence1': 'La ley albanesa está codificada y basada en la ley francesa.', 'sentence2': 'La ley albanesa está codificada y se basa en la ley francesa.'}\n",
      "{'sentence1': 'Nació en Usera, España (Madrid) el 18 de abril de 1976.', 'sentence2': 'Nació el 18 de abril de 1976 en Usera, España (Madrid).'}\n",
      "{'sentence1': 'Los pequeños blenios marinos blenioides (\"Ecsenius australianus\") son peces australianos del género \"Ecsenius\".', 'sentence2': \"Los pequeños Blennny de Blennioids marinos (`` Ecsenius australianus '') son peces australianos del género '' Ecsenius ''.\"}\n",
      "{'sentence1': \"El 24 de julio de 2013, John Hodgman apareció en el podcast Maximum Fun `` Judge Brother Ali '' como '' Expert Witness ''.\", 'sentence2': \"El 24 de julio de 2013, John Hodgman apareció en el podcast Maximum Fun `` Judge Brother Ali '' como un `` Expert Witness ''.\"}\n",
      "{'sentence1': 'El padre de Ted y Tom fue Steve.', 'sentence2': 'Steve fue el padre de Ted y Tom.'}\n",
      "{'sentence1': 'Danielle, Nadine y Mark tienen sus fotos visibles en la vitrina de la escuela.', 'sentence2': 'Mark, Nadine y Danielle han visto sus fotos en el caso de la escuela.'}\n",
      "{'sentence1': 'Cuando las empresas de agua y alcantarillado se privatizaron en Irlanda del Norte en 1989, estos servicios siguieron siendo públicos en Inglaterra, Gales y Escocia.', 'sentence2': 'Cuando las empresas de agua y alcantarillado se privatizaron en Irlanda del Norte en 1989, estos servicios siguieron siendo públicos en Inglaterra, Gales y Escocia.'}\n",
      "{'sentence1': \"Los únicos valores de `` n '' a 600000, para los cuales hay más primos impares pitagóricos que no pitagóricos, son, por ejemplo, 26861 y 26862.\", 'sentence2': \"Por ejemplo, los únicos valores de `` n '' hasta 600000 para los cuales hay más números primos impares pitagóricos que no pitagóricos son 26861 y 26862.\"}\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(PAWSXDataDict[\"train\"][1050 + i * 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3ddfe4-b6ce-4559-a19f-1c290c9d6265",
   "metadata": {},
   "source": [
    "##### Dataset propio\n",
    "Dado que hemos aplicado técnicas de aumento de datos, disponemos pares de frases (paráfrasis) adaptadas a nuestro dominio (reseñas de estaciones de metro). \n",
    "\n",
    "Se han aplicado 3 variaciones de la retrotraducción, por lo que solo con esto, se disponen de tres paraes de frases por cada elemento del conjunto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "65ef9823-6776-47ff-9af2-57ce09f4a8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "invalidOriginalPath = \"/home/ibon/Documentos/GitHub/TFG/1. Data/4. Labeled Reviews/2. Without Emojis/InvalidReviews.txt\"\n",
    "validOriginalPath = \"/home/ibon/Documentos/GitHub/TFG/1. Data/4. Labeled Reviews/2. Without Emojis/ValidReviews.txt\"\n",
    "\n",
    "backtranslationFolder = \"/home/ibon/Documentos/GitHub/TFG/2. Review Classifier/1. Data Augmentation/1. Back Translation/1. Google Translator\"\n",
    "invalidBacktranslationPath1 = backtranslationFolder + \"/InvalidReviewsTranslationsEsEnEnEs.txt\"\n",
    "invalidBacktranslationPath2 = backtranslationFolder + \"/InvalidReviewsTranslationsEsFrFrJaJaRuRuEs.txt\"\n",
    "invalidBacktranslationPath3 = backtranslationFolder + \"/InvalidReviewsTranslationsEsJaJaEs.txt\"\n",
    "validBacktranslationPath1 = backtranslationFolder + \"/ValidReviewsTranslationsEsEnEnEs.txt\"\n",
    "validBacktranslationPath2 = backtranslationFolder + \"/ValidReviewsTranslationsEsFrFrJaJaRuRuEs.txt\"\n",
    "validBacktranslationPath3 = backtranslationFolder + \"/ValidReviewsTranslationsEsJaJaEs.txt\"\n",
    "\n",
    "invalidParaphrasesDictList1 = importParaphraseFromTxtToDictList(invalidOriginalPath, invalidBacktranslationPath1)\n",
    "invalidParaphrasesDictList2 = importParaphraseFromTxtToDictList(invalidOriginalPath, invalidBacktranslationPath2)\n",
    "invalidParaphrasesDictList3 = importParaphraseFromTxtToDictList(invalidOriginalPath, invalidBacktranslationPath3)\n",
    "validParaphrasesDictList1 = importParaphraseFromTxtToDictList(validOriginalPath, validBacktranslationPath1)\n",
    "validParaphrasesDictList2 = importParaphraseFromTxtToDictList(validOriginalPath, validBacktranslationPath2)\n",
    "validParaphrasesDictList3 = importParaphraseFromTxtToDictList(validOriginalPath, validBacktranslationPath3)\n",
    "\n",
    "#Join all the lists\n",
    "backtranslationDictList = invalidParaphrasesDictList1 + invalidParaphrasesDictList2 + invalidParaphrasesDictList3 + validParaphrasesDictList1 + validParaphrasesDictList2 + validParaphrasesDictList3\n",
    "#Shuffle the lists\n",
    "random.shuffle(backtranslationDictList)\n",
    "\n",
    "#Convert it to a Hugging Face dataset\n",
    "backtranslationDataset = Dataset.from_list(backtranslationDictList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6b77b63b-7f5e-40ae-97e7-636aa8354270",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the data\n",
    "backtranslationDataDict = trainTestValidationSplit(backtranslationDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "08cd79cb-a934-4de6-8b41-ceea2413bbb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['sentence1', 'sentence2'],\n",
      "        num_rows: 7761\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['sentence1', 'sentence2'],\n",
      "        num_rows: 971\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['sentence1', 'sentence2'],\n",
      "        num_rows: 970\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(backtranslationDataDict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86243929-b9f9-4450-a6cc-4a4e34206bfa",
   "metadata": {},
   "source": [
    "##### Unión del PAWSX filtrado y el dataset propio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "94a86229-2d48-4887-83f8-715e058bd287",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Concatenate the datasets\n",
    "unionDatasetDict = DatasetDict()\n",
    "\n",
    "for key in PAWSXDataDict.keys():\n",
    "    newDataset = concatenate_datasets([PAWSXDataDict[key], backtranslationDataDict[key]])\n",
    "\n",
    "    unionDatasetDict[key] = newDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d38873f9-bcdc-4a57-b408-9fd5bb999169",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['sentence1', 'sentence2'],\n",
      "        num_rows: 29590\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['sentence1', 'sentence2'],\n",
      "        num_rows: 1878\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['sentence1', 'sentence2'],\n",
      "        num_rows: 1817\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(unionDatasetDict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42303da7-def2-4849-a5a9-84f2734eccc6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Diseño de los modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30388efa-e79e-4ee2-b97f-2ba5f8f25b2b",
   "metadata": {},
   "source": [
    "Se va a usar el paquete unsloth para cargar el modelo preentrenado poruqe permite una descarga y fine-tuning más rápidos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bb91c61c-9886-4418-83ca-12dbeac8092a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "🦥 Unsloth Zoo will now patch everything to make training faster!\n",
      "==((====))==  Unsloth 2025.2.12: Fast Llama patching. Transformers: 4.46.1.\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 3060. Max memory: 11.747 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.6. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "\n",
    "modelName = \"unsloth/DeepSeek-R1-Distill-Llama-8B-unsloth-bnb-4bit\"\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = modelName,\n",
    "    max_seq_length = 2048, #Maximum lenght of the input sequence that the model can process\n",
    "    dtype = None,\n",
    "    load_in_4bit = True,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97986b2-8536-4e68-9a9a-e1fb6ef7a253",
   "metadata": {},
   "source": [
    "Se añaden los adaptadores LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c09780db-e03d-47e7-8de1-e631f5a691bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Dropout = 0 is supported for fast patching. You are using dropout = 0.05.\n",
      "Unsloth will patch all other layers, except LoRA matrices, causing a performance hit.\n",
      "Unsloth 2025.2.12 patched 32 layers with 0 QKV layers, 0 O layers and 0 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 64, #Rank of the low-rank matrices in LoRA adaptation\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 32,\n",
    "    lora_dropout = 0.05, #Introduces dropout to the low-rank matrices during the training of this LoRA adapter model\n",
    "    bias = \"none\",    # Can be set to any, but = \"none\" is optimized\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3977,\n",
    "    use_rslora = False,  # unsloth also supports rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d8fbc7-f0aa-4e2d-9722-afeb12c0defd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Diseño del promt\n",
    "\n",
    "Un promt es una instrucción que se le da a un modelo de lenguaje para obtener una respuesta específica.\n",
    "\n",
    "La ingeniería de promting es una técnica para diseñar promts de manera estratégica para obtener las mejores respuestas posibles.\n",
    "\n",
    "Se va a añadir un campo a los datasets en el que se introduza el promt al completo (intrucción, frase original y frase parafraseada)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "51ed2131-4f5c-4c1b-9e88-e148aa38d2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction = \"Eres un experto en generación de datos sintéticos. Dado un texto de entrada, genera una paráfrasis que conserve el significado original pero utilice un vocabulario diferente y una estructura gramatical variada. El resultado debe ser útil para entrenar redes neuronales con datos sintéticos.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91e7f36-b65b-4d66-aaf4-79e14a00220b",
   "metadata": {},
   "source": [
    "Dataset con solo los datos obtenidos de la técnica de retrotraducción"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bc5fabe9-fee0-4a2d-8039-321de4fef82b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 7761/7761 [00:00<00:00, 122478.59 examples/s]\n",
      "Map: 100%|██████████| 971/971 [00:00<00:00, 111111.18 examples/s]\n",
      "Map: 100%|██████████| 970/970 [00:00<00:00, 101722.04 examples/s]\n"
     ]
    }
   ],
   "source": [
    "EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n",
    "backtranslationDataDict = backtranslationDataDict.map(lambda x: formattingPrompts(x, instruction), batched = True,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce3fb59-11fd-4a7f-9967-2abe63665542",
   "metadata": {},
   "source": [
    "Dataset con los datos de PAWS filtrado y los obtenidos mediante la retrotraducción"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fbe72549-a49d-4dde-a0a8-6cbf2d58c4f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 29590/29590 [00:00<00:00, 122347.29 examples/s]\n",
      "Map: 100%|██████████| 1878/1878 [00:00<00:00, 79050.45 examples/s]\n",
      "Map: 100%|██████████| 1817/1817 [00:00<00:00, 86339.90 examples/s]\n"
     ]
    }
   ],
   "source": [
    "unionDatasetDict = unionDatasetDict.map(lambda x: formattingPrompts(x, instruction), batched = True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "56325718-e409-40ca-9f7a-05e4870f039f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'text'],\n",
       "        num_rows: 29590\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'text'],\n",
       "        num_rows: 1878\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'text'],\n",
       "        num_rows: 1817\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unionDatasetDict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5f911a-9849-466f-9828-77cc5f68a7c1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Entrenamiento de los modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0c6dcd81-f8ae-4062-9fdc-58a4bb240a8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ibon/Programas/anaconda3/envs/TFG/lib/python3.12/site-packages/transformers/training_args.py:1559: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/home/ibon/Programas/anaconda3/envs/TFG/lib/python3.12/site-packages/transformers/training_args.py:1559: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "Tokenizing train dataset (num_proc=2): 100%|██████████| 29590/29590 [00:04<00:00, 6167.80 examples/s]\n",
      "Tokenizing train dataset (num_proc=2): 100%|██████████| 29590/29590 [00:01<00:00, 15565.52 examples/s]\n",
      "Tokenizing eval dataset (num_proc=2): 100%|██████████| 1817/1817 [00:01<00:00, 1779.10 examples/s]\n",
      "Tokenizing eval dataset (num_proc=2): 100%|██████████| 1817/1817 [00:00<00:00, 6649.24 examples/s]\n",
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "max_seq_length = 2048\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model, # The model with LoRA adapters\n",
    "    tokenizer = tokenizer, # The tokenizer of the model\n",
    "    train_dataset = unionDatasetDict[\"train\"], # The dataset to use for training\n",
    "    eval_dataset=unionDatasetDict[\"validation\"],\n",
    "    dataset_text_field = \"text\", # The field in the dataset that contains the structured data\n",
    "    max_seq_length = max_seq_length, # Max length of input sequence that the model can process\n",
    "    dataset_num_proc = 2, # Noe of processes to use for loading and processing the data\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 2, # Batch size per GPU\n",
    "        gradient_accumulation_steps = 4, # Step size of gradient accumulation\n",
    "        warmup_steps = 5,\n",
    "        # num_train_epochs = 1, # Set this for 1 full training run.\n",
    "        max_steps = 150, # Maximum steps of training\n",
    "        learning_rate = 5e-5, # Initial learning rate\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\", # The optimizer that will be used for updating the weights\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",\n",
    "        report_to = \"none\", # Use this for WandB etc\n",
    "        evaluation_strategy=\"steps\",  # Evaluation by steps\n",
    "        eval_steps=10,  # Evaluate every 10 steps\n",
    "        save_strategy=\"steps\",  # Save every 10 steps\n",
    "        save_steps=10,  # Save every 10 steps\n",
    "        load_best_model_at_end=True,  # Load the best model at the end\n",
    "        metric_for_best_model=\"loss\",  # Use the validation loss as the metric for the best model\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f6ec6a-19f6-4645-be72-9c706d64bdde",
   "metadata": {},
   "source": [
    "Entrenar y guardar el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044f95d0-b026-47c9-8178-eaa1f5357005",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 29,590 | Num Epochs = 1\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 4\n",
      "\\        /    Total batch size = 8 | Total steps = 150\n",
      " \"-____-\"     Number of trainable parameters = 167,772,160\n",
      "'UnslothSFTConfig' object has no attribute 'average_tokens_across_devices'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='151' max='150' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [150/150 1:40:05, Epoch 0.04/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.469100</td>\n",
       "      <td>11.189466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.411100</td>\n",
       "      <td>11.150325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.531800</td>\n",
       "      <td>11.136603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.521300</td>\n",
       "      <td>11.152895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.444200</td>\n",
       "      <td>11.170336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.555600</td>\n",
       "      <td>11.186615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.534200</td>\n",
       "      <td>11.188937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.962000</td>\n",
       "      <td>11.204674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.561300</td>\n",
       "      <td>11.212711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.537900</td>\n",
       "      <td>11.229830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.689000</td>\n",
       "      <td>11.229006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.980000</td>\n",
       "      <td>11.229245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.711900</td>\n",
       "      <td>11.230718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.657800</td>\n",
       "      <td>11.229288</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='820' max='909' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [820/909 05:38 < 00:36, 2.42 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'UnslothSFTConfig' object has no attribute 'average_tokens_across_devices'\n",
      "'UnslothSFTConfig' object has no attribute 'average_tokens_across_devices'\n",
      "'UnslothSFTConfig' object has no attribute 'average_tokens_across_devices'\n",
      "'UnslothSFTConfig' object has no attribute 'average_tokens_across_devices'\n",
      "'UnslothSFTConfig' object has no attribute 'average_tokens_across_devices'\n",
      "'UnslothSFTConfig' object has no attribute 'average_tokens_across_devices'\n",
      "'UnslothSFTConfig' object has no attribute 'average_tokens_across_devices'\n",
      "'UnslothSFTConfig' object has no attribute 'average_tokens_across_devices'\n",
      "'UnslothSFTConfig' object has no attribute 'average_tokens_across_devices'\n",
      "Unsloth: Not an error, but LlamaForCausalLM does not accept `num_items_in_batch`.\n",
      "Using gradient accumulation will be very slightly less accurate.\n",
      "Read more on gradient accumulation issues here: https://unsloth.ai/blog/gradient\n",
      "'UnslothSFTConfig' object has no attribute 'average_tokens_across_devices'\n",
      "'UnslothSFTConfig' object has no attribute 'average_tokens_across_devices'\n",
      "'UnslothSFTConfig' object has no attribute 'average_tokens_across_devices'\n",
      "'UnslothSFTConfig' object has no attribute 'average_tokens_across_devices'\n",
      "'UnslothSFTConfig' object has no attribute 'average_tokens_across_devices'\n",
      "'UnslothSFTConfig' object has no attribute 'average_tokens_across_devices'\n",
      "'UnslothSFTConfig' object has no attribute 'average_tokens_across_devices'\n",
      "'UnslothSFTConfig' object has no attribute 'average_tokens_across_devices'\n",
      "'UnslothSFTConfig' object has no attribute 'average_tokens_across_devices'\n",
      "'UnslothSFTConfig' object has no attribute 'average_tokens_across_devices'\n",
      "'UnslothSFTConfig' object has no attribute 'average_tokens_across_devices'\n",
      "'UnslothSFTConfig' object has no attribute 'average_tokens_across_devices'\n",
      "'UnslothSFTConfig' object has no attribute 'average_tokens_across_devices'\n",
      "'UnslothSFTConfig' object has no attribute 'average_tokens_across_devices'\n",
      "'UnslothSFTConfig' object has no attribute 'average_tokens_across_devices'\n",
      "'UnslothSFTConfig' object has no attribute 'average_tokens_across_devices'\n",
      "'UnslothSFTConfig' object has no attribute 'average_tokens_across_devices'\n",
      "'UnslothSFTConfig' object has no attribute 'average_tokens_across_devices'\n",
      "'UnslothSFTConfig' object has no attribute 'average_tokens_across_devices'\n",
      "'UnslothSFTConfig' object has no attribute 'average_tokens_across_devices'\n",
      "'UnslothSFTConfig' object has no attribute 'average_tokens_across_devices'\n",
      "'UnslothSFTConfig' object has no attribute 'average_tokens_across_devices'\n",
      "'UnslothSFTConfig' object has no attribute 'average_tokens_across_devices'\n",
      "'UnslothSFTConfig' object has no attribute 'average_tokens_across_devices'\n",
      "'UnslothSFTConfig' object has no attribute 'average_tokens_across_devices'\n",
      "'UnslothSFTConfig' object has no attribute 'average_tokens_across_devices'\n",
      "'UnslothSFTConfig' object has no attribute 'average_tokens_across_devices'\n",
      "'UnslothSFTConfig' object has no attribute 'average_tokens_across_devices'\n",
      "'UnslothSFTConfig' object has no attribute 'average_tokens_across_devices'\n",
      "'UnslothSFTConfig' object has no attribute 'average_tokens_across_devices'\n",
      "'UnslothSFTConfig' object has no attribute 'average_tokens_across_devices'\n",
      "'UnslothSFTConfig' object has no attribute 'average_tokens_across_devices'\n",
      "'UnslothSFTConfig' object has no attribute 'average_tokens_across_devices'\n",
      "'UnslothSFTConfig' object has no attribute 'average_tokens_across_devices'\n",
      "'UnslothSFTConfig' object has no attribute 'average_tokens_across_devices'\n",
      "'UnslothSFTConfig' object has no attribute 'average_tokens_across_devices'\n",
      "'UnslothSFTConfig' object has no attribute 'average_tokens_across_devices'\n",
      "'UnslothSFTConfig' object has no attribute 'average_tokens_across_devices'\n",
      "'UnslothSFTConfig' object has no attribute 'average_tokens_across_devices'\n",
      "'UnslothSFTConfig' object has no attribute 'average_tokens_across_devices'\n",
      "'UnslothSFTConfig' object has no attribute 'average_tokens_across_devices'\n",
      "'UnslothSFTConfig' object has no attribute 'average_tokens_across_devices'\n",
      "'UnslothSFTConfig' object has no attribute 'average_tokens_across_devices'\n",
      "'UnslothSFTConfig' object has no attribute 'average_tokens_across_devices'\n",
      "'UnslothSFTConfig' object has no attribute 'average_tokens_across_devices'\n",
      "'UnslothSFTConfig' object has no attribute 'average_tokens_across_devices'\n",
      "'UnslothSFTConfig' object has no attribute 'average_tokens_across_devices'\n",
      "'UnslothSFTConfig' object has no attribute 'average_tokens_across_devices'\n",
      "'UnslothSFTConfig' object has no attribute 'average_tokens_across_devices'\n",
      "'UnslothSFTConfig' object has no attribute 'average_tokens_across_devices'\n",
      "'UnslothSFTConfig' object has no attribute 'average_tokens_across_devices'\n",
      "'UnslothSFTConfig' object has no attribute 'average_tokens_across_devices'\n",
      "'UnslothSFTConfig' object has no attribute 'average_tokens_across_devices'\n",
      "'UnslothSFTConfig' object has no attribute 'average_tokens_across_devices'\n",
      "'UnslothSFTConfig' object has no attribute 'average_tokens_across_devices'\n",
      "'UnslothSFTConfig' object has no attribute 'average_tokens_across_devices'\n",
      "'UnslothSFTConfig' object has no attribute 'average_tokens_across_devices'\n",
      "'UnslothSFTConfig' object has no attribute 'average_tokens_across_devices'\n",
      "'UnslothSFTConfig' object has no attribute 'average_tokens_across_devices'\n",
      "'UnslothSFTConfig' object has no attribute 'average_tokens_across_devices'\n",
      "'UnslothSFTConfig' object has no attribute 'average_tokens_across_devices'\n",
      "'UnslothSFTConfig' object has no attribute 'average_tokens_across_devices'\n",
      "'UnslothSFTConfig' object has no attribute 'average_tokens_across_devices'\n",
      "'UnslothSFTConfig' object has no attribute 'average_tokens_across_devices'\n",
      "'UnslothSFTConfig' object has no attribute 'average_tokens_across_devices'\n",
      "'UnslothSFTConfig' object has no attribute 'average_tokens_across_devices'\n",
      "'UnslothSFTConfig' object has no attribute 'average_tokens_across_devices'\n",
      "'UnslothSFTConfig' object has no attribute 'average_tokens_across_devices'\n",
      "'UnslothSFTConfig' object has no attribute 'average_tokens_across_devices'\n",
      "'UnslothSFTConfig' object has no attribute 'average_tokens_across_devices'\n",
      "'UnslothSFTConfig' object has no attribute 'average_tokens_across_devices'\n",
      "'UnslothSFTConfig' object has no attribute 'average_tokens_across_devices'\n",
      "'UnslothSFTConfig' object has no attribute 'average_tokens_across_devices'\n",
      "'UnslothSFTConfig' object has no attribute 'average_tokens_across_devices'\n",
      "'UnslothSFTConfig' object has no attribute 'average_tokens_across_devices'\n",
      "'UnslothSFTConfig' object has no attribute 'average_tokens_across_devices'\n",
      "'UnslothSFTConfig' object has no attribute 'average_tokens_across_devices'\n",
      "'UnslothSFTConfig' object has no attribute 'average_tokens_across_devices'\n",
      "'UnslothSFTConfig' object has no attribute 'average_tokens_across_devices'\n",
      "'UnslothSFTConfig' object has no attribute 'average_tokens_across_devices'\n",
      "'UnslothSFTConfig' object has no attribute 'average_tokens_across_devices'\n",
      "'UnslothSFTConfig' object has no attribute 'average_tokens_across_devices'\n",
      "'UnslothSFTConfig' object has no attribute 'average_tokens_across_devices'\n",
      "'UnslothSFTConfig' object has no attribute 'average_tokens_across_devices'\n",
      "'UnslothSFTConfig' object has no attribute 'average_tokens_across_devices'\n",
      "'UnslothSFTConfig' object has no attribute 'average_tokens_across_devices'\n",
      "'UnslothSFTConfig' object has no attribute 'average_tokens_across_devices'\n",
      "'UnslothSFTConfig' object has no attribute 'average_tokens_across_devices'\n",
      "'UnslothSFTConfig' object has no attribute 'average_tokens_across_devices'\n",
      "'UnslothSFTConfig' object has no attribute 'average_tokens_across_devices'\n",
      "'UnslothSFTConfig' object has no attribute 'average_tokens_across_devices'\n",
      "'UnslothSFTConfig' object has no attribute 'average_tokens_across_devices'\n",
      "'UnslothSFTConfig' object has no attribute 'average_tokens_across_devices'\n",
      "'UnslothSFTConfig' object has no attribute 'average_tokens_across_devices'\n",
      "'UnslothSFTConfig' object has no attribute 'average_tokens_across_devices'\n",
      "'UnslothSFTConfig' object has no attribute 'average_tokens_across_devices'\n",
      "'UnslothSFTConfig' object has no attribute 'average_tokens_across_devices'\n",
      "'UnslothSFTConfig' object has no attribute 'average_tokens_across_devices'\n",
      "'UnslothSFTConfig' object has no attribute 'average_tokens_across_devices'\n",
      "'UnslothSFTConfig' object has no attribute 'average_tokens_across_devices'\n",
      "'UnslothSFTConfig' object has no attribute 'average_tokens_across_devices'\n",
      "'UnslothSFTConfig' object has no attribute 'average_tokens_across_devices'\n",
      "'UnslothSFTConfig' object has no attribute 'average_tokens_across_devices'\n",
      "'UnslothSFTConfig' object has no attribute 'average_tokens_across_devices'\n",
      "'UnslothSFTConfig' object has no attribute 'average_tokens_across_devices'\n",
      "'UnslothSFTConfig' object has no attribute 'average_tokens_across_devices'\n",
      "'UnslothSFTConfig' object has no attribute 'average_tokens_across_devices'\n",
      "'UnslothSFTConfig' object has no attribute 'average_tokens_across_devices'\n",
      "'UnslothSFTConfig' object has no attribute 'average_tokens_across_devices'\n",
      "'UnslothSFTConfig' object has no attribute 'average_tokens_across_devices'\n",
      "'UnslothSFTConfig' object has no attribute 'average_tokens_across_devices'\n",
      "'UnslothSFTConfig' object has no attribute 'average_tokens_across_devices'\n",
      "'UnslothSFTConfig' object has no attribute 'average_tokens_across_devices'\n",
      "'UnslothSFTConfig' object has no attribute 'average_tokens_across_devices'\n",
      "'UnslothSFTConfig' object has no attribute 'average_tokens_across_devices'\n",
      "'UnslothSFTConfig' object has no attribute 'average_tokens_across_devices'\n",
      "'UnslothSFTConfig' object has no attribute 'average_tokens_across_devices'\n",
      "'UnslothSFTConfig' object has no attribute 'average_tokens_across_devices'\n",
      "'UnslothSFTConfig' object has no attribute 'average_tokens_across_devices'\n",
      "'UnslothSFTConfig' object has no attribute 'average_tokens_across_devices'\n",
      "'UnslothSFTConfig' object has no attribute 'average_tokens_across_devices'\n",
      "'UnslothSFTConfig' object has no attribute 'average_tokens_across_devices'\n",
      "'UnslothSFTConfig' object has no attribute 'average_tokens_across_devices'\n",
      "'UnslothSFTConfig' object has no attribute 'average_tokens_across_devices'\n",
      "'UnslothSFTConfig' object has no attribute 'average_tokens_across_devices'\n",
      "'UnslothSFTConfig' object has no attribute 'average_tokens_across_devices'\n",
      "'UnslothSFTConfig' object has no attribute 'average_tokens_across_devices'\n",
      "'UnslothSFTConfig' object has no attribute 'average_tokens_across_devices'\n",
      "'UnslothSFTConfig' object has no attribute 'average_tokens_across_devices'\n",
      "'UnslothSFTConfig' object has no attribute 'average_tokens_across_devices'\n",
      "'UnslothSFTConfig' object has no attribute 'average_tokens_across_devices'\n",
      "'UnslothSFTConfig' object has no attribute 'average_tokens_across_devices'\n",
      "'UnslothSFTConfig' object has no attribute 'average_tokens_across_devices'\n",
      "'UnslothSFTConfig' object has no attribute 'average_tokens_across_devices'\n",
      "'UnslothSFTConfig' object has no attribute 'average_tokens_across_devices'\n",
      "'UnslothSFTConfig' object has no attribute 'average_tokens_across_devices'\n",
      "'UnslothSFTConfig' object has no attribute 'average_tokens_across_devices'\n",
      "'UnslothSFTConfig' object has no attribute 'average_tokens_across_devices'\n",
      "'UnslothSFTConfig' object has no attribute 'average_tokens_across_devices'\n",
      "'UnslothSFTConfig' object has no attribute 'average_tokens_across_devices'\n"
     ]
    }
   ],
   "source": [
    "trainer_stats = trainer.train()\n",
    "\n",
    "version = 1\n",
    "\n",
    "#Save the model\n",
    "savingPath = \"/home/ibon/Documentos/1. Models/DeepSeek-R1\"\n",
    "tokenizer.save_pretrained(savingPath + \"/\" + modelName.replace(\"/\", \"%\") + \"v\" + str(version))\n",
    "model.save_pretrained(savingPath + \"/\" + modelName.replace(\"/\", \"%\") + \"v\" + str(version))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "aefb9bf2-a27e-4b54-9fc0-97b5523e9e0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<｜begin▁of▁sentence｜>### Instrucción:\\nEres un experto en generación de datos sintéticos. Dado un texto de entrada, genera una paráfrasis que conserve el significado original pero utilice un vocabulario diferente y una estructura gramatical variada. El resultado debe ser útil para entrenar redes neuronales con datos sintéticos.\\n\\n### Entrada:\\nEl servicio es aceptable pero la etación es muy pequeña\\n\\n### Respuesta:\\nEl servicio es aceptable, pero la estación es muy pequeña.<｜end▁of▁sentence｜>']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "inpt = \"El servicio es aceptable pero la etación es muy pequeña\"\n",
    "text = f\"### Instrucción:\\n{instruction}\\n\\n### Entrada:\\n{inpt}\\n\\n### Respuesta:\\n\"\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    text\n",
    "], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)\n",
    "tokenizer.batch_decode(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8839403d-dc3e-4599-b3cf-c15419a3497b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Generación de paráfrasis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ead737-c878-4fe5-a46e-92b6ccb9dd35",
   "metadata": {},
   "source": [
    "Cargar el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94dc94b0-a4ef-45cc-8100-cbda28b4b4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n",
    "savingPath = \"/home/ibon/Documentos/1. Models/DeepSeek-R1\"\n",
    "\n",
    "#Path and version of the model\n",
    "version = 1\n",
    "modelPath = savingPath + \"/\" + modelName.replace(\"/\", \"%\") + \"v\" + str(version)\n",
    "\n",
    "#Load the tokenizer and the model\n",
    "tokenizer = AutoTokenizer.from_pretrained(modelPath)\n",
    "model = FastLanguageModel.for_inference(modelPath) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f03644-7135-4a12-a3d2-a1c49f974585",
   "metadata": {},
   "source": [
    "Generar las paráfrasis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5de5b5-d4fb-42e5-a120-22df53c019a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction = \"Eres un experto en generación de datos sintéticos. Dado un texto de entrada, genera una paráfrasis que conserve el significado original pero utilice un vocabulario diferente y una estructura gramatical variada. El resultado debe ser útil para entrenar redes neuronales con datos sintéticos.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc47a71-84cd-460c-900e-f81cb642f5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "inpt = \"El servicio es aceptable pero la etación es muy pequeña\"\n",
    "text = f\"### Instrucción:\\n{instruction}\\n\\n### Entrada:\\n{inpt}\\n\\n### Respuesta:\\n\"\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    text\n",
    "], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens = 200, use_cache = True, temperature = 0.7)\n",
    "tokenizer.batch_decode(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7594325-4f46-4af1-a090-28896f87fef9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Aumento de datos utilizando modelos de última generación vía API\n",
    "\n",
    "Debido a las limitaciones hardware del equipo, la única solución viable para conseguir paráfrases de calidad usando redes neuronales es acceder a modelos vía API. Concretamente, se accederán a los modelos de OpenAI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed59379-d5f3-45da-8220-001c89f5c1fb",
   "metadata": {},
   "source": [
    "## Generación"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd7d487-8f57-465b-a068-9388804ded32",
   "metadata": {},
   "source": [
    "### Imports necesarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b29b7a28-19ae-460c-af72-177571372102",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7766b38-afd8-4ada-8685-4ab7157d07d0",
   "metadata": {},
   "source": [
    "### Cargar las variables de entorno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "afb239ac-2a81-4a32-ba20-abb5b0b27222",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar variables de entorno desde .env\n",
    "load_dotenv()\n",
    "\n",
    "# Obtener la clave API\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0697b8-4388-4df4-a858-4a8fb2a73ded",
   "metadata": {},
   "source": [
    "### Funciones que se van a utilizar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897d3cca-8a4e-45ba-a36c-385ffbb5dede",
   "metadata": {},
   "source": [
    "Función que hace una petición a la API de OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e58da95f-b8cb-421b-bddf-4d5aeb1f3dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateText(model, systemMessageContent, prompt):\n",
    "    systemMessage = {\"role\" : \"system\", \"content\" : systemMessageContent}\n",
    "    userMessage = {\"role\" : \"user\", \"content\" : prompt}\n",
    "    messages = [systemMessage, userMessage]\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model = model,\n",
    "        messages = messages\n",
    "    )\n",
    "\n",
    "    \n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b1f639-61a1-4858-8dfb-5c690097b4e6",
   "metadata": {},
   "source": [
    "### Generación de las paráfrasis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ec1c26-12f8-4e18-b7a3-2dcc5aa90dea",
   "metadata": {},
   "source": [
    "#### Diseño del mensaje del sistema\n",
    "\n",
    "Este mensaje del sistema está diseñado para asegurar que el modelo mantenga el significado original, use diferentes estructuras gramaticales y varíe el vocabulario sin perder claridad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a8ddaa7-2a80-4f4b-9908-ae06a86333ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "systemMessageContent = f\"\"\"Eres un experto en procesamiento del lenguaje natural con habilidades avanzadas en reformulación de textos.\n",
    "Tu tarea es parafrasear un texto de manera que el significado se mantenga intacto, pero con cambios sustanciales en la estructura sintáctica y el vocabulario. \n",
    "Usa sinónimos, reformulaciones y construcciones alternativas para evitar repeticiones. \n",
    "Evita cambiar el tono y la intención del mensaje original. \n",
    "Si el texto contiene términos técnicos o nombres propios, consérvalos sin cambios.\n",
    "No agregues ni elimines información que altere el significado original.\n",
    "La respuesta debes proporcionarla en una linea\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386d977f-2460-474e-987b-fcf666118087",
   "metadata": {},
   "source": [
    "#### Diseño del prompt\n",
    "\n",
    "El prompt está diseñado con la misma intención que el mensaje del sistema. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9813d17d-f214-40f5-8cce-6d2b2df37136",
   "metadata": {},
   "outputs": [],
   "source": [
    "def promptFunction(originalText):\n",
    "\n",
    "    prompt = f\"\"\" Parafrasea el siguiente texto asegurando una alta variabilidad léxica y sintáctica, pero conservando el significado original:\n",
    "    \n",
    "    {originalText}\n",
    "    \n",
    "    Usa sinónimos y expresiones equivalentes.  \n",
    "    Cambia la estructura de las oraciones sin perder claridad.  \n",
    "    Mantén el tono y la intención del mensaje.  \n",
    "    No agregues ni elimines información clave.  \n",
    "    Asegúrate de que la reformulación sea natural y fluida. \n",
    "    Asegúrate de que la respuesta está toda en una linea.\n",
    "    \n",
    "    Respuesta: \n",
    "    \"\"\"\n",
    "\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a9b518-fb39-44b0-b0d0-3d5f06cfe353",
   "metadata": {},
   "source": [
    "#### Selección del modelo\n",
    "\n",
    "Como se puede ver, con un modelo económico como gpt-4o-mini se obtinen resultados de muy alta calidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a69ca44a-8cf6-40c4-8fb2-74829ef29f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"gpt-4o-mini\"\n",
    "\n",
    "text = \"La estación es antigua, aparte de tener una sola salida de metro no está habilitada para personas con discapacidad. Además comunica por un pasillo subterráneo (que parece infinito) con Embajadores que pertenece a la línea 3 de metro\"\n",
    "\n",
    "parafrasis = generateText(model, systemMessageContent, promptFunction(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "48fe343e-d910-4be8-80c2-b57dca4a48ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La estación es vieja y, además de contar con una única entrada de metro, no está adaptada para personas con discapacidades; también conecta con Embajadores, que forma parte de la línea 3 de metro, a través de un pasillo subterráneo que parece no tener fin.\n"
     ]
    }
   ],
   "source": [
    "print(parafrasis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a8cc59-e22d-48ec-b1f7-19b76bce0489",
   "metadata": {},
   "source": [
    "#### Parafrasis sobre el conjunto al completo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd527429-7132-4d85-a842-530430139bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateParaphrasing(inputPath, outputPath):\n",
    "\n",
    "    #Open input and output file\n",
    "    with open(inputPath, \"r\", encoding = \"utf-8\") as inputFile, open(outputPath, \"w\", encoding = \"utf-8\") as outputFile:\n",
    "        #Paraphrase every line in the input file\n",
    "        for line in inputFile:\n",
    "            \n",
    "            line = line.strip()\n",
    "            #Paraphrase\n",
    "            paraphrase = generateText(model, systemMessageContent, promptFunction(line))\n",
    "            #Write to the output file\n",
    "            outputFile.write('\"' + paraphrase + '\"\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "914ad903-d9c4-40dd-8e02-1a1d51c944c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "invalidInputPath = \"/home/ibon/Documentos/GitHub/TFG/1. Data/4. Labeled Reviews/2. Without Emojis/InvalidReviews.txt\"\n",
    "validInputPath = \"/home/ibon/Documentos/GitHub/TFG/1. Data/4. Labeled Reviews/2. Without Emojis/ValidReviews.txt\"\n",
    "\n",
    "invalidOutputPath = \"/home/ibon/Documentos/GitHub/TFG/2. Review Classifier/1. Data Augmentation/2. Pretrained NLP/GPT4o/invalidParaphrasing.txt\"\n",
    "validOutputPath = \"/home/ibon/Documentos/GitHub/TFG/2. Review Classifier/1. Data Augmentation/2. Pretrained NLP/GPT4o/validParaphrasing.txt\"\n",
    "\n",
    "generateParaphrasing(invalidInputPath, invalidOutputPath)\n",
    "generateParaphrasing(validInputPath, validOutputPath)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f124c11-414f-4ab8-8c16-485425631bb0",
   "metadata": {},
   "source": [
    "## Análisis de la generación"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd9b169-89be-45e6-a518-f57c22818a4e",
   "metadata": {},
   "source": [
    "### Similitud Semántica\n",
    "\n",
    "A continuación se va a diseñar una función para calcular la similitud semántica entre pares de oraciones. Es decir, se van a calcular los embeddings de oraciones de cada par de frases y se va a usar una métrica de similitud para ver como de parecido es el significado de ambas frases.\n",
    "\n",
    "Se va a usar una versión de SBERT, llamada MiniLM (Minimal Lenguaje Model), que utiliza una variante más pequeña. Se usa MiniLM de seis capas (L6), que logra una precisón buena con menos recursos.\n",
    "\n",
    "Este modelo fue entrenado usando un dataset que incluye datos en varios idiomas, entre ellos el español. Consecuentemente, no hay problema al introducir frases en castellano. Es cierto, que obtiene mejores resultados para frases en inglés, ya que se entreno con más datos en este idioma.\n",
    "\n",
    "MiniLM es un modelo específicamente entrenado para mapear frases y parrafos a un espacio vectorial de 384 dimensiones. Es decir, este modelo permite obtener un embedding de una frase directamente. Usando otros modelos esta tarea no es posible de forma directa, ya que devuelven un embedding para cada palabra del texto.\n",
    "\n",
    "El método de similitud que se va a usar es la similitud del coseno, por lo que los valores más cercanos a uno indicarán una mayor similitud entre las frases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "339ecffa-2d9b-4ab4-9d48-b0dd35f24231",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "#Given two texts and a model, the semantic similarity of the texts is returned\n",
    "def getSemanticSimilarity(text1, text2, model):\n",
    "    #Get the embeddings of the senteces\n",
    "    embedding1 = model.encode(text1)\n",
    "    embedding2 = model.encode(text2)\n",
    "\n",
    "    #Get the cosine similarity of the senteces\n",
    "    similarity = cosine_similarity([embedding1], [embedding2])\n",
    "\n",
    "    return similarity[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151db5ef-7b96-42af-bf97-412d15db5b29",
   "metadata": {},
   "source": [
    "### Similitud Léxica\n",
    "Se va a diseñar una función para calcular la similitud léxica entre pares de oraciones. La similitud léxica mide el grado de coincidencia de palabras o términos entre dos frases o textos, sin tener en cuenta el significado subyacente.\n",
    "\n",
    "Hay varias formas de realizar este cálculo: similitud del coseno basada en frecuencia de palabras, coeficiente de Jaccard,coeficiente de Dice ...\n",
    "\n",
    "En este caso, se cree que la mejor opción es usar el coeficiente de Jaccard ya que calcula la similitud en función de la proporción de palabras comunes sobre el total de palabras únicas. Consecuentemente, esto nos permitirá detectar frases con menos coincidencias exactas en palabras.\n",
    "\n",
    "Cuanto más cercano a uno sea el coeficiente de Jaccard más similares léxicamente serán las frases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0e7c276-c84e-4c31-8f78-ed30ce14efd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import re\n",
    "\n",
    "#Clean up the text removing punctuation, accent marks and convertin everything to lowercase\n",
    "def cleanText(text):\n",
    "    text = unicodedata.normalize('NFKD', text.lower()).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "554d4240-7131-4de7-a256-3c18259dd3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSpanishStopWords():\n",
    "    determinantes = {\"el\", \"la\", \"los\", \"las\", \"un\", \"una\", \"unos\", \"unas\", \"este\", \"esta\", \"estos\", \"estas\",\n",
    "                 \"ese\", \"esa\", \"esos\", \"esas\", \"aquel\", \"aquella\", \"aquellos\", \"aquellas\", \"mi\", \"mis\",\n",
    "                 \"tu\", \"tus\", \"su\", \"sus\", \"nuestro\", \"nuestra\", \"nuestros\", \"nuestras\", \"vuestro\", \n",
    "                 \"vuestra\", \"vuestros\", \"vuestras\", \"primer\", \"primero\", \"primera\", \"segundo\", \"segunda\"}\n",
    "\n",
    "    preposiciones = {\"a\", \"ante\", \"bajo\", \"cabe\", \"con\", \"contra\", \"de\", \"desde\", \"durante\", \"en\", \"entre\", \n",
    "                 \"hacia\", \"hasta\", \"mediante\", \"para\", \"por\", \"según\", \"sin\", \"sobre\", \"tras\", \"versus\", \"vía\"}\n",
    "\n",
    "    conjunciones = {\"y\", \"e\", \"ni\", \"o\", \"u\", \"pero\", \"sino\", \"sino que\", \"mas\", \"aunque\", \"que\", \"porque\", \n",
    "                \"como\", \"cuando\", \"donde\", \"mientras\", \"para que\", \"a fin de que\", \"puesto que\", \"ya que\", \n",
    "                \"si\", \"siempre que\"}\n",
    "    pronombres = {\n",
    "        # Pronombres personales\n",
    "        \"yo\", \"tú\", \"vos\", \"él\", \"ella\", \"nosotros\", \"nosotras\", \n",
    "        \"vosotros\", \"vosotras\", \"ellos\", \"ellas\", \"usted\", \"ustedes\",\n",
    "        \"me\", \"te\", \"lo\", \"la\", \"nos\", \"os\", \"los\", \"las\", \"le\", \"les\", \"se\",\n",
    "    \n",
    "        # Pronombres posesivos\n",
    "        \"mío\", \"mía\", \"míos\", \"mías\", \n",
    "        \"tuyo\", \"tuya\", \"tuyos\", \"tuyas\", \n",
    "        \"suyo\", \"suya\", \"suyos\", \"suyas\", \n",
    "        \"nuestro\", \"nuestra\", \"nuestros\", \"nuestras\", \n",
    "        \"vuestro\", \"vuestra\", \"vuestros\", \"vuestras\",\n",
    "    \n",
    "        # Pronombres demostrativos\n",
    "        \"este\", \"esta\", \"estos\", \"estas\", \n",
    "        \"ese\", \"esa\", \"esos\", \"esas\", \n",
    "        \"aquel\", \"aquella\", \"aquellos\", \"aquellas\",\n",
    "    \n",
    "        # Pronombres relativos\n",
    "        \"que\", \"cual\", \"cuales\", \"quien\", \"quienes\", \n",
    "        \"cuyo\", \"cuya\", \"cuyos\", \"cuyas\", \"donde\",\n",
    "    \n",
    "        # Pronombres interrogativos y exclamativos\n",
    "        \"qué\", \"quién\", \"quiénes\", \"cuál\", \"cuáles\", \n",
    "        \"cuánto\", \"cuánta\", \"cuántos\", \"cuántas\", \n",
    "        \"dónde\", \"cómo\", \"cuándo\",\n",
    "    \n",
    "        # Pronombres indefinidos\n",
    "        \"alguien\", \"algo\", \"nadie\", \"nada\", \"cualquiera\", \n",
    "        \"todos\", \"todas\", \"varios\", \"varias\", \"muchos\", \n",
    "        \"muchas\", \"pocos\", \"pocas\", \"alguno\", \"alguna\", \n",
    "        \"algunos\", \"algunas\", \"ninguno\", \"ninguna\", \n",
    "        \"uno\", \"una\", \"unos\", \"unas\", \"demás\"\n",
    "    }\n",
    "\n",
    "    #Combine all the words in one set\n",
    "    spanishStopWords = determinantes | preposiciones | conjunciones | pronombres\n",
    "\n",
    "    return spanishStopWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6130aca6-fa82-47d3-b56e-0d9dcf7ceb3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def revomeSpanishStopWords(text):\n",
    "    spanishStopWords = getSpanishStopWords()\n",
    "\n",
    "    textWithoutStopWords = [word for word in text.split() if word.lower() not in spanishStopWords]\n",
    "\n",
    "    return \" \".join(textWithoutStopWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4122568a-1731-402a-add6-ef36c62bb11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Jaccard similarity\n",
    "def jaccardSimilarity(text1, text2):\n",
    "    #Get the set of words of each text\n",
    "    wordsInText1 = set(revomeSpanishStopWords(cleanText(text1)).split())\n",
    "    wordsInText2 = set(revomeSpanishStopWords(cleanText(text2)).split())\n",
    "\n",
    "    intersection = len(wordsInText1.intersection(wordsInText2)) \n",
    "    union = len(wordsInText1.union(wordsInText2))\n",
    "    \n",
    "    if union == 0:\n",
    "        return 0\n",
    "\n",
    "    #intersection / union\n",
    "    return intersection / union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb36e48d-2b4d-4f82-a5e3-e4dc36a58e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Given two texts, the Jaccard similarity of those texts is returned\n",
    "def getLexicalSimilarity(text1, text2):\n",
    "    return jaccardSimilarity(text1, text2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33bee1b-a994-43fa-ba71-938cb393f3aa",
   "metadata": {},
   "source": [
    "### Similitudes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b468ab0e-d13c-496f-8fb4-52fe1ab38723",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ibon/Programas/anaconda3/envs/TFG/lib/python3.12/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "2025-03-01 17:35:01.707689: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-03-01 17:35:01.715174: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1740846901.725243    6676 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1740846901.728274    6676 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-03-01 17:35:01.738912: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "#originalDataList: list of texts representing the original dataset\n",
    "#allAugmentedDataList: list of list of texts representing the aumented data \n",
    "#(allAugmentedDataList = [augmentedDataList1, ... ,augmentedDataListN], where augmentedDataList = [augmentedData1, ..., augmentedDataM])\n",
    "#pathWithOriginal: path of the csv with two columns (the original text and the best augmented text)\n",
    "#pathAugmentedData: path of the file with only the augmented data (without the original text)\n",
    "def processAugmentation(originalDataList, allAugmentedDataList, pathWithOriginal, pathAugmentedData):\n",
    "    #Select the model for the semantic similarity\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "    #Open the files in which the augmented data will be strored\n",
    "    withOriginalFile = open(pathWithOriginal, \"w\", encoding=\"utf-8\")\n",
    "    augmentedDataFile = open(pathAugmentedData, \"w\", encoding=\"utf-8\")\n",
    "\n",
    "    #Write the titles of the csv\n",
    "    withOriginalFile.write(\"OriginalText,AugmentedText,SemanticSimilarity,LexicalSimilarity\\n\")\n",
    "    \n",
    "    resul = []\n",
    "    #Analize every phrase in the original data\n",
    "    for i, originalText in enumerate(originalDataList):\n",
    "        allAugmentedDataInfoDict = {}\n",
    "        bestIdx = 1\n",
    "        \n",
    "        #Analize every traduction\n",
    "        for j, augmentedDataList in enumerate(allAugmentedDataList):\n",
    "            #Compute the similarities of the corresponding traduction\n",
    "            semanticSimilarity = getSemanticSimilarity(originalText, augmentedDataList[i], model)\n",
    "            lexicalSimilarity = getLexicalSimilarity(originalText, augmentedDataList[i])\n",
    "\n",
    "            #Save the traduction and the similarities in a dictionary\n",
    "            allAugmentedDataInfoDict.update({\n",
    "                f\"augmented{j + 1}\": augmentedDataList[i],\n",
    "                f\"semanticSimilarity{j + 1}\": semanticSimilarity,\n",
    "                f\"lexicalSimilarity{j + 1}\": lexicalSimilarity\n",
    "            })\n",
    "\n",
    "            #Get the index of the traduction with greater semantic similarity and less lexical similarity\n",
    "            bestIdx = max(bestIdx, j + 1,\n",
    "                key = lambda k: (allAugmentedDataInfoDict[f\"semanticSimilarity{k}\"] - allAugmentedDataInfoDict[f\"lexicalSimilarity{k}\"])\n",
    "            )\n",
    "\n",
    "        #Select the information of the best augmentation\n",
    "        info = {\n",
    "            \"originalText\": originalText,\n",
    "            \"bestAugmentation\": allAugmentedDataInfoDict[f\"augmented{bestIdx}\"],\n",
    "            \"bestAugmentedDataSemanticSimilarity\": allAugmentedDataInfoDict[f\"semanticSimilarity{bestIdx}\"],\n",
    "            \"bestAugmentedDataLexicalSimiliratity\": allAugmentedDataInfoDict[f\"lexicalSimilarity{bestIdx}\"]\n",
    "        }\n",
    "        info.update(allAugmentedDataInfoDict)\n",
    "\n",
    "        #Save the information\n",
    "        resul.append(info)\n",
    "\n",
    "        #Write the information in the files\n",
    "        withOriginalFile.write(originalText + \",\" + allAugmentedDataInfoDict[f\"augmented{bestIdx}\"] + \",\" + str(allAugmentedDataInfoDict[f\"semanticSimilarity{bestIdx}\"]) + \",\" + str(allAugmentedDataInfoDict[f\"lexicalSimilarity{bestIdx}\"]) + \"\\n\")\n",
    "        #If the text is not empty write it on  the file\n",
    "        if allAugmentedDataInfoDict[f\"augmented{bestIdx}\"]  != '\"\"':\n",
    "            augmentedDataFile.write(allAugmentedDataInfoDict[f\"augmented{bestIdx}\"] + \"\\n\")\n",
    "\n",
    "    #Close the files\n",
    "    withOriginalFile.close()\n",
    "    augmentedDataFile.close()\n",
    "    \n",
    "    return resul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a1fdf5d3-6d32-4cfa-8b7a-0eab4d18aa4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def importFromTxtToList(source):\n",
    "    with open(source, 'r', encoding=\"utf-8\") as file:\n",
    "        #Generate a list with all the reviews\n",
    "        targetList = [line.strip() for line in file]\n",
    "    return targetList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4d324cba-0ae4-474e-b1b0-65333dc6d37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "invalidPath = \"/home/ibon/Documentos/GitHub/TFG/1. Data/4. Labeled Reviews/2. Without Emojis/InvalidReviews.txt\"\n",
    "validPath = \"/home/ibon/Documentos/GitHub/TFG/1. Data/4. Labeled Reviews/2. Without Emojis/ValidReviews.txt\"\n",
    "\n",
    "invalidList = importFromTxtToList(invalidPath)\n",
    "validList = importFromTxtToList(validPath)\n",
    "\n",
    "invalidParaphrasedPath = \"/home/ibon/Documentos/GitHub/TFG/2. Review Classifier/1. Data Augmentation/2. Pretrained NLP/GPT4oMini/invalidParaphrasing.txt\"\n",
    "validParaphrasedPath = \"/home/ibon/Documentos/GitHub/TFG/2. Review Classifier/1. Data Augmentation/2. Pretrained NLP/GPT4oMini/validParaphrasing.txt\"\n",
    "\n",
    "invalidParaphrasedList = importFromTxtToList(invalidParaphrasedPath)\n",
    "validParaphrasedList = importFromTxtToList(validParaphrasedPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1947f635-6906-43a7-a637-5655c93e4684",
   "metadata": {},
   "outputs": [],
   "source": [
    "validWithOriginalPath = '2. Pretrained NLP/2. Augmented Data/ValidGPT4oMiniWithOriginal.csv'\n",
    "validAugmentedPath = '2. Pretrained NLP/2. Augmented Data/ValidGPT4oMiniData.txt'\n",
    "invalidWithOriginalPath = '2. Pretrained NLP/2. Augmented Data/InvalidGPT4oMiniWithOriginal.csv'\n",
    "invalidAugmentedPath = '2. Pretrained NLP/2. Augmented Data/InvalidGPT4oMiniData.txt'\n",
    "\n",
    "infoValid = processAugmentation(validList, [validParaphrasedList], validWithOriginalPath, validAugmentedPath)\n",
    "infoValidDF = pd.DataFrame(infoValid)\n",
    "infoInvalid = processAugmentation(invalidList, [invalidParaphrasedList], invalidWithOriginalPath, invalidAugmentedPath)\n",
    "infoInvalidDF = pd.DataFrame(infoInvalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "239bf33b-0717-4b17-9e57-a5ed0524ee2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>originalText</th>\n",
       "      <th>bestAugmentation</th>\n",
       "      <th>bestAugmentedDataSemanticSimilarity</th>\n",
       "      <th>bestAugmentedDataLexicalSimiliratity</th>\n",
       "      <th>augmented1</th>\n",
       "      <th>semanticSimilarity1</th>\n",
       "      <th>lexicalSimilarity1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"Tiene fácil acceso para las personas con movi...</td>\n",
       "      <td>\"El lugar es fácilmente accesible para individ...</td>\n",
       "      <td>0.791221</td>\n",
       "      <td>0.280000</td>\n",
       "      <td>\"El lugar es fácilmente accesible para individ...</td>\n",
       "      <td>0.791221</td>\n",
       "      <td>0.280000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"Espero que hayan mejorais\"</td>\n",
       "      <td>\"\"Confío en que hayan progresado.\"\"</td>\n",
       "      <td>0.547604</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>\"\"Confío en que hayan progresado.\"\"</td>\n",
       "      <td>0.547604</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"La estación es antigua, aparte de tener una s...</td>\n",
       "      <td>\"La estación es vetusta y, además de tener úni...</td>\n",
       "      <td>0.844201</td>\n",
       "      <td>0.424242</td>\n",
       "      <td>\"La estación es vetusta y, además de tener úni...</td>\n",
       "      <td>0.844201</td>\n",
       "      <td>0.424242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"Bien\"</td>\n",
       "      <td>\"\"Correcto\"\"</td>\n",
       "      <td>0.370179</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>\"\"Correcto\"\"</td>\n",
       "      <td>0.370179</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"Bonito comodo\"</td>\n",
       "      <td>\"\"Agradable y confortable.\"\"</td>\n",
       "      <td>0.202906</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>\"\"Agradable y confortable.\"\"</td>\n",
       "      <td>0.202906</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        originalText  \\\n",
       "0  \"Tiene fácil acceso para las personas con movi...   \n",
       "1                        \"Espero que hayan mejorais\"   \n",
       "2  \"La estación es antigua, aparte de tener una s...   \n",
       "3                                             \"Bien\"   \n",
       "4                                    \"Bonito comodo\"   \n",
       "\n",
       "                                    bestAugmentation  \\\n",
       "0  \"El lugar es fácilmente accesible para individ...   \n",
       "1                \"\"Confío en que hayan progresado.\"\"   \n",
       "2  \"La estación es vetusta y, además de tener úni...   \n",
       "3                                       \"\"Correcto\"\"   \n",
       "4                       \"\"Agradable y confortable.\"\"   \n",
       "\n",
       "   bestAugmentedDataSemanticSimilarity  bestAugmentedDataLexicalSimiliratity  \\\n",
       "0                             0.791221                              0.280000   \n",
       "1                             0.547604                              0.200000   \n",
       "2                             0.844201                              0.424242   \n",
       "3                             0.370179                              0.000000   \n",
       "4                             0.202906                              0.000000   \n",
       "\n",
       "                                          augmented1  semanticSimilarity1  \\\n",
       "0  \"El lugar es fácilmente accesible para individ...             0.791221   \n",
       "1                \"\"Confío en que hayan progresado.\"\"             0.547604   \n",
       "2  \"La estación es vetusta y, además de tener úni...             0.844201   \n",
       "3                                       \"\"Correcto\"\"             0.370179   \n",
       "4                       \"\"Agradable y confortable.\"\"             0.202906   \n",
       "\n",
       "   lexicalSimilarity1  \n",
       "0            0.280000  \n",
       "1            0.200000  \n",
       "2            0.424242  \n",
       "3            0.000000  \n",
       "4            0.000000  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infoValidDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "88efcb72-6c7e-4141-8945-309114dfa9de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>originalText</th>\n",
       "      <th>bestAugmentation</th>\n",
       "      <th>bestAugmentedDataSemanticSimilarity</th>\n",
       "      <th>bestAugmentedDataLexicalSimiliratity</th>\n",
       "      <th>augmented1</th>\n",
       "      <th>semanticSimilarity1</th>\n",
       "      <th>lexicalSimilarity1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"He vivido 35 años en el barrio y reconozco qu...</td>\n",
       "      <td>\"\"Después de haber pasado 35 años en este veci...</td>\n",
       "      <td>0.815581</td>\n",
       "      <td>0.289474</td>\n",
       "      <td>\"\"Después de haber pasado 35 años en este veci...</td>\n",
       "      <td>0.815581</td>\n",
       "      <td>0.289474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"localización con muchos bares interesantes\"</td>\n",
       "      <td>\"\"zona con una gran cantidad de bares atractiv...</td>\n",
       "      <td>0.633096</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>\"\"zona con una gran cantidad de bares atractiv...</td>\n",
       "      <td>0.633096</td>\n",
       "      <td>0.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"…\"</td>\n",
       "      <td>\"Parece que no has proporcionado un texto para...</td>\n",
       "      <td>0.280626</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>\"Parece que no has proporcionado un texto para...</td>\n",
       "      <td>0.280626</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"Muy rica comida..\"</td>\n",
       "      <td>\"\"Deliciosa gastronomía.\"\"</td>\n",
       "      <td>0.309440</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>\"\"Deliciosa gastronomía.\"\"</td>\n",
       "      <td>0.309440</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"Estación del.metro\"</td>\n",
       "      <td>\"\"Terminal del tren subterráneo\"\"</td>\n",
       "      <td>0.342315</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>\"\"Terminal del tren subterráneo\"\"</td>\n",
       "      <td>0.342315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        originalText  \\\n",
       "0  \"He vivido 35 años en el barrio y reconozco qu...   \n",
       "1       \"localización con muchos bares interesantes\"   \n",
       "2                                                \"…\"   \n",
       "3                                \"Muy rica comida..\"   \n",
       "4                               \"Estación del.metro\"   \n",
       "\n",
       "                                    bestAugmentation  \\\n",
       "0  \"\"Después de haber pasado 35 años en este veci...   \n",
       "1  \"\"zona con una gran cantidad de bares atractiv...   \n",
       "2  \"Parece que no has proporcionado un texto para...   \n",
       "3                         \"\"Deliciosa gastronomía.\"\"   \n",
       "4                  \"\"Terminal del tren subterráneo\"\"   \n",
       "\n",
       "   bestAugmentedDataSemanticSimilarity  bestAugmentedDataLexicalSimiliratity  \\\n",
       "0                             0.815581                              0.289474   \n",
       "1                             0.633096                              0.142857   \n",
       "2                             0.280626                              0.000000   \n",
       "3                             0.309440                              0.000000   \n",
       "4                             0.342315                              0.000000   \n",
       "\n",
       "                                          augmented1  semanticSimilarity1  \\\n",
       "0  \"\"Después de haber pasado 35 años en este veci...             0.815581   \n",
       "1  \"\"zona con una gran cantidad de bares atractiv...             0.633096   \n",
       "2  \"Parece que no has proporcionado un texto para...             0.280626   \n",
       "3                         \"\"Deliciosa gastronomía.\"\"             0.309440   \n",
       "4                  \"\"Terminal del tren subterráneo\"\"             0.342315   \n",
       "\n",
       "   lexicalSimilarity1  \n",
       "0            0.289474  \n",
       "1            0.142857  \n",
       "2            0.000000  \n",
       "3            0.000000  \n",
       "4            0.000000  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infoInvalidDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a9e3f6-4629-49ad-928d-e4f1bbd413ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
