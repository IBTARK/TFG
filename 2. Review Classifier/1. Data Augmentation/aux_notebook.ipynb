{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "794d982e-4b80-4fb2-a8c3-ef1c5275232a",
   "metadata": {},
   "source": [
    "# Ampliación de datos auxiliar\n",
    "\n",
    "En este notebook se va a aplicar la técnica de ampliación de datos con las técnicas EDA y NLP albumentation a un conjunto de reseñas de Google Maps separadas en dos ficheros: uno con las reseñas que se van a considerar válidas y el otro con las inválidas. Cada línea es una reseña nueva. Este notebook complementa al otro notebook con otras técicas para la ampliación de datos sobre las reseñas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb03dad-6fb0-4b4e-8fd7-08e258d63202",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "7bd86c94-1406-46b9-b519-be661883cd08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\franp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\franp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import copy\n",
    "import time\n",
    "import unicodedata\n",
    "import re\n",
    "import random\n",
    "from stop_words import get_stop_words\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5876f10-3312-4a0b-b563-deea4a395aeb",
   "metadata": {},
   "source": [
    "### Direcorio de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0739d916-b1c3-4bf9-9803-16ca063e1a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "validReviewsSource = \"C:\\\\Users\\\\franp\\\\OneDrive - Universidad Complutense de Madrid (UCM)\\\\Escritorio\\\\tfg\\\\TFG\\\\1. Data\\\\4. Labeled Reviews\\\\2. Without Emojis\\\\ValidReviews.txt\"\n",
    "invalidReviewsSource = \"C:\\\\Users\\\\franp\\\\OneDrive - Universidad Complutense de Madrid (UCM)\\\\Escritorio\\\\tfg\\\\TFG\\\\1. Data\\\\4. Labeled Reviews\\\\2. Without Emojis\\\\InvalidReviews.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78c8af7-54c4-49a2-bfb8-f89a1dcd0aeb",
   "metadata": {},
   "source": [
    "## Funciones auxiliares\n",
    "Aquí se definen unas funciones auxiliares para la ampliación del conjunto de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb2916fa-f5dc-43eb-9c02-55582fb25b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def importFromTxtToList(source):\n",
    "    with open(source, 'r', encoding=\"utf-8\") as file:\n",
    "        #Generate a list with all the reviews\n",
    "        targetList = [line.strip() for line in file]\n",
    "    return targetList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94fb7760-e6c4-4bcd-aa59-50184bd37e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read the file with the valid reviews\n",
    "validReviewsList = importFromTxtToList(validReviewsSource)\n",
    "#Read the file with the invalid reviews\n",
    "invalidReviewsList = importFromTxtToList(invalidReviewsSource)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7adedde1-ac59-4ade-a52e-ad1c94a6e0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean up the text removing punctuation, accent marks and convertin everything to lowercase\n",
    "def cleanText(text):\n",
    "    text = unicodedata.normalize('NFKD', text.lower()).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "396dbcf2-b0a6-417f-bed0-d298c4e0b251",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean up the text removing spanish stop words\n",
    "def revomeSpanishStopWords(text):\n",
    "    determinantes = {\"el\", \"la\", \"los\", \"las\", \"un\", \"una\", \"unos\", \"unas\", \"este\", \"esta\", \"estos\", \"estas\",\n",
    "                 \"ese\", \"esa\", \"esos\", \"esas\", \"aquel\", \"aquella\", \"aquellos\", \"aquellas\", \"mi\", \"mis\",\n",
    "                 \"tu\", \"tus\", \"su\", \"sus\", \"nuestro\", \"nuestra\", \"nuestros\", \"nuestras\", \"vuestro\", \n",
    "                 \"vuestra\", \"vuestros\", \"vuestras\", \"primer\", \"primero\", \"primera\", \"segundo\", \"segunda\"}\n",
    "\n",
    "    preposiciones = {\"a\", \"ante\", \"bajo\", \"cabe\", \"con\", \"contra\", \"de\", \"desde\", \"durante\", \"en\", \"entre\", \n",
    "                 \"hacia\", \"hasta\", \"mediante\", \"para\", \"por\", \"según\", \"sin\", \"sobre\", \"tras\", \"versus\", \"vía\"}\n",
    "\n",
    "    conjunciones = {\"y\", \"e\", \"ni\", \"o\", \"u\", \"pero\", \"sino\", \"sino que\", \"mas\", \"aunque\", \"que\", \"porque\", \n",
    "                \"como\", \"cuando\", \"donde\", \"mientras\", \"para que\", \"a fin de que\", \"puesto que\", \"ya que\", \n",
    "                \"si\", \"siempre que\"}\n",
    "    pronombres = {\n",
    "        # Pronombres personales\n",
    "        \"yo\", \"tú\", \"vos\", \"él\", \"ella\", \"nosotros\", \"nosotras\", \n",
    "        \"vosotros\", \"vosotras\", \"ellos\", \"ellas\", \"usted\", \"ustedes\",\n",
    "        \"me\", \"te\", \"lo\", \"la\", \"nos\", \"os\", \"los\", \"las\", \"le\", \"les\", \"se\",\n",
    "    \n",
    "        # Pronombres posesivos\n",
    "        \"mío\", \"mía\", \"míos\", \"mías\", \n",
    "        \"tuyo\", \"tuya\", \"tuyos\", \"tuyas\", \n",
    "        \"suyo\", \"suya\", \"suyos\", \"suyas\", \n",
    "        \"nuestro\", \"nuestra\", \"nuestros\", \"nuestras\", \n",
    "        \"vuestro\", \"vuestra\", \"vuestros\", \"vuestras\",\n",
    "    \n",
    "        # Pronombres demostrativos\n",
    "        \"este\", \"esta\", \"estos\", \"estas\", \n",
    "        \"ese\", \"esa\", \"esos\", \"esas\", \n",
    "        \"aquel\", \"aquella\", \"aquellos\", \"aquellas\",\n",
    "    \n",
    "        # Pronombres relativos\n",
    "        \"que\", \"cual\", \"cuales\", \"quien\", \"quienes\", \n",
    "        \"cuyo\", \"cuya\", \"cuyos\", \"cuyas\", \"donde\",\n",
    "    \n",
    "        # Pronombres interrogativos y exclamativos\n",
    "        \"qué\", \"quién\", \"quiénes\", \"cuál\", \"cuáles\", \n",
    "        \"cuánto\", \"cuánta\", \"cuántos\", \"cuántas\", \n",
    "        \"dónde\", \"cómo\", \"cuándo\",\n",
    "    \n",
    "        # Pronombres indefinidos\n",
    "        \"alguien\", \"algo\", \"nadie\", \"nada\", \"cualquiera\", \n",
    "        \"todos\", \"todas\", \"varios\", \"varias\", \"muchos\", \n",
    "        \"muchas\", \"pocos\", \"pocas\", \"alguno\", \"alguna\", \n",
    "        \"algunos\", \"algunas\", \"ninguno\", \"ninguna\", \n",
    "        \"uno\", \"una\", \"unos\", \"unas\", \"demás\"\n",
    "    }\n",
    "\n",
    "    #Combine all the words in one set\n",
    "    spanishStopWords = determinantes | preposiciones | conjunciones | pronombres\n",
    "\n",
    "    textWithoutStopWords = [word for word in text.split() if word.lower() not in spanishStopWords]\n",
    "\n",
    "    return \" \".join(textWithoutStopWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3af74b92-72b3-4b39-8f4e-188aa7c9bf7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add quotes to the text given\n",
    "def add_quotes(text):\n",
    "    return f'\"{text}\"'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "3e42cb9c-3a92-4e35-a349-8d439257c5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove quotes from the text given\n",
    "def remove_quotes(text):\n",
    "    if text.startswith('\"') and text.endswith('\"'):\n",
    "        return text[1:-1]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a199517-936d-48e9-a911-2439ea421b41",
   "metadata": {},
   "source": [
    "## Synonym Replacement\n",
    "Este método consiste en elejir aleatoriamente n palabras de la re que no sean palabras vacías. Reemplaza cada una de estas palabras con uno de sus sinónimos elegido al azar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad7ed901-da49-4d42-a4eb-6dfe507c9101",
   "metadata": {},
   "outputs": [],
   "source": [
    "#list of spanish stop words from the python library\n",
    "stop_words = get_stop_words('spanish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4e003352-9c7b-4c06-b6e8-8614937d8541",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Swaps the word given by its synonym\n",
    "def swap_synonym(word):\n",
    "    #gets all synonyms from the word given\n",
    "    synset = wordnet.synsets(word, lang='spa')\n",
    "    if synset:\n",
    "        #if the word has one of more synonym we swap it\n",
    "        synset = wordnet.synsets(word, lang='spa')[0]\n",
    "        sinonimos = synset.lemma_names('spa') \n",
    "        limpios = [lemma.replace('_', ' ').strip() for lemma in sinonimos]\n",
    "        #filter to make sure its a diferent word\n",
    "        distintos = [s for s in limpios if s.lower() != word.lower()]\n",
    "        #choose a random synonym if the word has one\n",
    "        if distintos:\n",
    "            elegido = random.choice(distintos)\n",
    "            return elegido\n",
    "        else:\n",
    "            return word\n",
    "    else:\n",
    "        return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f317d8df-0c13-45d9-8540-d88212be01c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def swapBySynonymLine(line):\n",
    "    # Split the line into individual words\n",
    "    words = line.split();\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        # Check if the word is not a stop word\n",
    "        if word not in stop_words: \n",
    "            # With 40% probability, replace the word with a synonym\n",
    "            if random.random() <= 0.4:\n",
    "                new_word = swap_synonym(word)\n",
    "            else: \n",
    "                new_word = word\n",
    "            new_words.append(new_word)\n",
    "        else:\n",
    "            new_words.append(word)\n",
    "    # Join the words back into a single line and return it\n",
    "    return ' '.join(new_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6b922e2c-146f-4cf6-b6b4-775b896ffecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#synonym replacement method\n",
    "def synonymReplacement(list):\n",
    "    newList = []\n",
    "    for line in list:\n",
    "        newList.append(swapBySynonymLine(line))\n",
    "    return newList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3cf4266f-a50e-4612-bc0c-b3399531a7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fromListToTxt(file, path):\n",
    "    with open(path, 'w', encoding='utf-8') as archivo:\n",
    "        for line in file:\n",
    "            archivo.write(line + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cf6d4c18-5158-48c4-97e5-0b0d9bc749b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_path = \"C:\\\\Users\\\\franp\\\\OneDrive - Universidad Complutense de Madrid (UCM)\\\\Escritorio\\\\tfg\\\\TFG\\\\2. Review Classifier\\\\1. Data Augmentation\\\\3. Synonym Replacement\\\\validSynonymsReviews.txt\"\n",
    "invalid_path = \"C:\\\\Users\\\\franp\\\\OneDrive - Universidad Complutense de Madrid (UCM)\\\\Escritorio\\\\tfg\\\\TFG\\\\2. Review Classifier\\\\1. Data Augmentation\\\\3. Synonym Replacement\\\\invalidSynonymsReviews.txt\"\n",
    "\n",
    "validSynonymsReviews = synonymReplacement(validReviewsList).copy()\n",
    "invalidSynonymsReviews = synonymReplacement(invalidReviewsList).copy()\n",
    "\n",
    "fromListToTxt(validSynonymsReviews, valid_path)\n",
    "fromListToTxt(invalidSynonymsReviews, invalid_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14bfe4a4-e02b-4a52-ab69-9f956be0c31b",
   "metadata": {},
   "source": [
    "## Random Insertion\n",
    "Este método consiste en encontrar un sinónimo aleatorio de una palabra aleatoria en la oración que no sea una palabra vacía e insertar ese sinónimo en una posición aleatoria de la oración."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0180635b-3a95-48d4-8d55-bb8b113066ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to calculate the number of insertions or replacements or deletions for the methods\n",
    "def calculateInsertions(line):\n",
    "    return max(1, int(len(line.split()) * 0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "00c18670-ef38-45b6-ad42-b160cb86519f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordInsertion(line):\n",
    "    # Clean the line of text and remove extra spaces or special characters\n",
    "    newLine = cleanText(line)\n",
    "    # Split the cleaned line into words\n",
    "    words = newLine.split()   \n",
    "    # Remove Spanish stop words and split the result into words\n",
    "    withoutStopWords = revomeSpanishStopWords(newLine).split() \n",
    "    # Proceed only if there are words left after removing stop words\n",
    "    if withoutStopWords:\n",
    "        # Determine the number of insertions based on line length\n",
    "        for i in range(calculateInsertions(line)):\n",
    "            # Choose a random important word from the list without stop words\n",
    "            elegida = random.choice(withoutStopWords)    \n",
    "            # Get a synonym of the chosen word\n",
    "            sinonimo = swap_synonym(elegida) \n",
    "            # Choose a random position to insert the synonym\n",
    "            pos = random.randint(0, len(words))    \n",
    "            # Insert the synonym at the chosen position, removing any extra spaces\n",
    "            words.insert(pos, sinonimo.strip())      \n",
    "    # Return the modified line with quotation marks around it\n",
    "    return add_quotes(' '.join(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e21d48c2-84ea-4198-9150-55f2328ba685",
   "metadata": {},
   "outputs": [],
   "source": [
    "#random insertion method\n",
    "def randomInsertion(list):\n",
    "    newList = []\n",
    "    for line in list:\n",
    "        newList.append(wordInsertion(line))\n",
    "    return newList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9d24d28d-37cd-41e1-a903-d81319ad97f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_path = \"C:\\\\Users\\\\franp\\\\OneDrive - Universidad Complutense de Madrid (UCM)\\\\Escritorio\\\\tfg\\\\TFG\\\\2. Review Classifier\\\\1. Data Augmentation\\\\4. Random Insertion\\\\validInsertionsReviews.txt\"\n",
    "invalid_path = \"C:\\\\Users\\\\franp\\\\OneDrive - Universidad Complutense de Madrid (UCM)\\\\Escritorio\\\\tfg\\\\TFG\\\\2. Review Classifier\\\\1. Data Augmentation\\\\4. Random Insertion\\\\invalidInsertionsReviews.txt\"\n",
    "\n",
    "validRandomInsertionReviews = randomInsertion(validReviewsList).copy()\n",
    "invalidRandomInsertionReviews = randomInsertion(invalidReviewsList).copy()\n",
    "\n",
    "fromListToTxt(validRandomInsertionReviews, valid_path)\n",
    "fromListToTxt(invalidRandomInsertionReviews, invalid_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace30510-0c36-4361-bce2-9d31ddfeb5c9",
   "metadata": {},
   "source": [
    "## Random Swap\n",
    "Este método consiste en cambiar dos palabras aleatoriamente en la frase n veces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f089258f-4625-4e6e-907b-d937af6a97b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def posNotStopWord(words, pos):\n",
    "    # Generate a random index within the range of the words list\n",
    "    ret = random.randint(0, len(words) - 1)\n",
    "    # Keep generating a new index until it is not equal to the provided position\n",
    "    while ret == pos:\n",
    "        ret = random.randint(0, len(words) - 1)   \n",
    "    # Return the valid random index that is different from the provided position\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0827d321-38e5-4e0f-aaa0-6fd6ef2ab52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordSwap(line):\n",
    "    # Clean the input line and split it into a list of words\n",
    "    words = cleanText(line).split()\n",
    "    # Check if there are more than one word to perform swapping\n",
    "    if len(words) > 1:\n",
    "        # Loop for the number of insertions calculated for the line\n",
    "        for i in range(calculateInsertions(line)):\n",
    "            # Get two diferent random numbers\n",
    "            pos1 = posNotStopWord(words, -1)\n",
    "            pos2 = posNotStopWord(words, pos1)\n",
    "            # Swap the words at the two random positions\n",
    "            aux = words[pos1]\n",
    "            words[pos1] = words[pos2]\n",
    "            words[pos2] = aux\n",
    "    # Return the modified line with quotes added\n",
    "    return add_quotes(' '.join(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "340ccd5c-0887-4f8c-b569-647b7e197831",
   "metadata": {},
   "outputs": [],
   "source": [
    "#random swap method\n",
    "def randomSwap(list):\n",
    "    newList = []\n",
    "    for line in list:\n",
    "        newList.append(wordSwap(line))\n",
    "    return newList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "14b5ee4f-5d84-4154-87b4-d0cc5c1242d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_path = \"C:\\\\Users\\\\franp\\\\OneDrive - Universidad Complutense de Madrid (UCM)\\\\Escritorio\\\\tfg\\\\TFG\\\\2. Review Classifier\\\\1. Data Augmentation\\\\5. Random Swap\\\\validSwapReviews.txt\"\n",
    "invalid_path = \"C:\\\\Users\\\\franp\\\\OneDrive - Universidad Complutense de Madrid (UCM)\\\\Escritorio\\\\tfg\\\\TFG\\\\2. Review Classifier\\\\1. Data Augmentation\\\\5. Random Swap\\\\invalidSwapReviews.txt\"\n",
    "\n",
    "validRandomSwapReviews = randomSwap(validReviewsList).copy()\n",
    "invalidRandomSwapReviews = randomSwap(invalidReviewsList).copy()\n",
    "\n",
    "fromListToTxt(validRandomSwapReviews, valid_path)\n",
    "fromListToTxt(invalidRandomSwapReviews, invalid_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a28e03-ba53-4d8c-b8cb-3e926004f4a8",
   "metadata": {},
   "source": [
    "## Random Deletion\n",
    "Este método consiste en eliminar una palabra aleatoria en la oración que no sea una palabra vacía."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cd9457ea-fc07-4a04-be8f-f5067850038f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordDelete(line):\n",
    "    # Clean the input line and split it into a list of words\n",
    "    words = cleanText(line).split()\n",
    "    # Check if there are more than one word to delete from\n",
    "    if len(words) > 1:\n",
    "        # Calculate and performs the number of deletions based on the line's content\n",
    "        for i in range(calculateInsertions(line)):\n",
    "            # Generate a random index to select a word for deletion\n",
    "            pos = random.randint(0, len(words) - 1)\n",
    "            # Check if the selected word is not a stop word\n",
    "            if words[pos] not in stop_words:\n",
    "                # Remove the word at the selected position from the list\n",
    "                words.pop(pos)\n",
    "    # Join the remaining words into a string, add quotes, and return the result\n",
    "    return add_quotes(' '.join(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cecdb920-714c-44bb-bd63-5011563af7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomDeletion(list):\n",
    "    newList = []\n",
    "    for line in list:\n",
    "        newList.append(wordDelete(line))\n",
    "    return newList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e0972350-c513-4c62-ae75-3ecb3e669f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_path = \"C:\\\\Users\\\\franp\\\\OneDrive - Universidad Complutense de Madrid (UCM)\\\\Escritorio\\\\tfg\\\\TFG\\\\2. Review Classifier\\\\1. Data Augmentation\\\\6. Random Deletion\\\\validDeletionReviews.txt\"\n",
    "invalid_path = \"C:\\\\Users\\\\franp\\\\OneDrive - Universidad Complutense de Madrid (UCM)\\\\Escritorio\\\\tfg\\\\TFG\\\\2. Review Classifier\\\\1. Data Augmentation\\\\6. Random Deletion\\\\invalidDeletionReviews.txt\"\n",
    "\n",
    "validRandomDeletionReviews = randomDeletion(validReviewsList).copy()\n",
    "invalidRandomDeletionReviews = randomDeletion(invalidReviewsList).copy()\n",
    "\n",
    "fromListToTxt(validRandomDeletionReviews, valid_path)\n",
    "fromListToTxt(invalidRandomDeletionReviews, invalid_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97972da4-57f4-4304-9aa0-d75f269bfb1c",
   "metadata": {},
   "source": [
    "## Mixed EDA Methods\n",
    "Para intentar que haya la mayor de variaciones posibles se va a pasar las reseñas por todas las tecnicas de EDA realizadas anteriormente y de esta manera evitar un posible overfiting cuando se entrene al modelo con estos datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "22423099-f212-41f7-8546-2b3ca5283004",
   "metadata": {},
   "outputs": [],
   "source": [
    "#performs all EDA transformations\n",
    "def mixedEDAMethods(list):\n",
    "    newList = synonymReplacement(list).copy()\n",
    "    newList = randomInsertion(newList).copy()\n",
    "    newList = randomSwap(newList).copy()\n",
    "    newList = randomDeletion(newList).copy()\n",
    "    return newList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "8c1d0005-2140-4d5a-8e03-0958e923f463",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_path = \"C:\\\\Users\\\\franp\\\\OneDrive - Universidad Complutense de Madrid (UCM)\\\\Escritorio\\\\tfg\\\\TFG\\\\2. Review Classifier\\\\1. Data Augmentation\\\\7. Mixed EDA Methods\\\\validMixedReviews.txt\"\n",
    "invalid_path = \"C:\\\\Users\\\\franp\\\\OneDrive - Universidad Complutense de Madrid (UCM)\\\\Escritorio\\\\tfg\\\\TFG\\\\2. Review Classifier\\\\1. Data Augmentation\\\\7. Mixed EDA Methods\\\\invalidMixedReviews.txt\"\n",
    "\n",
    "validMixedReviews = mixedEDAMethods(validReviewsList).copy()\n",
    "invalidMixedReviews = mixedEDAMethods(invalidReviewsList).copy()\n",
    "\n",
    "fromListToTxt(validMixedReviews, valid_path)\n",
    "fromListToTxt(invalidMixedReviews, invalid_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1362c648-4140-413f-8752-7ea8ed61bd41",
   "metadata": {},
   "source": [
    "## NLP Albumentation\n",
    "Consiste en cambiar de orden las frases de la reseña en caso de que tengan más de una frase y eliminar las frases que sean iguales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a9a488b9-4e5a-4e82-8d4d-e5352bfebe06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getUniqueSentences(line):\n",
    "    # Remove quotes from the input line to ensure clean processing\n",
    "    newLine = remove_quotes(line)\n",
    "    # Split the cleaned line into sentences using '.' as the delimiter and create a set comprehension to ensure unique sentences\n",
    "    sentences = {sentence.strip() + \".\" for sentence in newLine.split('.') if sentence.strip()}\n",
    "    # Return the set of unique sentences\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "3ec2d963-bc57-430c-811e-34ee53c09441",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mixSentences(sentences):\n",
    "    # Convert the input set of sentences into a list for shuffling\n",
    "    newList = list(sentences)\n",
    "    # Shuffle the list in place to randomize the order of sentences\n",
    "    random.shuffle(newList)\n",
    "    # Join the shuffled sentences into a single string, adding quotes around it\n",
    "    return add_quotes(' '.join(newList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "381fa8e1-72f3-4ab2-aba2-1fa0d11f52e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NLP albumentation method\n",
    "def albumentation(list):\n",
    "    newList = []\n",
    "    for line in list:\n",
    "        newList.append(mixSentences(getUniqueSentences(line)))\n",
    "    return newList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "bb54f170-9302-4f26-8916-4770b62631cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_path = \"C:\\\\Users\\\\franp\\\\OneDrive - Universidad Complutense de Madrid (UCM)\\\\Escritorio\\\\tfg\\\\TFG\\\\2. Review Classifier\\\\1. Data Augmentation\\\\8. NLP Albumentation\\\\validAlbumentationReviews.txt\"\n",
    "invalid_path = \"C:\\\\Users\\\\franp\\\\OneDrive - Universidad Complutense de Madrid (UCM)\\\\Escritorio\\\\tfg\\\\TFG\\\\2. Review Classifier\\\\1. Data Augmentation\\\\8. NLP Albumentation\\\\invalidAlbumentationReviews.txt\"\n",
    "\n",
    "validAlbumentationReviews = albumentation(validReviewsList).copy()\n",
    "invalidAlbumentationReviews = albumentation(invalidReviewsList).copy()\n",
    "\n",
    "fromListToTxt(validAlbumentationReviews, valid_path)\n",
    "fromListToTxt(invalidAlbumentationReviews, invalid_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eeb2164-f326-4a32-a52f-b5d49d563728",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
