{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0ea00ee-ffa1-4e3e-b6b5-d71c0cc1f5f2",
   "metadata": {},
   "source": [
    "# Extracci√≥n de caracter√≠sticas\n",
    "\n",
    "La extracci√≥n de caracter√≠sticas consiste en extraer toda la informaci√≥n posible y relevante de las reviews de cada estaci√≥n. Esto nos permitir√° dar recomendaciones de las rutas m√°s personalizadas al poder aplicar filtros con la informaci√≥n que consigamos obtener a trav√©s de las reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a493b6",
   "metadata": {},
   "source": [
    "## Imports necesarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3727997e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\evano\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\evano\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json: 426kB [00:00, 20.3MB/s]                    \n",
      "2025-04-27 13:55:36 INFO: Downloaded file to C:\\Users\\evano\\stanza_resources\\resources.json\n",
      "2025-04-27 13:55:36 INFO: Downloading default packages for language: es (Spanish) ...\n",
      "2025-04-27 13:55:38 INFO: File exists: C:\\Users\\evano\\stanza_resources\\es\\default.zip\n",
      "2025-04-27 13:55:44 INFO: Finished downloading models and saved to C:\\Users\\evano\\stanza_resources\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize \n",
    "nltk.download('punkt_tab')\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download(\"stopwords\")\n",
    "import spacy\n",
    "from collections import defaultdict, Counter\n",
    "import stanza\n",
    "stanza.download(\"es\")  # Descargar el modelo en espa√±ol\n",
    "from deep_translator import GoogleTranslator\n",
    "from nltk.corpus import wordnet as wn\n",
    "from unidecode import unidecode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff79d89",
   "metadata": {},
   "source": [
    "## Implementaci√≥n de m√©todos para extracci√≥n de caracter√≠sticas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77169f4-a6e6-4481-a8df-4d295c582afb",
   "metadata": {},
   "source": [
    "### N-gramas\n",
    "\n",
    "Para la extracci√≥n de caracter√≠sticas vamos a emplear n-gramas. Un n-grama es una secuencia de n palabras consecutivas en un texto. Esto nos va a permitir poder sacar la infomaci√≥n relevante de las rese√±as al poder aplicar distintos n-gramas y ver sus relaciones.\n",
    "\n",
    "Vamos primero a probar con unas rese√±as de prueba para ver como de bien funcionan los n-gramas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f91ce0a6-04aa-49fc-a94f-d278f2dd4cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = [\n",
    "    \"La estaci√≥n de metro es moderna y limpia, pero los trenes son lentos.\",\n",
    "    \"A veces hay demasiada gente y el servicio es malo.\",\n",
    "    \"El metro de Madrid es r√°pido y eficiente, aunque algunas estaciones est√°n sucias.\",\n",
    "    \"Buena conexi√≥n con otras l√≠neas, pero los horarios nunca son confiables.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da487ba0-12b2-49d8-a480-5b8c438982ae",
   "metadata": {},
   "source": [
    "Para asegurarnos de conseguir la informaci√≥n m√°s relevante vamos a eliminar de nuestras reviews de prueba todas las stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd322dc2-f67d-4612-a117-7db2494b0028",
   "metadata": {},
   "source": [
    "Eliminamos las stopwords en espa√±ol de las reviews de prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b96f12e-0903-47d9-8445-54838d6eaba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['estaci√≥n metro moderna limpia trenes lentos', 'veces demasiada gente servicio malo', 'metro madrid r√°pido eficiente aunque estaciones sucias', 'buena conexi√≥n l√≠neas horarios nunca confiables']\n"
     ]
    }
   ],
   "source": [
    "#Obtenemos las stopwords en espa√±ol\n",
    "spanishStopwords = list(stopwords.words(\"spanish\"))\n",
    "\n",
    "# Eliminar stopwords de cada review\n",
    "filteredReviews = [\n",
    "    \" \".join([word for word in word_tokenize(review.lower()) if word not in spanishStopwords and word.isalnum()])\n",
    "    for review in reviews\n",
    "]\n",
    "\n",
    "# Imprimir resultado\n",
    "print(filteredReviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a93ae17-d694-47ec-bba4-f08ef064ec5b",
   "metadata": {},
   "source": [
    "Una vez eliminadas las stopwords procedemos a usar los n-gramas para poder analizarlas y extraer las caracter√≠sticas. \n",
    "Para que sea lo m√°s fiable posible vamos a usar n-gramas de 2 a 5, es decir, desde bigramas hasta pentagramas, para obtener una precisi√≥n que sea la mejor posible. Si solo tenemos bigramas nos vamos a dejar informaci√≥n relevante pero si usamos n-gramas muy grandes la precisi√≥n tambi√©n se ver√≠a reducida, por lo que se opt√≥ por usar n-gramas de 2 a 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c364eb6f-1f4e-4569-a34a-5ea1e4b8f57a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ngramas encontrados: ['aunque estaciones' 'aunque estaciones sucias' 'buena conexi√≥n'\n",
      " 'buena conexi√≥n l√≠neas' 'buena conexi√≥n l√≠neas horarios'\n",
      " 'buena conexi√≥n l√≠neas horarios nunca' 'conexi√≥n l√≠neas'\n",
      " 'conexi√≥n l√≠neas horarios' 'conexi√≥n l√≠neas horarios nunca'\n",
      " 'conexi√≥n l√≠neas horarios nunca confiables' 'demasiada gente'\n",
      " 'demasiada gente servicio' 'demasiada gente servicio malo'\n",
      " 'eficiente aunque' 'eficiente aunque estaciones'\n",
      " 'eficiente aunque estaciones sucias' 'estaciones sucias' 'estaci√≥n metro'\n",
      " 'estaci√≥n metro moderna' 'estaci√≥n metro moderna limpia'\n",
      " 'estaci√≥n metro moderna limpia trenes' 'gente servicio'\n",
      " 'gente servicio malo' 'horarios nunca' 'horarios nunca confiables'\n",
      " 'limpia trenes' 'limpia trenes lentos' 'l√≠neas horarios'\n",
      " 'l√≠neas horarios nunca' 'l√≠neas horarios nunca confiables'\n",
      " 'madrid r√°pido' 'madrid r√°pido eficiente'\n",
      " 'madrid r√°pido eficiente aunque'\n",
      " 'madrid r√°pido eficiente aunque estaciones' 'metro madrid'\n",
      " 'metro madrid r√°pido' 'metro madrid r√°pido eficiente'\n",
      " 'metro madrid r√°pido eficiente aunque' 'metro moderna'\n",
      " 'metro moderna limpia' 'metro moderna limpia trenes'\n",
      " 'metro moderna limpia trenes lentos' 'moderna limpia'\n",
      " 'moderna limpia trenes' 'moderna limpia trenes lentos' 'nunca confiables'\n",
      " 'r√°pido eficiente' 'r√°pido eficiente aunque'\n",
      " 'r√°pido eficiente aunque estaciones'\n",
      " 'r√°pido eficiente aunque estaciones sucias' 'servicio malo'\n",
      " 'trenes lentos' 'veces demasiada' 'veces demasiada gente'\n",
      " 'veces demasiada gente servicio' 'veces demasiada gente servicio malo']\n",
      "Matriz de ngramas:\n",
      " [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0\n",
      "  0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 1 1]\n",
      " [1 1 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1\n",
      "  1 1 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0]\n",
      " [0 0 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 1 1 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Crear un vectorizador de ngramas\n",
    "vectorizer = CountVectorizer(ngram_range=(2, 5))\n",
    "\n",
    "# Aplicar a las rese√±as\n",
    "X = vectorizer.fit_transform(filteredReviews)\n",
    "\n",
    "# Ver n-gramas encontrados\n",
    "ngramas = vectorizer.get_feature_names_out()\n",
    "print(\"ngramas encontrados:\", ngramas)\n",
    "\n",
    "# Ver la matriz de frecuencia\n",
    "print(\"Matriz de ngramas:\\n\", X.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1552e3d6-5396-43f0-ad63-25e833725e8d",
   "metadata": {},
   "source": [
    "Una vez obtenidos los n-gramas de las reviews filtradas sin los stopwords lo que vamos a hacer es quedarnos de cada n-grama con el sustantivo y su adjetivo adyacente m√°s cercano. Esto es debido a que en espa√±ol los adjetivos calificativos suelen ir al lado o muy cerca del sustantivo al que califica. De esta manera podremos sacar todas las caracter√≠sticas.\n",
    "\n",
    "Nos descargamos e importamos las librer√≠as necesarias para ello. En este caso, hacemos uso de la librer√≠a spaCy de python para poder saber la categor√≠a gramatical de las palabras en espa√±ol:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6942fe5c-a068-496d-9244-25624a2f6dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"es_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "677ab081-1e37-4a6e-80e5-8bc7c8fa8421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'set'>, {'estaci√≥n': {'sucio'}, 'conexi√≥n': {'horario', 'confiable', 'l√≠nea'}, 'gente': {'malo', 'servicio'}, 'servicio': {'malo'}, 'moderna': {'limpio'}, 'horario': {'confiable'}, 'tren': {'lento'}, 'l√≠nea': {'horario', 'confiable'}, 'metro': {'r√°pido', 'moderno', 'eficiente'}})\n"
     ]
    }
   ],
   "source": [
    "# Crear el mapa: {sustantivo: {(adjetivo))}}\n",
    "caracteristicasNGramas = defaultdict(set)\n",
    "\n",
    "for ngrama in ngramas:\n",
    "    doc = nlp(ngrama)\n",
    "    sustantivoActual = None\n",
    "    for token in doc:\n",
    "        if token.pos_ == \"NOUN\":\n",
    "            sustantivoActual = token.lemma_.lower()\n",
    "        elif token.pos_ == \"ADJ\" and sustantivoActual:\n",
    "            caracteristicasNGramas[sustantivoActual].add(token.lemma_.lower())\n",
    "\n",
    "print(caracteristicasNGramas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae4776d-ec82-475a-a99b-5534af9140f3",
   "metadata": {},
   "source": [
    "Se puede apreciar como para las reviews de ejemplo que hemos utilizado hay incongruencias, es decir, se registra que los trenes son lentos pero que el metro es r√°pido. Adem√°s, la librer√≠a interpreta algunas palabras como sustantivos o como adjetivos que no son."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec0534c-7f0e-419c-9736-3fbdbbe65801",
   "metadata": {},
   "source": [
    "### Librer√≠as con redes neuronales\n",
    "\n",
    "Con los n-gramas solo puedes ver las palabras que tengan cerca. Esto en la mayoria de casos es lo que buscas y necesitas pero sin embargo en otras ocasiones un adjetivo al final de la oraci√≥n puede referirse a un sustantivo del principio o un adjetivo cerca de un sustantivo puede referirse a otro sustantivo, y esto con los n-gramas no se puede ver. Para ello, haremos uso de las distinntas librer√≠as de python para realizar un an√°lisis m√°s exahustivo de cada review y poder tener resultados m√°s precisos.\n",
    "\n",
    "El uso de estas librer√≠as va a ser muy similar al de los n-gramas. Vamos a sacar los distintos adjetivos que esten relacionados con los sustantivos correspondientes. De esta manera se ahorra tiempo ya que podemos hacerlo evitando la repetici√≥n de los n-gramas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b50b57-ad46-42a4-a1ad-fd035fd81de0",
   "metadata": {},
   "source": [
    "#### SpaCy\n",
    "\n",
    "La primera libre√≠a que vamos a utilizar es la de spaCy, previamente utilizada para saber que clase de palabra era cada una en las reviews. Sin embargo, esta librer√≠a no solo es capaz de saber la clase gramatical de las palabras sino que adem√°s es capaz de decirte con que palabra esta relacionada, que es justo lo que necesitamos saber para extraer las caracter√≠sticas de las distintas reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7552f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = [\n",
    "    \"La estaci√≥n de metro es moderna y limpia, pero los trenes son lentos.\",\n",
    "    \"A veces hay demasiada gente y el servicio es malo.\",\n",
    "    \"El metro de Madrid es r√°pido y eficiente, aunque algunas estaciones est√°n sucias.\",\n",
    "    \"Buena conexi√≥n con otras l√≠neas, pero los horarios nunca son confiables.\",\n",
    "    \"La estacion es muy fea y est√° muy mal cuidada\",\n",
    "    \"Los horarios son muy utiles\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dd8edcb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"es_core_news_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0e2ad649-2777-40a8-bbe5-5f5431f8662f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_forms(adj):\n",
    "    \"\"\"\n",
    "    Gather all adjective variants:\n",
    "      1) If there are no advmods ‚Üí base lemma (\"limpio\")\n",
    "      2) If there are >=1 advmods ‚Üí join them all + lemma\n",
    "         e.g. [\"muy\",\"mal\"] + \"cuidada\" ‚Üí \"muy mal cuidada\"\n",
    "      3) Recursively include any conjunct ADJ (dep==\"conj\")\n",
    "         only if that conjunct has NO of its own subject\n",
    "    \"\"\"\n",
    "    # 1) get all ADV modifiers in document order\n",
    "    advs = sorted(\n",
    "        [c for c in adj.children if c.dep_ == \"advmod\" and c.pos_ == \"ADV\"],\n",
    "        key=lambda c: c.i\n",
    "    )\n",
    "    adv_lemmas = [c.lemma_.lower() for c in advs]\n",
    "    base = adj.lemma_.lower()\n",
    "\n",
    "    forms = set()\n",
    "    if adv_lemmas:\n",
    "        forms.add(\" \".join(adv_lemmas + [base]))\n",
    "    else:\n",
    "        forms.add(base)\n",
    "\n",
    "    # 2) propagate any simple conjunctions\n",
    "    for child in adj.children:\n",
    "        if child.dep_ == \"conj\" and child.pos_ == \"ADJ\":\n",
    "            # only if that conjunct has no own noun subject\n",
    "            if not any(c.dep_ == \"nsubj\" and c.pos_ == \"NOUN\"\n",
    "                       for c in child.children):\n",
    "                forms |= collect_forms(child)\n",
    "\n",
    "    return forms\n",
    "\n",
    "def find_noun(adj):\n",
    "    \"\"\"\n",
    "    Given an ADJ token, locate its corresponding NOUN (if any) by:\n",
    "      ‚Äì direct modifier: dep_==\"amod\" ‚Üí head NOUN\n",
    "      ‚Äì predicative: child dep_==\"nsubj\" ‚Üí that NOUN\n",
    "      ‚Äì coordination: dep_==\"conj\" ‚Üí recurse to head ADJ\n",
    "      ‚Äì copular chains: dep_ in {\"acomp\",\"attr\"} on a VERB ‚Üí its nsubj NOUN\n",
    "    \"\"\"\n",
    "    # a) direct modifier\n",
    "    if adj.dep_ == \"amod\" and adj.head.pos_ == \"NOUN\":\n",
    "        return adj.head\n",
    "\n",
    "    # b) predicative adjective\n",
    "    for child in adj.children:\n",
    "        if child.dep_ == \"nsubj\" and child.pos_ == \"NOUN\":\n",
    "            return child\n",
    "\n",
    "    # c) coordinated adjective: inherit from head ADJ\n",
    "    if adj.dep_ == \"conj\" and adj.head.pos_ == \"ADJ\":\n",
    "        return find_noun(adj.head)\n",
    "\n",
    "    # d) in case of acomp/attr on a verb\n",
    "    if adj.dep_ in {\"acomp\", \"attr\"} and adj.head.pos_ == \"VERB\":\n",
    "        for child in adj.head.children:\n",
    "            if child.dep_ == \"nsubj\" and child.pos_ == \"NOUN\":\n",
    "                return child\n",
    "\n",
    "    return None\n",
    "\n",
    "def extract_aspects(reviews):\n",
    "    \"\"\"\n",
    "    Returns: defaultdict(set) mapping noun ‚Üí set of adjectival phrases.\n",
    "    \"\"\"\n",
    "    nlp = spacy.load(\"es_core_news_sm\")\n",
    "    aspects = defaultdict(set)\n",
    "\n",
    "    for doc in nlp.pipe(reviews, batch_size=20):\n",
    "        for token in doc:\n",
    "            # only care about ADJ tokens\n",
    "            if token.pos_ != \"ADJ\":\n",
    "                continue\n",
    "            noun = find_noun(token)\n",
    "            if noun:\n",
    "                aspects[noun.lemma_.lower()] |= collect_forms(token)\n",
    "\n",
    "    return aspects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "189eab81",
   "metadata": {},
   "outputs": [],
   "source": [
    "caracteristicasSpacy = extract_aspects(reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ebbea1-ab35-419c-8698-13ba9d5879a3",
   "metadata": {},
   "source": [
    "#### Stanza\n",
    "\n",
    "La otra librer√≠a que vamos a utilizar es stanza que es una librer√≠a de python desarrollada por Standford. Esta librer√≠a es muy similar a la de spaCy, la cual, tambi√©n permite saber la clase gramatical a la cual pertenece la palabra y con que palabra en la oraci√≥n esta relacionada. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0feb8ce7-0d3e-45b7-a6fd-148cf3e62382",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-27 13:02:04 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json: 426kB [00:00, 11.0MB/s]                    \n",
      "2025-04-27 13:02:04 INFO: Downloaded file to C:\\Users\\evano\\stanza_resources\\resources.json\n",
      "2025-04-27 13:02:05 INFO: Loading these models for language: es (Spanish):\n",
      "=================================\n",
      "| Processor | Package           |\n",
      "---------------------------------\n",
      "| tokenize  | combined          |\n",
      "| mwt       | combined          |\n",
      "| pos       | combined_charlm   |\n",
      "| lemma     | combined_nocharlm |\n",
      "| depparse  | combined_charlm   |\n",
      "| ner       | conll02           |\n",
      "=================================\n",
      "\n",
      "2025-04-27 13:02:05 INFO: Using device: cpu\n",
      "2025-04-27 13:02:05 INFO: Loading: tokenize\n",
      "2025-04-27 13:02:10 INFO: Loading: mwt\n",
      "2025-04-27 13:02:10 INFO: Loading: pos\n",
      "2025-04-27 13:02:13 INFO: Loading: lemma\n",
      "2025-04-27 13:02:15 INFO: Loading: depparse\n",
      "2025-04-27 13:02:16 INFO: Loading: ner\n",
      "2025-04-27 13:02:19 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "nlp = stanza.Pipeline(\"es\", processors=\"tokenize,mwt,pos,lemma,ner,depparse\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0ad38d",
   "metadata": {},
   "source": [
    "##### Procesar cada review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "67b0a298",
   "metadata": {},
   "outputs": [],
   "source": [
    "def processReviewsStanza(reviews):\n",
    "\n",
    "    caracteristicasStanza = defaultdict(set)\n",
    "    \n",
    "    for review in reviews:\n",
    "        doc = nlp(review)\n",
    "        for sentence in doc.sentences:\n",
    "            for word in sentence.words:\n",
    "                if word.upos == \"NOUN\":\n",
    "                    sustantivo = word.lemma.lower()\n",
    "                    headWord = sentence.words[word.head - 1]\n",
    "                    if headWord.upos == \"ADJ\":\n",
    "                        adjetivo = headWord.lemma.lower()\n",
    "                        caracteristicasStanza[sustantivo].add(adjetivo)\n",
    "                elif word.upos == \"ADJ\":\n",
    "                    adjetivo = word.lemma.lower()\n",
    "                    headWord = sentence.words[word.head - 1]\n",
    "                    if headWord.upos == \"NOUN\":\n",
    "                        sustantivo = headWord.lemma.lower()\n",
    "                        caracteristicasStanza[sustantivo].add(adjetivo)\n",
    "\n",
    "    return caracteristicasStanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b5bd2a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "caracteristicasStanza = processReviewsStanza(reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ef6069-cfb8-4598-bec6-ccc2821e2a8e",
   "metadata": {},
   "source": [
    "### Comparaci√≥n de resultados\n",
    "\n",
    "Vamos a ver que resultados sacan cada opci√≥n previamente explicadas y compararlas entre s√≠. De esta manera nos quedaremos con el que mejor resultados de para utilizarlo con todas y cada una de las reviews de las distintas estaciones de metro que tenemos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b71bf01c-87a2-4efa-8887-acee6a8f4388",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Las caracteristicas de los n-gramas \n",
      "\n",
      "defaultdict(<class 'set'>, {'estaci√≥n': {'sucio'}, 'conexi√≥n': {'horario', 'confiable', 'l√≠nea'}, 'gente': {'malo', 'servicio'}, 'servicio': {'malo'}, 'moderna': {'limpio'}, 'horario': {'confiable'}, 'tren': {'lento'}, 'l√≠nea': {'horario', 'confiable'}, 'metro': {'r√°pido', 'moderno', 'eficiente'}})\n",
      "\n",
      "\n",
      "Las caracteristicas de spacy \n",
      "\n",
      "defaultdict(<class 'set'>, {'estaci√≥n': {'sucio', 'moderno', 'limpio'}, 'servicio': {'malo'}, 'metro': {'r√°pido', 'eficiente'}, 'conexi√≥n': {'buen'}, 'horario': {'nunca confiable', 'mucho util'}})\n",
      "\n",
      "\n",
      "Las caracteristicas de stanza \n",
      "\n",
      "defaultdict(<class 'set'>, {'estaci√≥n': {'sucia', 'moderno'}, 'tren': {'lento'}, 'servicio': {'malo'}, 'metro': {'r√°pido'}, 'conexi√≥n': {'confiable', 'buena'}, 'horario': {'confiable', 'util'}, 'estacion': {'feo'}})\n"
     ]
    }
   ],
   "source": [
    "print(\"Las caracteristicas de los n-gramas \\n\")\n",
    "print(caracteristicasNGramas)\n",
    "print(\"\\n\")\n",
    "print(\"Las caracteristicas de spacy \\n\")\n",
    "print(caracteristicasSpacy)\n",
    "print(\"\\n\")\n",
    "print(\"Las caracteristicas de stanza \\n\")\n",
    "print(caracteristicasStanza)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a629dd-79bf-4236-8670-2a43d4b5c86f",
   "metadata": {},
   "source": [
    "Podemos observar como con los n-gramas parece que se sacan m√°s caracter√≠sticas que con las librer√≠as. Sin embargo, si lo analizamos m√°s profundamente podemos ver como con los n-gramas se a√±aden palabras que no son sustantivos o que no son adjetivos o que no tienen ninguna relevancia. Esto demuestra que los resultados con las librer√≠as son m√°s precisas. Sin embargo, las librer√≠as alguna caracter√≠stica como que la estaci√≥n esta limpia tambi√©n se la ha saltado. \n",
    "\n",
    "Entre las dos librer√≠as usadas vemos como dan resultados exactamente iguales, por lo que nos quedaremos con spacy debido a que podemos ver los hijos de una palabra y la palabra con la que esta relacionada. Esto cuando haya m√°s reviews y m√°s largas da m√°s seguridad en que se van a sacar el m√°ximo de caracter√≠sticas posibles aunque se pierdan algunas como ya hemos podido ver anteriormente. \n",
    "\n",
    "Por todo esto la librer√≠a que vamos a usar para extraer todas las psoibles caracter√≠sticas va a ser spacy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1544027d-afce-4113-ae86-8c474a9264fc",
   "metadata": {},
   "source": [
    "## Obtenci√≥n de todas las caracter√≠sticas de las reviews\n",
    "\n",
    "Una vez realizada la comparaci√≥n nos quedaremos con la librer√≠a spacy que es la que mejor resultados obtuvo. A continuaci√≥n realizamos la extracci√≥n de las caracteristicas para cada una de las distintas estaciones de metro."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69f3bca-f02c-477f-a53f-e490f7e2446a",
   "metadata": {},
   "source": [
    "Para ello vamos a realizar otra pasada a las reviews y contando su frecuencia para despu√©s ver si son relevantes o no"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c84e2c7-106e-476d-92e1-f943e42aae2b",
   "metadata": {},
   "source": [
    "Funci√≥n auxiliar para contar la frecuencia:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0119d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_forms(adj):\n",
    "    \"\"\"\n",
    "    Gather all adjective variants:\n",
    "      1) If there are no advmods ‚Üí base lemma (\"limpio\")\n",
    "      2) If there are >=1 advmods ‚Üí join them all + lemma\n",
    "         e.g. [\"muy\",\"mal\"] + \"cuidada\" ‚Üí \"muy mal cuidada\"\n",
    "      3) Recursively include any conjunct ADJ (dep==\"conj\")\n",
    "         only if that conjunct has NO of its own subject\n",
    "    \"\"\"\n",
    "    # 1) get all ADV modifiers in document order\n",
    "    advs = sorted(\n",
    "        [c for c in adj.children if c.dep_ == \"advmod\" and c.pos_ == \"ADV\"],\n",
    "        key=lambda c: c.i\n",
    "    )\n",
    "    adv_lemmas = [c.lemma_.lower() for c in advs]\n",
    "    base = adj.lemma_.lower()\n",
    "\n",
    "    forms = set()\n",
    "    if adv_lemmas:\n",
    "        forms.add(\" \".join(adv_lemmas + [base]))\n",
    "    else:\n",
    "        forms.add(base)\n",
    "\n",
    "    # 2) propagate any simple conjunctions\n",
    "    for child in adj.children:\n",
    "        if child.dep_ == \"conj\" and child.pos_ == \"ADJ\":\n",
    "            # only if that conjunct has no own noun subject\n",
    "            if not any(c.dep_ == \"nsubj\" and c.pos_ == \"NOUN\"\n",
    "                       for c in child.children):\n",
    "                forms |= collect_forms(child)\n",
    "\n",
    "    return forms\n",
    "\n",
    "def find_noun(adj):\n",
    "    \"\"\"\n",
    "    Dado un ADJ (o participio), localiza su NOUN.\n",
    "    \"\"\"\n",
    "    # a) modificador directo\n",
    "    if adj.dep_ == \"amod\" and adj.head.pos_ == \"NOUN\":\n",
    "        return adj.head\n",
    "\n",
    "    # b) predicativo: atributo de un verbo/AUX copulativo\n",
    "    if adj.dep_ in {\"acomp\", \"attr\"} and adj.head.pos_ in {\"VERB\", \"AUX\"}:   # ‚Üê CAMBIO 1\n",
    "        for child in adj.head.children:\n",
    "            if child.dep_ == \"nsubj\" and child.pos_ == \"NOUN\":\n",
    "                return child\n",
    "\n",
    "    # c) coordenado\n",
    "    if adj.dep_ == \"conj\" and adj.head.pos_ == \"ADJ\":\n",
    "        return find_noun(adj.head)\n",
    "\n",
    "    # d) participio como acl\n",
    "    if adj.dep_ == \"acl\" and adj.head.pos_ == \"NOUN\":\n",
    "        return adj.head\n",
    "\n",
    "    # e) predicativo con subj propio\n",
    "    for child in adj.children:\n",
    "        if child.dep_ == \"nsubj\" and child.pos_ == \"NOUN\":\n",
    "            return child\n",
    "    \n",
    "    # f) adjetivo que es la ra√≠z (p. ej. ‚ÄúLos trenes son LENTOS‚Äù)\n",
    "    if adj.dep_ == \"ROOT\":\n",
    "        for child in adj.children:\n",
    "            if child.dep_ == \"nsubj\" and child.pos_ == \"NOUN\":\n",
    "                return child\n",
    "    \n",
    "    up = adj\n",
    "    while up.dep_ != \"ROOT\":\n",
    "        up = up.head\n",
    "        if up.pos_ == \"NOUN\":\n",
    "            return up\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "def extract_aspects(reviews):\n",
    "    \"\"\"\n",
    "    Returns: defaultdict(set) mapping noun ‚Üí set of adjectival phrases.\n",
    "    \"\"\"\n",
    "    nlp = spacy.load(\"es_core_news_sm\")\n",
    "    aspects = defaultdict(Counter)\n",
    "\n",
    "    for doc in nlp.pipe(reviews, batch_size=20):\n",
    "        for token in doc:\n",
    "            if not (token.pos_ == \"ADJ\" or \"Part\" in token.morph.get(\"VerbForm\")):\n",
    "                continue\n",
    "            if token.dep_ == \"conj\" and token.head.pos_ == \"ADJ\":\n",
    "                continue\n",
    "            noun = find_noun(token)\n",
    "            if noun:\n",
    "                for form in collect_forms(token):\n",
    "                    noun_key = unidecode(noun.lemma_.lower())\n",
    "                    aspects[noun_key][form] += 1\n",
    "\n",
    "    return aspects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c7f19547",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî∏  NO NOUN: fea ROOT ADJ ‚Üí head: fea ROOT\n"
     ]
    }
   ],
   "source": [
    "results = extract_aspects(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "91b77fcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['La estaci√≥n de metro es moderna y limpia, pero los trenes son lentos.',\n",
       " 'A veces hay demasiada gente y el servicio es malo.',\n",
       " 'El metro de Madrid es r√°pido y eficiente, aunque algunas estaciones est√°n sucias.',\n",
       " 'Buena conexi√≥n con otras l√≠neas, pero los horarios nunca son confiables.',\n",
       " 'La estacion es muy fea y est√° muy mal cuidada',\n",
       " 'Los horarios son muy utiles']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9c8b8c9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(collections.Counter,\n",
       "            {'estacion': Counter({'moderno': 1, 'limpio': 1, 'sucio': 1}),\n",
       "             'servicio': Counter({'malo': 1}),\n",
       "             'metro': Counter({'r√°pido': 1, 'eficiente': 1}),\n",
       "             'conexion': Counter({'buen': 1}),\n",
       "             'horario': Counter({'nunca confiable': 1, 'mucho util': 1})})"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "62826944-5b6f-446c-a2d3-fbecb8bd4587",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agrega un adjetivo con su frecuencia al sustantivo correspondiente en el diccionario.\n",
    "def addAdjective(sustantivo, adjetivo):\n",
    "    for adj, freq in caracteristicas[sustantivo]:\n",
    "        if adj == adjetivo:\n",
    "            caracteristicas[sustantivo].remove((adj, freq))\n",
    "            caracteristicas[sustantivo].add((adj, freq + 1))\n",
    "            return\n",
    "    caracteristicas[sustantivo].add((adjetivo, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a8e6b710-e324-49c6-80df-622c5a4e0341",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'set'>, {'estaci√≥n': {('sucio', 2), ('moderno', 2)}, 'tren': {('lento', 2)}, 'servicio': {('malo', 2)}, 'metro': {('r√°pido', 2)}, 'conexi√≥n': {('confiable', 1), ('buen', 1)}, 'horario': {('confiable', 2)}})\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "caracteristicas = defaultdict(set)\n",
    "\n",
    "# Procesar cada review\n",
    "for review in reviews:\n",
    "    doc = nlp(review)\n",
    "    for token in doc:\n",
    "        if token.pos_ == \"ADJ\":\n",
    "            adjetivo = token.lemma_.lower()\n",
    "            for child in token.children:\n",
    "                if child.pos_ == \"NOUN\":\n",
    "                    sustantivo = child.lemma_.lower()\n",
    "                    addAdjective(sustantivo, adjetivo)\n",
    "            if token.head.pos_ == \"NOUN\":\n",
    "                sustantivo = token.head.lemma_.lower()\n",
    "                addAdjective(sustantivo, adjetivo)\n",
    "        elif token.pos_ == \"NOUN\":\n",
    "            sustantivo = token.lemma_.lower()\n",
    "            if token.head.pos_== \"ADJ\":\n",
    "                adjetivo = token.head.lemma_\n",
    "                addAdjective(sustantivo, adjetivo)\n",
    "\n",
    "print(caracteristicas) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dfcf568-cf09-4a1d-bb45-e49ea600515c",
   "metadata": {},
   "source": [
    "Como puede haber discrepancias entre las caracteristicas que hemos sacado debido a la variabilidad de todas las reviews solo se van a quedar los que no tengan antonimos para un mismo sustantivo o el antonimo que mayor frecuencia tenga. Por ejemplo, si tenemos que el metro esta limpio con una frecuencia de 7 y que el metro esta sucio con una frecuencia de 5, se quedar√≠a que el metro esta limpio.\n",
    "\n",
    "Sin embargo, para poder sacar los antonimos de la mejor manera posible vamos a traducirlos al ingles y luego de vuelta al espa√±ol debido a que en ingles esta mucho mejor optimizado y falla mucho menos que en espa√±ol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0353e3bb-ff80-4987-bdeb-afcb2ad0d3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Traducir palabra al ingles\n",
    "def traducir(palabra, sr, tg):\n",
    "    traduccion = GoogleTranslator(source=sr, target=tg).translate(palabra)\n",
    "    return traduccion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ebee42ff-7199-4ec0-a420-b67377af2149",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener todos los antonimos\n",
    "def getAntonimos(palabra):\n",
    "    antonimos = set()\n",
    "    for sentido in wn.synsets(palabra, pos=wn.ADJ):  # todos los significados de la palabra como adjetivo\n",
    "        for lema in sentido.lemmas(): # cada sin√≥nimo dentro de ese significado\n",
    "            for antonimo in lema.antonyms():  # para cada sin√≥nimo, se busca sus ant√≥nimos\n",
    "                antonimos.add(antonimo.name()) # se a√±ade el nombre del ant√≥nimo al set\n",
    "    return antonimos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d23188ce-77c3-4208-8753-c8cebfaf1408",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quitar los antonimos que tengan menor frecuencia\n",
    "def quitarAntonimos(diccionario):\n",
    "    for sustantivo, adjs in diccionario.items():\n",
    "        lista = list(adjs) # los pasamos a lista para poder iterar\n",
    "        nuevosAdjs = set() # adjetivos que van a quedar despu√©s del filtro\n",
    "        procesados = set() # para no comparar un adjetivo m√°s de una vez\n",
    "\n",
    "        for i, (adj1, freq1) in enumerate(lista):\n",
    "            if adj1 in procesados:\n",
    "                continue # si fue comparado se salta\n",
    "\n",
    "            antonimosIngles = getAntonimos(traducir(adj1,'es','en')) # traducimos las palabras para sacar mejor los antonimos\n",
    "            antonimos = [traducir(ant, 'en', 'es').lower() for ant in antonimosIngles]\n",
    "            emparejado = False\n",
    "\n",
    "            for j in range(i + 1, len(lista)):\n",
    "                adj2, freq2 = lista[j]\n",
    "                if adj2 in procesados: # si fue comparado se salta\n",
    "                    continue\n",
    "                    \n",
    "                if adj2 in antonimos: # se guarda el de mayor frecuencia\n",
    "                    if freq1 > freq2: \n",
    "                        nuevosAdjs.add((adj1, freq1))\n",
    "                    elif freq1 < freq2:\n",
    "                        nuevosAdjs.add((adj2, freq2))\n",
    "                        \n",
    "                    procesados.update([adj1, adj2])\n",
    "                    emparejado = True\n",
    "                    break\n",
    "\n",
    "            if not emparejado: # si no ten√≠a ning√∫n ant√≥nimo en la lista se agrega tal cual\n",
    "                nuevosAdjs.add((adj1, freq1))\n",
    "                procesados.add(adj1)\n",
    "\n",
    "        diccionario[sustantivo] = nuevosAdjs     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d19a1a1f-ecca-45b9-9137-c0caeb233e9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'estaci√≥n': {('sucio', 2), ('moderno', 2)}, 'tren': {('lento', 2)}, 'servicio': {('malo', 2)}, 'conexi√≥n': {('confiable', 1), ('buen', 1)}, 'horario': {('confiable', 2)}, 'metro': {('limpio', 7), ('r√°pido', 4)}}\n"
     ]
    }
   ],
   "source": [
    "aux = {'estaci√≥n': {('sucio', 2), ('moderno', 2)}, 'tren': {('lento', 2)}, 'servicio': {('malo', 2)}, 'conexi√≥n': {('confiable', 1), ('buen', 1)}, 'horario': {('confiable', 2)}, \"metro\": {(\"limpio\", 7), (\"r√°pido\", 4), (\"lento\", 2), (\"sucio\",5)}}\n",
    "\n",
    "quitarAntonimos(aux)\n",
    "print(aux)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46feab79-b84a-4d33-bb1d-d6a9daa94992",
   "metadata": {},
   "source": [
    "Una vez hemos quitado los antonimos con menos frecuencia, es decir, caracteristicas irrelevantes, vamos a a√±adir y guardar las caracter√≠sticas sin la frecuencia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "98bfdbc2-24a2-4da0-b2bf-c1dbefa9bcf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'estaci√≥n': {'sucio', 'moderno'}, 'tren': {'lento'}, 'servicio': {'malo'}, 'metro': {'limpio', 'r√°pido'}, 'conexi√≥n': {'buen', 'confiable'}, 'horario': {'confiable'}}\n"
     ]
    }
   ],
   "source": [
    "caracteristicasFinal = {}\n",
    "\n",
    "def quitarFrecuencias(dicF, dic):\n",
    "    for sustantivo, adjetivos in dicF.items():\n",
    "        dic[sustantivo] = {adj for adj, _ in adjetivos}\n",
    "    return dic\n",
    "\n",
    "caracteristicasFinal = quitarFrecuencias(aux, caracteristicasFinal)\n",
    "print(caracteristicasFinal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606fb135-74b3-4691-b9ea-703e2c7c3377",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfg-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
