{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0ea00ee-ffa1-4e3e-b6b5-d71c0cc1f5f2",
   "metadata": {},
   "source": [
    "# Extracción de características\n",
    "\n",
    "La extracción de características consiste en extraer toda la información posible y relevante de las reviews de cada estación. Esto nos permitirá dar recomendaciones de las rutas más personalizadas al poder aplicar filtros con la información que consigamos obtener a través de las reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a493b6",
   "metadata": {},
   "source": [
    "## Imports y descargas necesarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3727997e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\evano\\Documents\\GitHub\\TFG\\tfg-env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import pickle\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "from collections import defaultdict, Counter\n",
    "import stanza\n",
    "from deep_translator import GoogleTranslator\n",
    "from nltk.corpus import wordnet as wn\n",
    "from unidecode import unidecode\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8683f57b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\evano\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\evano\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\evano\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json: 426kB [00:00, 32.2MB/s]                    \n",
      "2025-05-01 11:05:28 INFO: Downloaded file to C:\\Users\\evano\\stanza_resources\\resources.json\n",
      "2025-05-01 11:05:28 INFO: Downloading default packages for language: es (Spanish) ...\n",
      "2025-05-01 11:05:29 INFO: File exists: C:\\Users\\evano\\stanza_resources\\es\\default.zip\n",
      "2025-05-01 11:05:35 INFO: Finished downloading models and saved to C:\\Users\\evano\\stanza_resources\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt_tab')\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download('wordnet')\n",
    "stanza.download(\"es\")  # Descargar el modelo en español"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff79d89",
   "metadata": {},
   "source": [
    "## Implementación de métodos para extracción de características"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77169f4-a6e6-4481-a8df-4d295c582afb",
   "metadata": {},
   "source": [
    "### N-gramas\n",
    "\n",
    "Para la extracción de características vamos a emplear n-gramas. Un n-grama es una secuencia de n palabras consecutivas en un texto. Esto nos va a permitir poder sacar la infomación relevante de las reseñas al poder aplicar distintos n-gramas y ver sus relaciones.\n",
    "\n",
    "Vamos primero a probar con unas reseñas de prueba para ver como de bien funcionan los n-gramas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f91ce0a6-04aa-49fc-a94f-d278f2dd4cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = [\n",
    "    \"La estación de metro es moderna y limpia, pero los trenes son lentos.\",\n",
    "    \"A veces hay demasiada gente y el servicio es malo.\",\n",
    "    \"El metro de Madrid es rápido y eficiente, aunque algunas estaciones están sucias.\",\n",
    "    \"Buena conexión con otras líneas, pero los horarios nunca son confiables.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da487ba0-12b2-49d8-a480-5b8c438982ae",
   "metadata": {},
   "source": [
    "Para asegurarnos de conseguir la información más relevante vamos a eliminar de nuestras reviews de prueba todas las stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd322dc2-f67d-4612-a117-7db2494b0028",
   "metadata": {},
   "source": [
    "Eliminamos las stopwords en español de las reviews de prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b96f12e-0903-47d9-8445-54838d6eaba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['estación metro moderna limpia trenes lentos', 'veces demasiada gente servicio malo', 'metro madrid rápido eficiente aunque estaciones sucias', 'buena conexión líneas horarios nunca confiables']\n"
     ]
    }
   ],
   "source": [
    "#Obtenemos las stopwords en español\n",
    "spanishStopwords = list(stopwords.words(\"spanish\"))\n",
    "\n",
    "# Eliminar stopwords de cada review\n",
    "filteredReviews = [\n",
    "    \" \".join([word for word in word_tokenize(review.lower()) if word not in spanishStopwords and word.isalnum()])\n",
    "    for review in reviews\n",
    "]\n",
    "\n",
    "# Imprimir resultado\n",
    "print(filteredReviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a93ae17-d694-47ec-bba4-f08ef064ec5b",
   "metadata": {},
   "source": [
    "Una vez eliminadas las stopwords procedemos a usar los n-gramas para poder analizarlas y extraer las características. \n",
    "Para que sea lo más fiable posible vamos a usar n-gramas de 2 a 5, es decir, desde bigramas hasta pentagramas, para obtener una precisión que sea la mejor posible. Si solo tenemos bigramas nos vamos a dejar información relevante pero si usamos n-gramas muy grandes la precisión también se vería reducida, por lo que se optó por usar n-gramas de 2 a 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c364eb6f-1f4e-4569-a34a-5ea1e4b8f57a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ngramas encontrados: ['aunque estaciones' 'aunque estaciones sucias' 'buena conexión'\n",
      " 'buena conexión líneas' 'buena conexión líneas horarios'\n",
      " 'buena conexión líneas horarios nunca' 'conexión líneas'\n",
      " 'conexión líneas horarios' 'conexión líneas horarios nunca'\n",
      " 'conexión líneas horarios nunca confiables' 'demasiada gente'\n",
      " 'demasiada gente servicio' 'demasiada gente servicio malo'\n",
      " 'eficiente aunque' 'eficiente aunque estaciones'\n",
      " 'eficiente aunque estaciones sucias' 'estaciones sucias' 'estación metro'\n",
      " 'estación metro moderna' 'estación metro moderna limpia'\n",
      " 'estación metro moderna limpia trenes' 'gente servicio'\n",
      " 'gente servicio malo' 'horarios nunca' 'horarios nunca confiables'\n",
      " 'limpia trenes' 'limpia trenes lentos' 'líneas horarios'\n",
      " 'líneas horarios nunca' 'líneas horarios nunca confiables'\n",
      " 'madrid rápido' 'madrid rápido eficiente'\n",
      " 'madrid rápido eficiente aunque'\n",
      " 'madrid rápido eficiente aunque estaciones' 'metro madrid'\n",
      " 'metro madrid rápido' 'metro madrid rápido eficiente'\n",
      " 'metro madrid rápido eficiente aunque' 'metro moderna'\n",
      " 'metro moderna limpia' 'metro moderna limpia trenes'\n",
      " 'metro moderna limpia trenes lentos' 'moderna limpia'\n",
      " 'moderna limpia trenes' 'moderna limpia trenes lentos' 'nunca confiables'\n",
      " 'rápido eficiente' 'rápido eficiente aunque'\n",
      " 'rápido eficiente aunque estaciones'\n",
      " 'rápido eficiente aunque estaciones sucias' 'servicio malo'\n",
      " 'trenes lentos' 'veces demasiada' 'veces demasiada gente'\n",
      " 'veces demasiada gente servicio' 'veces demasiada gente servicio malo']\n",
      "Matriz de ngramas:\n",
      " [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0\n",
      "  0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 1 1]\n",
      " [1 1 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1\n",
      "  1 1 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0]\n",
      " [0 0 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 1 1 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Crear un vectorizador de ngramas\n",
    "vectorizer = CountVectorizer(ngram_range=(2, 5))\n",
    "\n",
    "# Aplicar a las reseñas\n",
    "X = vectorizer.fit_transform(filteredReviews)\n",
    "\n",
    "# Ver n-gramas encontrados\n",
    "ngramas = vectorizer.get_feature_names_out()\n",
    "print(\"ngramas encontrados:\", ngramas)\n",
    "\n",
    "# Ver la matriz de frecuencia\n",
    "print(\"Matriz de ngramas:\\n\", X.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1552e3d6-5396-43f0-ad63-25e833725e8d",
   "metadata": {},
   "source": [
    "Una vez obtenidos los n-gramas de las reviews filtradas sin los stopwords lo que vamos a hacer es quedarnos de cada n-grama con el sustantivo y su adjetivo adyacente más cercano. Esto es debido a que en español los adjetivos calificativos suelen ir al lado o muy cerca del sustantivo al que califica. De esta manera podremos sacar todas las características.\n",
    "\n",
    "Nos descargamos e importamos las librerías necesarias para ello. En este caso, hacemos uso de la librería spaCy de python para poder saber la categoría gramatical de las palabras en español:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6942fe5c-a068-496d-9244-25624a2f6dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"es_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "677ab081-1e37-4a6e-80e5-8bc7c8fa8421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'set'>, {'estación': {'sucio'}, 'conexión': {'línea', 'confiable', 'horario'}, 'gente': {'malo', 'servicio'}, 'servicio': {'malo'}, 'moderna': {'limpio'}, 'horario': {'confiable'}, 'tren': {'lento'}, 'línea': {'confiable', 'horario'}, 'metro': {'rápido', 'moderno', 'eficiente'}})\n"
     ]
    }
   ],
   "source": [
    "# Crear el mapa: {sustantivo: {(adjetivo))}}\n",
    "caracteristicasNGramas = defaultdict(set)\n",
    "\n",
    "for ngrama in ngramas:\n",
    "    doc = nlp(ngrama)\n",
    "    sustantivoActual = None\n",
    "    for token in doc:\n",
    "        if token.pos_ == \"NOUN\":\n",
    "            sustantivoActual = token.lemma_.lower()\n",
    "        elif token.pos_ == \"ADJ\" and sustantivoActual:\n",
    "            caracteristicasNGramas[sustantivoActual].add(token.lemma_.lower())\n",
    "\n",
    "print(caracteristicasNGramas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae4776d-ec82-475a-a99b-5534af9140f3",
   "metadata": {},
   "source": [
    "Se puede apreciar como para las reviews de ejemplo que hemos utilizado hay incongruencias, es decir, se registra que los trenes son lentos pero que el metro es rápido. Además, la librería interpreta algunas palabras como sustantivos o como adjetivos que no son."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec0534c-7f0e-419c-9736-3fbdbbe65801",
   "metadata": {},
   "source": [
    "### Librerías con redes neuronales\n",
    "\n",
    "Con los n-gramas solo puedes ver las palabras que tengan cerca. Esto en la mayoria de casos es lo que buscas y necesitas pero sin embargo en otras ocasiones un adjetivo al final de la oración puede referirse a un sustantivo del principio o un adjetivo cerca de un sustantivo puede referirse a otro sustantivo, y esto con los n-gramas no se puede ver. Para ello, haremos uso de las distinntas librerías de python para realizar un análisis más exahustivo de cada review y poder tener resultados más precisos.\n",
    "\n",
    "El uso de estas librerías va a ser muy similar al de los n-gramas. Vamos a sacar los distintos adjetivos que esten relacionados con los sustantivos correspondientes. De esta manera se ahorra tiempo ya que podemos hacerlo evitando la repetición de los n-gramas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b50b57-ad46-42a4-a1ad-fd035fd81de0",
   "metadata": {},
   "source": [
    "#### SpaCy\n",
    "\n",
    "La primera libreía que vamos a utilizar es la de spaCy, previamente utilizada para saber que clase de palabra era cada una en las reviews. Sin embargo, esta librería no solo es capaz de saber la clase gramatical de las palabras sino que además es capaz de decirte con que palabra esta relacionada, que es justo lo que necesitamos saber para extraer las características de las distintas reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e7552f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = [\n",
    "    \"La estación de metro es moderna y limpia, pero los trenes son lentos.\",\n",
    "    \"A veces hay demasiada gente y el servicio es malo.\",\n",
    "    \"El metro de Madrid es rápido y eficiente, aunque algunas estaciones están sucias.\",\n",
    "    \"Buena conexión con otras líneas, pero los horarios nunca son confiables.\",\n",
    "    \"La estacion es muy fea y está muy mal cuidada\",\n",
    "    \"Los horarios son muy utiles\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd8edcb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"es_core_news_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0e2ad649-2777-40a8-bbe5-5f5431f8662f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treat both common nouns (NOUN) and proper nouns (PROPN) as valid targets\n",
    "NOUN_POS = {\"NOUN\", \"PROPN\"}\n",
    "\n",
    "def collectForms(adj):\n",
    "    \"\"\"Return every adjectival *surface form* that belongs to `adj`.\n",
    "\n",
    "    Includes:\n",
    "    * the base lemma itself\n",
    "    * any preceding adverb modifiers (advmod) in document order\n",
    "    * any coordinated adjectives (dep == \"conj\") that have NO own subject\n",
    "    \"\"\"\n",
    "    # Adverbial intensifiers/modifiers to the adjective\n",
    "    advs = sorted(\n",
    "        [c for c in adj.children if c.dep_ == \"advmod\" and c.pos_ == \"ADV\"],\n",
    "        key=lambda c: c.i\n",
    "    )\n",
    "    adv_lemmas = [c.lemma_.lower() for c in advs]\n",
    "    base = adj.lemma_.lower()\n",
    "\n",
    "    forms = set()\n",
    "    forms.add(\" \".join(adv_lemmas + [base]) if adv_lemmas else base)\n",
    "\n",
    "    # Recursively walk into adjective conjuncts\n",
    "    for child in adj.children:\n",
    "        if child.dep_ == \"conj\" and child.pos_ == \"ADJ\":\n",
    "            # skip conjuncts that have their own subject (they describe\n",
    "            # a different entity)\n",
    "            if not any(c.dep_ == \"nsubj\" and c.pos_ in NOUN_POS\n",
    "                       for c in child.children):\n",
    "                forms |= collectForms(child)\n",
    "    return forms\n",
    "\n",
    "def findNoun(adj):\n",
    "    \"\"\"Given an ADJ (or participle), locate the noun it describes.\n",
    "\n",
    "    Handles:\n",
    "    a) 'amod' modifiers\n",
    "    b) copular/adjectival complements ('acomp' / 'attr')\n",
    "    c) coordinated adjectives\n",
    "    d) participial relative clauses ('acl')\n",
    "    e) adjectives that carry their own nsubj\n",
    "    f) adjectives that are the sentence ROOT (\"Los trenes son LENTOS\")\n",
    "    g) fallback: climb ancestors until a NOUN/PROPN is found\n",
    "    \"\"\"\n",
    "    # a) direct pre-nominal modifier\n",
    "    if adj.dep_ == \"amod\" and adj.head.pos_ in NOUN_POS:\n",
    "        return adj.head\n",
    "\n",
    "    # b) copular predicate after SER/ESTAR\n",
    "    if adj.dep_ in {\"acomp\", \"attr\"} and adj.head.pos_ in {\"VERB\", \"AUX\"}:\n",
    "        for child in adj.head.children:\n",
    "            if child.dep_ == \"nsubj\" and child.pos_ in NOUN_POS:\n",
    "                return child\n",
    "\n",
    "    # c) coordinated adjective – inherit target from the head adjective\n",
    "    if adj.dep_ == \"conj\" and adj.head.pos_ == \"ADJ\":\n",
    "        return findNoun(adj.head)\n",
    "\n",
    "    # d) participle used as adjectival clause\n",
    "    if adj.dep_ == \"acl\" and adj.head.pos_ in NOUN_POS:\n",
    "        return adj.head\n",
    "\n",
    "    # e) the adjective has its own nominal subject\n",
    "    for child in adj.children:\n",
    "        if child.dep_ == \"nsubj\" and child.pos_ in NOUN_POS:\n",
    "            return child\n",
    "\n",
    "    # f) adjective is ROOT; subject may sit under the AUX\n",
    "    if adj.dep_ == \"ROOT\":\n",
    "        # direct subject\n",
    "        for child in adj.children:\n",
    "            if child.dep_ == \"nsubj\" and child.pos_ in NOUN_POS:\n",
    "                return child\n",
    "        \n",
    "            if child.pos_ in {\"AUX\", \"VERB\"}:\n",
    "                for gc in child.children:\n",
    "                    if gc.dep_ == \"nsubj\" and gc.pos_ in NOUN_POS:\n",
    "                        return gc\n",
    "\n",
    "    # g) climb ancestors as last resort\n",
    "    up = adj\n",
    "    while up.dep_ != \"ROOT\":\n",
    "        up = up.head\n",
    "        if up.pos_ in NOUN_POS:\n",
    "            return up\n",
    "\n",
    "    return None\n",
    "\n",
    "def extractAspects(reviews):\n",
    "    \"\"\"Main entry: return defaultdict(Counter).\"\"\"\n",
    "    nlp = spacy.load(\"es_core_news_sm\")\n",
    "    aspects = defaultdict(set)\n",
    "\n",
    "    fallbackLlema = \"estacion\" # If no noun is found this would be the default\n",
    "\n",
    "    for doc in nlp.pipe(reviews, batch_size=20):\n",
    "        for token in doc:\n",
    "            # accept adjectives and adjectival participles\n",
    "            if not (token.pos_ == \"ADJ\" or \"Part\" in token.morph.get(\"VerbForm\") or (token.dep_ == \"ROOT\" and token.pos_ in {\"INTJ\", \"ADV\", \"PROPN\"})):\n",
    "                continue\n",
    "            # skip conjunct duplicates (will be handled via their head)\n",
    "            if token.dep_ == \"conj\" and token.head.pos_ == \"ADJ\" and token.pos_ == \"ADJ\":\n",
    "                continue\n",
    "\n",
    "            noun = findNoun(token)\n",
    "\n",
    "            if noun is None:\n",
    "                if token.dep_ == \"ROOT\":\n",
    "                        noun_key = fallbackLlema\n",
    "                else:\n",
    "                        continue\n",
    "            else:\n",
    "                noun_key = unidecode(noun.lemma_.lower()) \n",
    "                \n",
    "            for form in collectForms(token):\n",
    "                if (form == noun_key or unidecode(form) == noun_key):\n",
    "                   continue \n",
    "                \n",
    "                aspects[noun_key].add(form)   \n",
    "\n",
    "    return aspects\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "189eab81",
   "metadata": {},
   "outputs": [],
   "source": [
    "caracteristicasSpacy = extractAspects(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c89fa6e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(set,\n",
       "            {'estacion': {'limpio',\n",
       "              'mal cuidado',\n",
       "              'moderno',\n",
       "              'mucho feo',\n",
       "              'sucio'},\n",
       "             'servicio': {'malo'},\n",
       "             'metro': {'eficiente', 'rápido'},\n",
       "             'conexion': {'buen'},\n",
       "             'horario': {'mucho util', 'nunca confiable'}})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "caracteristicasSpacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ebbea1-ab35-419c-8698-13ba9d5879a3",
   "metadata": {},
   "source": [
    "#### Stanza\n",
    "\n",
    "La otra librería que vamos a utilizar es stanza que es una librería de python desarrollada por Standford. Esta librería es muy similar a la de spaCy, la cual, también permite saber la clase gramatical a la cual pertenece la palabra y con que palabra en la oración esta relacionada. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0feb8ce7-0d3e-45b7-a6fd-148cf3e62382",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-29 16:15:30 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json: 426kB [00:00, 20.6MB/s]                    \n",
      "2025-04-29 16:15:30 INFO: Downloaded file to C:\\Users\\evano\\stanza_resources\\resources.json\n",
      "2025-04-29 16:15:31 INFO: Loading these models for language: es (Spanish):\n",
      "=================================\n",
      "| Processor | Package           |\n",
      "---------------------------------\n",
      "| tokenize  | combined          |\n",
      "| mwt       | combined          |\n",
      "| pos       | combined_charlm   |\n",
      "| lemma     | combined_nocharlm |\n",
      "| depparse  | combined_charlm   |\n",
      "| ner       | conll02           |\n",
      "=================================\n",
      "\n",
      "2025-04-29 16:15:31 INFO: Using device: cpu\n",
      "2025-04-29 16:15:31 INFO: Loading: tokenize\n",
      "2025-04-29 16:15:35 INFO: Loading: mwt\n",
      "2025-04-29 16:15:35 INFO: Loading: pos\n",
      "2025-04-29 16:15:38 INFO: Loading: lemma\n",
      "2025-04-29 16:15:40 INFO: Loading: depparse\n",
      "2025-04-29 16:15:41 INFO: Loading: ner\n",
      "2025-04-29 16:15:44 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "nlp = stanza.Pipeline(\"es\", processors=\"tokenize,mwt,pos,lemma,ner,depparse\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0ad38d",
   "metadata": {},
   "source": [
    "##### Procesar cada review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "67b0a298",
   "metadata": {},
   "outputs": [],
   "source": [
    "def processReviewsStanza(reviews):\n",
    "\n",
    "    caracteristicasStanza = defaultdict(set)\n",
    "    \n",
    "    for review in reviews:\n",
    "        doc = nlp(review)\n",
    "        for sentence in doc.sentences:\n",
    "            for word in sentence.words:\n",
    "                if word.upos == \"NOUN\":\n",
    "                    sustantivo = word.lemma.lower()\n",
    "                    headWord = sentence.words[word.head - 1]\n",
    "                    if headWord.upos == \"ADJ\":\n",
    "                        adjetivo = headWord.lemma.lower()\n",
    "                        caracteristicasStanza[sustantivo].add(adjetivo)\n",
    "                elif word.upos == \"ADJ\":\n",
    "                    adjetivo = word.lemma.lower()\n",
    "                    headWord = sentence.words[word.head - 1]\n",
    "                    if headWord.upos == \"NOUN\":\n",
    "                        sustantivo = headWord.lemma.lower()\n",
    "                        caracteristicasStanza[sustantivo].add(adjetivo)\n",
    "\n",
    "    return caracteristicasStanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b5bd2a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "caracteristicasStanza = processReviewsStanza(reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ef6069-cfb8-4598-bec6-ccc2821e2a8e",
   "metadata": {},
   "source": [
    "### Comparación de resultados\n",
    "\n",
    "Vamos a ver que resultados sacan cada opción previamente explicadas y compararlas entre sí. De esta manera nos quedaremos con el que mejor resultados de para utilizarlo con todas y cada una de las reviews de las distintas estaciones de metro que tenemos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b71bf01c-87a2-4efa-8887-acee6a8f4388",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Las caracteristicas de los n-gramas \n",
      "\n",
      "defaultdict(<class 'set'>, {'estación': {'sucio'}, 'conexión': {'línea', 'confiable', 'horario'}, 'gente': {'malo', 'servicio'}, 'servicio': {'malo'}, 'moderna': {'limpio'}, 'horario': {'confiable'}, 'tren': {'lento'}, 'línea': {'confiable', 'horario'}, 'metro': {'rápido', 'moderno', 'eficiente'}})\n",
      "\n",
      "\n",
      "Las caracteristicas de spacy \n",
      "\n",
      "defaultdict(<class 'set'>, {'estacion': {'limpio', 'mucho feo', 'mal cuidado', 'moderno', 'sucio'}, 'servicio': {'malo'}, 'metro': {'rápido', 'eficiente'}, 'conexion': {'buen'}, 'horario': {'mucho util', 'nunca confiable'}})\n",
      "\n",
      "\n",
      "Las caracteristicas de stanza \n",
      "\n",
      "defaultdict(<class 'set'>, {'estación': {'sucia', 'moderno'}, 'tren': {'lento'}, 'servicio': {'malo'}, 'metro': {'rápido'}, 'conexión': {'buena', 'confiable'}, 'horario': {'util', 'confiable'}, 'estacion': {'feo'}})\n"
     ]
    }
   ],
   "source": [
    "print(\"Las caracteristicas de los n-gramas \\n\")\n",
    "print(caracteristicasNGramas)\n",
    "print(\"\\n\")\n",
    "print(\"Las caracteristicas de spacy \\n\")\n",
    "print(caracteristicasSpacy)\n",
    "print(\"\\n\")\n",
    "print(\"Las caracteristicas de stanza \\n\")\n",
    "print(caracteristicasStanza)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a629dd-79bf-4236-8670-2a43d4b5c86f",
   "metadata": {},
   "source": [
    "Podemos observar como con los n-gramas parece que se sacan más características que con las librerías. Sin embargo, si lo analizamos más profundamente podemos ver como con los n-gramas se añaden palabras que no son sustantivos o que no son adjetivos o que no tienen ninguna relevancia. Esto demuestra que los resultados con las librerías son más precisas. Sin embargo, las librerías alguna característica como que la estación esta limpia también se la ha saltado. \n",
    "\n",
    "Entre las dos librerías usadas vemos como dan resultados exactamente iguales, por lo que nos quedaremos con spacy debido a que podemos ver los hijos de una palabra y la palabra con la que esta relacionada. Esto cuando haya más reviews y más largas da más seguridad en que se van a sacar el máximo de características posibles aunque se pierdan algunas como ya hemos podido ver anteriormente. \n",
    "\n",
    "Por todo esto la librería que vamos a usar para extraer todas las psoibles características va a ser spacy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1544027d-afce-4113-ae86-8c474a9264fc",
   "metadata": {},
   "source": [
    "## Obtención de todas las características de las reviews\n",
    "\n",
    "Una vez realizada la comparación nos quedaremos con la librería spacy que es la que mejor resultados obtuvo. A continuación realizamos la extracción de las caracteristicas para cada una de las distintas estaciones de metro."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69f3bca-f02c-477f-a53f-e490f7e2446a",
   "metadata": {},
   "source": [
    "Para ello vamos a realizar otra pasada a las reviews y contando su frecuencia para después ver si son relevantes o no"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c84e2c7-106e-476d-92e1-f943e42aae2b",
   "metadata": {},
   "source": [
    "Función auxiliar para contar la frecuencia:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cb0119d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "NOUN_POS = {\"NOUN\", \"PROPN\"}\n",
    "\n",
    "def collectForms(adj):\n",
    "    \"\"\"Return every adjectival *surface form* that belongs to `adj`.\n",
    "\n",
    "    Includes:\n",
    "    * the base lemma itself\n",
    "    * any preceding adverb modifiers (advmod) in document order\n",
    "    * any coordinated adjectives (dep == \"conj\") that have NO own subject\n",
    "    \"\"\"\n",
    "    # Adverbial intensifiers/modifiers to the adjective\n",
    "    advs = sorted(\n",
    "        [c for c in adj.children if c.dep_ == \"advmod\" and c.pos_ == \"ADV\"],\n",
    "        key=lambda c: c.i\n",
    "    )\n",
    "    adv_lemmas = [c.lemma_.lower() for c in advs]\n",
    "    base = adj.lemma_.lower()\n",
    "\n",
    "    forms = set()\n",
    "    forms.add(\" \".join(adv_lemmas + [base]) if adv_lemmas else base)\n",
    "\n",
    "    # Recursively walk into adjective conjuncts\n",
    "    for child in adj.children:\n",
    "        if child.dep_ == \"conj\" and child.pos_ == \"ADJ\":\n",
    "            # skip conjuncts that have their own subject (they describe\n",
    "            # a different entity)\n",
    "            if not any(c.dep_ == \"nsubj\" and c.pos_ in NOUN_POS\n",
    "                       for c in child.children):\n",
    "                forms |= collectForms(child)\n",
    "    return forms\n",
    "\n",
    "def findNoun(adj):\n",
    "    \"\"\"Given an ADJ (or participle), locate the noun it describes.\n",
    "\n",
    "    Handles:\n",
    "    a) 'amod' modifiers\n",
    "    b) copular/adjectival complements ('acomp' / 'attr')\n",
    "    c) coordinated adjectives\n",
    "    d) participial relative clauses ('acl')\n",
    "    e) adjectives that carry their own nsubj\n",
    "    f) adjectives that are the sentence ROOT (\"Los trenes son LENTOS\")\n",
    "    g) fallback: climb ancestors until a NOUN/PROPN is found\n",
    "    \"\"\"\n",
    "    # a) direct pre-nominal modifier\n",
    "    if adj.dep_ == \"amod\" and adj.head.pos_ in NOUN_POS:\n",
    "        return adj.head\n",
    "\n",
    "    # b) copular predicate after SER/ESTAR\n",
    "    if adj.dep_ in {\"acomp\", \"attr\"} and adj.head.pos_ in {\"VERB\", \"AUX\"}:\n",
    "        for child in adj.head.children:\n",
    "            if child.dep_ == \"nsubj\" and child.pos_ in NOUN_POS:\n",
    "                return child\n",
    "\n",
    "    # c) coordinated adjective – inherit target from the head adjective\n",
    "    if adj.dep_ == \"conj\" and adj.head.pos_ == \"ADJ\":\n",
    "        return findNoun(adj.head)\n",
    "\n",
    "    # d) participle used as adjectival clause\n",
    "    if adj.dep_ == \"acl\" and adj.head.pos_ in NOUN_POS:\n",
    "        return adj.head\n",
    "\n",
    "    # e) the adjective has its own nominal subject\n",
    "    for child in adj.children:\n",
    "        if child.dep_ == \"nsubj\" and child.pos_ in NOUN_POS:\n",
    "            return child\n",
    "\n",
    "    # f) adjective is ROOT; subject may sit under the AUX\n",
    "    if adj.dep_ == \"ROOT\":\n",
    "        # direct subject\n",
    "        for child in adj.children:\n",
    "            if child.dep_ == \"nsubj\" and child.pos_ in NOUN_POS:\n",
    "                return child\n",
    "        \n",
    "            if child.pos_ in {\"AUX\", \"VERB\"}:\n",
    "                for gc in child.children:\n",
    "                    if gc.dep_ == \"nsubj\" and gc.pos_ in NOUN_POS:\n",
    "                        return gc\n",
    "\n",
    "    # g) climb ancestors as last resort\n",
    "    up = adj\n",
    "    while up.dep_ != \"ROOT\":\n",
    "        up = up.head\n",
    "        if up.pos_ in NOUN_POS:\n",
    "            return up\n",
    "\n",
    "    return None\n",
    "\n",
    "def extractAspects(reviews):\n",
    "    \"\"\"Main entry: return defaultdict(Counter).\"\"\"\n",
    "    nlp = spacy.load(\"es_core_news_sm\")\n",
    "    aspects = defaultdict(Counter)\n",
    "\n",
    "    fallbackLlema = \"estacion\" # If no noun is found this would be the default\n",
    "\n",
    "    for doc in nlp.pipe(reviews, batch_size=20):\n",
    "        for token in doc:\n",
    "            # accept adjectives and adjectival participles\n",
    "            if not (token.pos_ == \"ADJ\" or \"Part\" in token.morph.get(\"VerbForm\") or (token.dep_ == \"ROOT\" and token.pos_ in {\"INTJ\", \"ADV\", \"PROPN\"})):\n",
    "                continue\n",
    "            # skip conjunct duplicates (will be handled via their head)\n",
    "            if token.dep_ == \"conj\" and token.head.pos_ == \"ADJ\" and token.pos_ == \"ADJ\":\n",
    "                continue\n",
    "\n",
    "            noun = findNoun(token)\n",
    "\n",
    "            if noun is None:\n",
    "                if token.dep_ == \"ROOT\":\n",
    "                        noun_key = fallbackLlema\n",
    "                else:\n",
    "                        continue\n",
    "            else:\n",
    "                noun_key = unidecode(noun.lemma_.lower()) \n",
    "                \n",
    "            for form in collectForms(token):\n",
    "                if (form == noun_key or unidecode(form) == noun_key):\n",
    "                   continue \n",
    "                \n",
    "                aspects[noun_key][form] += 1\n",
    "\n",
    "    return aspects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d4f446ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = [\n",
    "    \"La estación de metro es moderna y limpia, pero los trenes son lentos.\",\n",
    "    \"A veces hay demasiada gente y el servicio es malo.\",\n",
    "    \"El metro de Madrid es rápido y eficiente, aunque algunas estaciones están sucias.\",\n",
    "    \"Buena conexión con otras líneas, pero los horarios nunca son confiables.\",\n",
    "    \"Excelente\",\n",
    "    \"casa bonita. Horrible\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c7f19547",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = extractAspects(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9c8b8c9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(collections.Counter,\n",
       "            {'estacion': Counter({'moderno': 1,\n",
       "                      'limpio': 1,\n",
       "                      'sucio': 1,\n",
       "                      'excelente': 1,\n",
       "                      'horrible': 1}),\n",
       "             'servicio': Counter({'malo': 1}),\n",
       "             'metro': Counter({'rápido': 1, 'eficiente': 1}),\n",
       "             'conexion': Counter({'buen': 1}),\n",
       "             'horario': Counter({'nunca confiable': 1}),\n",
       "             'casa': Counter({'bonito': 1})})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5dee3e7",
   "metadata": {},
   "source": [
    "Función para cargar todas las reviews de un csv y devolverlas en una lista"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "479ac697",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadReviews(path):\n",
    "    with open(path, encoding=\"utf-8\", newline=\"\") as f:\n",
    "        reader = csv.reader(f, quotechar='\"', skipinitialspace=True)\n",
    "        rows = [row[0] for row in reader if row]\n",
    "\n",
    "    if rows and rows[0].strip().lower() == \"review\":\n",
    "        rows = rows[1:]\n",
    "\n",
    "    return rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9c3b2d",
   "metadata": {},
   "source": [
    "Función para guardar el objeto devulelto por extractAspects (pensado para guardar el objeto de python al completo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f7e65e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveCharacteristics(path, data):\n",
    "    with open(path, \"wb\") as file:\n",
    "        pickle.dump(data, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f336aee9",
   "metadata": {},
   "source": [
    "Función para guardar la información devuelta por extractAspects en un .txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad67c2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveCharacteristicsAsTxt(path, dataCounter):\n",
    "    with open(path, \"w\", encoding = \"utf-8\") as file:\n",
    "        for key in dataCounter:\n",
    "            file.write(str(key) + \": \")\n",
    "\n",
    "            for subkey in dataCounter[key]:\n",
    "                file.write(\"(\" + str(subkey) + \", \" + str(dataCounter[key][subkey]) + \")\" )\n",
    "            \n",
    "            file.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "972289dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputFolderPath = \"../1. Data/5. Classified Reviews\"\n",
    "outputFolderPath = \"../1. Data/6. Reviews Characteristics/1. Raw\"\n",
    "\n",
    "for station in os.listdir(inputFolderPath):\n",
    "\n",
    "    if not station.lower().endswith(\".csv\"):\n",
    "        continue\n",
    "\n",
    "    # Import the data\n",
    "    inputPath = os.path.join(inputFolderPath, station)\n",
    "    reviews = loadReviews(inputPath)\n",
    "\n",
    "    # Process the data\n",
    "    results = extractAspects(reviews)\n",
    "\n",
    "    # Save the data\n",
    "    if station.endswith('.csv'):\n",
    "        station = station[:-4]\n",
    "\n",
    "    outputPathPkl = os.path.join(outputFolderPath, station + \".pkl\")\n",
    "    outputPathTxt = os.path.join(outputFolderPath, station + \".txt\")\n",
    "\n",
    "    saveCharacteristics(outputPathPkl, results)\n",
    "    saveCharacteristicsAsTxt(outputPathTxt, results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a4ff96",
   "metadata": {},
   "source": [
    "## Análisis de la extracción"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb41c2e",
   "metadata": {},
   "source": [
    "Función para cargar las reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "80948c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadReviews(path):\n",
    "    with open(path, encoding=\"utf-8\", newline=\"\") as f:\n",
    "        reader = csv.reader(f, quotechar='\"', skipinitialspace=True)\n",
    "        rows = [row[0] for row in reader if row]\n",
    "\n",
    "    if rows and rows[0].strip().lower() == \"review\":\n",
    "        rows = rows[1:]\n",
    "\n",
    "    return rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031c02c5",
   "metadata": {},
   "source": [
    "Función para cargar las características desde los ficheros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "027ecf3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadCharacteristics(path):\n",
    "    with open(path, \"rb\") as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20c6450",
   "metadata": {},
   "source": [
    "Se van a escoger tres estaciones al azar y se va a analizar la calidad de la extracción de características"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "de6fcc65",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputFolderPath = \"../1. Data/6. Reviews Characteristics/1. Raw\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d243d3",
   "metadata": {},
   "source": [
    "### Abrantes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff98c4b0",
   "metadata": {},
   "source": [
    "#### Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "074b4c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputPath = \"../1. Data/5. Classified Reviews/Abrantes.csv\"\n",
    "reviews = loadReviews(inputPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e7ce4f09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Tiene fácil acceso para las personas con movilidad reducida, una salida hacia el colegio Julián Besteiro y el otro en frente del Burgen King.',\n",
       " 'Estación de metro que pertenece a la línea 11. No tiene cobertura móvil como en toda la linea. Dispone de ascensor. Tiene 2 accesos a la calle. La estación da un poco de mal rollo porque no es muy transitada.',\n",
       " 'Es una estación muy tranquila, ya que sólo circula por ella la línea de metro 11 de La Fortuna a Plaza Elíptica.',\n",
       " 'Los trabajadores de metro de esta estación son muy amargados... Te dicen las cosas de muy mala gana...',\n",
       " 'Asqueroso',\n",
       " 'Esta muy bien ubicado',\n",
       " 'Estacion con mal olor',\n",
       " 'Espero que hayan mejorais',\n",
       " 'Excelente',\n",
       " 'Excelente']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f0782f",
   "metadata": {},
   "source": [
    "#### Extracción de características"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "16a7a619",
   "metadata": {},
   "outputs": [],
   "source": [
    "characteristics = loadCharacteristics(os.path.join(inputFolderPath, \"Abrantes.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8110cf99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(collections.Counter,\n",
       "            {'acceso': Counter({'fácil': 1}),\n",
       "             'movilidad': Counter({'reducido': 1}),\n",
       "             'cobertura': Counter({'móvil': 1}),\n",
       "             'rollo': Counter({'mal': 1}),\n",
       "             'estacion': Counter({'excelente': 2,\n",
       "                      'mucho tranquilo': 1,\n",
       "                      'asqueroso': 1}),\n",
       "             'trabajador': Counter({'mucho amargado': 1}),\n",
       "             'olor': Counter({'mal': 1})})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "characteristics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c808e4c5",
   "metadata": {},
   "source": [
    "Como se puede ver, la extracción de características es bastante certera"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c982d8f",
   "metadata": {},
   "source": [
    "## Detección de sinónimos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a82c1b9-2be0-496a-ab62-f53f95441d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar modelo multilingüe\n",
    "model1 = SentenceTransformer(\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "model2 = SentenceTransformer(\"sentence-transformers/LaBSE\")\n",
    "\n",
    "def areSynonyms(palabra1, palabra2, umbral=0.65):\n",
    "    emb1 = model1.encode([palabra1])\n",
    "    emb2 = model1.encode([palabra2])\n",
    "    emb11 = model2.encode([palabra1])\n",
    "    emb22 = model2.encode([palabra2])\n",
    "    sim1 = cosine_similarity(emb1, emb2)[0][0]\n",
    "    sim2 = cosine_similarity(emb11, emb22)[0][0]\n",
    "    sim = ((sim1 + sim2) / 2)\n",
    "    return  sim >= umbral"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8283a6de",
   "metadata": {},
   "source": [
    "## Detección de antónimos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dfcf568-cf09-4a1d-bb45-e49ea600515c",
   "metadata": {},
   "source": [
    "Como puede haber discrepancias entre las caracteristicas que hemos sacado debido a la variabilidad de todas las reviews solo se van a quedar los que no tengan antonimos para un mismo sustantivo o el antonimo que mayor frecuencia tenga. Por ejemplo, si tenemos que el metro esta limpio con una frecuencia de 7 y que el metro esta sucio con una frecuencia de 5, se quedaría que el metro esta limpio.\n",
    "\n",
    "Sin embargo, para poder sacar los antonimos de la mejor manera posible vamos a traducirlos al ingles y luego de vuelta al español debido a que en ingles esta mucho mejor optimizado y falla mucho menos que en español."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0353e3bb-ff80-4987-bdeb-afcb2ad0d3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Traducir palabra al ingles\n",
    "def traducir(palabra, sr, tg):\n",
    "    traduccion = GoogleTranslator(source=sr, target=tg).translate(palabra)\n",
    "    return traduccion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ebee42ff-7199-4ec0-a420-b67377af2149",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener todos los antonimos\n",
    "def getAntonimos(palabra):\n",
    "    antonimos = set()\n",
    "    for sentido in wn.synsets(palabra, pos=wn.ADJ):  # todos los significados de la palabra como adjetivo\n",
    "        for lema in sentido.lemmas(): # cada sinónimo dentro de ese significado\n",
    "            for antonimo in lema.antonyms():  # para cada sinónimo, se busca sus antónimos\n",
    "                antonimos.add(antonimo.name()) # se añade el nombre del antónimo al set\n",
    "    return antonimos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4743478",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mucho', 'grande', 'alto', 'grande']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ant = getAntonimos(traducir(\"pequeño\", \"es\", \"en\"))\n",
    "\n",
    "newAnt = []\n",
    "for elem in ant:\n",
    "    newAnt.append(traducir(elem, \"en\", \"es\"))\n",
    "newAnt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ed726f",
   "metadata": {},
   "source": [
    "## Funciones para procesar un conjunto de características de una estación:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9532d1a8",
   "metadata": {},
   "source": [
    "### 1. Eliminación de sustantivos que sean sinónimos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d63023c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combineKeys(d):\n",
    "    \n",
    "    \"\"\"\n",
    "    Input:\n",
    "    d : A Counter where each key is a noun,\n",
    "              and each value is another Counter mapping adjetives and each frequency.\n",
    "              \n",
    "    Group the Counters keys that are synonyms\n",
    "    \n",
    "    Output:\n",
    "    final: A new Counter with synonymous keys merged into a single entry,\n",
    "          summing their Counter values.\n",
    "    \"\"\"\n",
    "    \n",
    "    final = {} # New dictionary to hold merged entries\n",
    "    processed = set() # keys that have already been processed\n",
    "    keys = list(d.keys()) # List of original keys\n",
    "\n",
    "    for i, key in enumerate(keys):\n",
    "        if key in processed:\n",
    "            continue # Skip if this key has already been processed as a synonym\n",
    "\n",
    "        total = Counter(d[key]) # Counter of the current key\n",
    "\n",
    "        for j in range(i + 1, len(keys)):\n",
    "            k2 = keys[j] # New key to see if its a synonym\n",
    "            if k2 in processed:\n",
    "                continue # Skip if this key has already been processed as a synonym\n",
    "            if areSynonyms(key, k2): # If the two keys are synonyms\n",
    "                total += d[k2] # Add frequencies from k2 into total\n",
    "                processed.add(k2) # Mark k2 as processed\n",
    "\n",
    "        final[key] = total # Save the merged result under the original key\n",
    "        processed.add(key) # Mark the current key as processed\n",
    "\n",
    "    return Counter(final) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043f06ed",
   "metadata": {},
   "source": [
    "### 2. Combinación de sinónimos en las características"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ada31b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combineSynonyms(d):\n",
    "\n",
    "    \"\"\"\n",
    "    Input:\n",
    "    d : A Counter where each key is a noun,\n",
    "              and each value is another Counter mapping adjetives and each frequency.\n",
    "\n",
    "    Groups synonymous adjectives (features) for each noun key in the dictionary.\n",
    "    For each group of synonyms, it keeps the one with the most antonyms as the representative.\n",
    "\n",
    "    Output:\n",
    "    final: A new Counter with the same noun keys,\n",
    "          but with synonymous adjectives grouped and frequencies combined.\n",
    "    \"\"\"\n",
    "\n",
    "    final = {} # New dictionary to hold the merged synonymous adjectives and frequencies\n",
    "\n",
    "    for noun in d.keys():\n",
    "        processed = set() # Adjetives that have already been processed\n",
    "        features = list(d[noun].keys()) # List of all adjectives for this noun\n",
    "        newFeatures = {} # New grouped features for this noun\n",
    "\n",
    "        for idx, feature in enumerate(features):\n",
    "\n",
    "            rep = feature # Representative feature\n",
    "            numAntRep = len(getAntonimos(feature)) # Number of antonyms for current representative\n",
    "\n",
    "            if feature in processed:\n",
    "                continue # Skip if this adjective was already merged\n",
    "\n",
    "            total = d[noun][feature] # Start with the frequency of the current feature\n",
    "\n",
    "            for j in range(idx + 1, len(features)):\n",
    "\n",
    "                feature2 = features[j] # New feature to see if its a synonym\n",
    "\n",
    "                if feature2 in processed:\n",
    "                    continue # Skip if already processed\n",
    "\n",
    "                if areSynonyms(feature, feature2): # If the two features are synonyms\n",
    "                    total += d[noun][feature2] # Add the frequency\n",
    "                    processed.add(feature2) # Mark as processed\n",
    "                    \n",
    "                    numAntFeature2 = len(getAntonimos(feature2)) # Get number of new feature antonyms\n",
    "                    if numAntFeature2 > numAntRep: # Compare number of antonyms\n",
    "                        numAntRep = numAntFeature2\n",
    "                        rep = feature2  # Use the feature with more antonyms as representative\n",
    "\n",
    "            \n",
    "            newFeatures.update({rep: total}) # Add the merged features \n",
    "            processed.add(feature) # Mark the current feature as processed\n",
    "        \n",
    "        final[noun] = Counter(newFeatures) # Save the merged features to its noun\n",
    "\n",
    "    return Counter(final)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049c82c2",
   "metadata": {},
   "source": [
    "### 3. Filtrado de las características que sean antónimos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e4258c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "globalAntonyms = {}\n",
    "\n",
    "def removeAntonyms(d):\n",
    "\n",
    "    \"\"\"\n",
    "    Input:\n",
    "    d : A Counter where each key is a noun,\n",
    "              and each value is another Counter mapping adjetives and each frequency.\n",
    "\n",
    "    For each noun, adjective pairs that are antonyms are removed.    \n",
    "    If two adjectives are antonyms, only the one with the highest frequency is kept.\n",
    "\n",
    "    Output:\n",
    "    final: A new Counter with the same noun keys,\n",
    "          but with synonymous adjectives grouped and frequencies combined.\n",
    "    \"\"\"\n",
    "    final = {} # Counter to store filtered adjectives per noun\n",
    "\n",
    "    for noun, adjs in d.items():\n",
    "        lst = list(adjs) # Convert the adjectives to a list for indexed access\n",
    "        newAdjs = {} # Store final adjectives after removing antonyms\n",
    "        processed = set() # Adjetives that have already been processed\n",
    "\n",
    "        for i, adj in enumerate(lst):\n",
    "            if adj in processed:\n",
    "                continue # Skip already processed adjectives\n",
    "\n",
    "            if adj not in globalAntonyms:\n",
    "                antonimosIngles = getAntonimos(traducir(adj,'es','en')) # Translate the adjective to English and get its antonyms\n",
    "                antonimos = [traducir(ant, 'en', 'es').lower() for ant in antonimosIngles] # Translate each antonym back to Spanish\n",
    "                globalAntonyms[adj] = antonimos\n",
    "\n",
    "            else:\n",
    "                antonimos = globalAntonyms[adj]\n",
    "\n",
    "            emparejado = False # Flag to indicate if an antonym match was found\n",
    "\n",
    "            for j in range(i + 1, len(lst)):\n",
    "                adj2 = lst[j]\n",
    "                if adj2 in processed: \n",
    "                    continue # Skip already processed adjectives\n",
    "\n",
    "                if adj2 in antonimos: # If adj2 is an antonym of adj\n",
    "                    if d[noun][adj] >= d[noun][adj2]: # Keep the adjective with the higher frequency\n",
    "                        newAdjs.update({adj: d[noun][adj]})\n",
    "                    else:\n",
    "                        newAdjs.update({adj2: d[noun][adj2]})\n",
    "\n",
    "                    processed.update([adj, adj2]) # Mark both as processed \n",
    "                    emparejado = True # Antonym match was found\n",
    "                    break # Break to avoid duplicate grouping\n",
    "\n",
    "            if not emparejado: # If no antonym was found for adj, keep it as it was\n",
    "                newAdjs.update({adj: d[noun][adj]})\n",
    "                processed.add(adj) # Mark adj as processed \n",
    "\n",
    "        final[noun] = Counter(newAdjs) # Assign the final filtered adjective Counter to the noun\n",
    "    \n",
    "    return Counter(final)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58f1a91",
   "metadata": {},
   "source": [
    "## Procesamiento de las características de todas las estaciones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e410876",
   "metadata": {},
   "source": [
    "Función para cargar las características de una estación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2bc4c237",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadCharacteristics(path):\n",
    "    with open(path, \"rb\") as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed5f7c8",
   "metadata": {},
   "source": [
    "Función para guardar las características de una estación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2d91727c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveCharacteristics(path, data):\n",
    "    with open(path, \"wb\") as file:\n",
    "        pickle.dump(data, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c47e8a",
   "metadata": {},
   "source": [
    "Función para guardar las características de una estación en un .txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "21a42098",
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveCharacteristicsAsTxt(path, dataCounter):\n",
    "    with open(path, \"w\", encoding = \"utf-8\") as file:\n",
    "        for key in dataCounter:\n",
    "            file.write(str(key) + \": \")\n",
    "\n",
    "            for subkey in dataCounter[key]:\n",
    "                file.write(\"(\" + str(subkey) + \", \" + str(dataCounter[key][subkey]) + \")\" )\n",
    "            \n",
    "            file.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0cee6c69",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 25\u001b[39m\n\u001b[32m     22\u001b[39m finalCharacteristics = combineSynonyms(finalCharacteristics)\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# Remove antonyms\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m finalCharacteristics = \u001b[43mremoveAntonyms\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfinalCharacteristics\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# Add it to the dictionary with all the information\u001b[39;00m\n\u001b[32m     28\u001b[39m data[station[:-\u001b[32m4\u001b[39m]] = finalCharacteristics\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 29\u001b[39m, in \u001b[36mremoveAntonyms\u001b[39m\u001b[34m(d)\u001b[39m\n\u001b[32m     26\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m \u001b[38;5;66;03m# Skip already processed adjectives\u001b[39;00m\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m adj \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m globalAntonyms:\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m     antonimosIngles = \u001b[43mgetAntonimos\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraducir\u001b[49m\u001b[43m(\u001b[49m\u001b[43madj\u001b[49m\u001b[43m,\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mes\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43men\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# Translate the adjective to English and get its antonyms\u001b[39;00m\n\u001b[32m     30\u001b[39m     antonimos = [traducir(ant, \u001b[33m'\u001b[39m\u001b[33men\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mes\u001b[39m\u001b[33m'\u001b[39m).lower() \u001b[38;5;28;01mfor\u001b[39;00m ant \u001b[38;5;129;01min\u001b[39;00m antonimosIngles] \u001b[38;5;66;03m# Translate each antonym back to Spanish\u001b[39;00m\n\u001b[32m     31\u001b[39m     globalAntonyms[adj] = antonimos\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 4\u001b[39m, in \u001b[36mgetAntonimos\u001b[39m\u001b[34m(palabra)\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgetAntonimos\u001b[39m(palabra):\n\u001b[32m      3\u001b[39m     antonimos = \u001b[38;5;28mset\u001b[39m()\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m sentido \u001b[38;5;129;01min\u001b[39;00m \u001b[43mwn\u001b[49m\u001b[43m.\u001b[49m\u001b[43msynsets\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpalabra\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mADJ\u001b[49m\u001b[43m)\u001b[49m:  \u001b[38;5;66;03m# todos los significados de la palabra como adjetivo\u001b[39;00m\n\u001b[32m      5\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m lema \u001b[38;5;129;01min\u001b[39;00m sentido.lemmas(): \u001b[38;5;66;03m# cada sinónimo dentro de ese significado\u001b[39;00m\n\u001b[32m      6\u001b[39m             \u001b[38;5;28;01mfor\u001b[39;00m antonimo \u001b[38;5;129;01min\u001b[39;00m lema.antonyms():  \u001b[38;5;66;03m# para cada sinónimo, se busca sus antónimos\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\evano\\Documents\\GitHub\\TFG\\tfg-env\\Lib\\site-packages\\nltk\\corpus\\reader\\wordnet.py:1765\u001b[39m, in \u001b[36mWordNetCorpusReader.synsets\u001b[39m\u001b[34m(self, lemma, pos, lang, check_exceptions)\u001b[39m\n\u001b[32m   1758\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msynsets\u001b[39m(\u001b[38;5;28mself\u001b[39m, lemma, pos=\u001b[38;5;28;01mNone\u001b[39;00m, lang=\u001b[33m\"\u001b[39m\u001b[33meng\u001b[39m\u001b[33m\"\u001b[39m, check_exceptions=\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m   1759\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Load all synsets with a given lemma and part of speech tag.\u001b[39;00m\n\u001b[32m   1760\u001b[39m \u001b[33;03m    If no pos is specified, all synsets for all parts of speech\u001b[39;00m\n\u001b[32m   1761\u001b[39m \u001b[33;03m    will be loaded.\u001b[39;00m\n\u001b[32m   1762\u001b[39m \u001b[33;03m    If lang is specified, all the synsets associated with the lemma name\u001b[39;00m\n\u001b[32m   1763\u001b[39m \u001b[33;03m    of that language will be returned.\u001b[39;00m\n\u001b[32m   1764\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1765\u001b[39m     lemma = \u001b[43mlemma\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlower\u001b[49m()\n\u001b[32m   1767\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m lang == \u001b[33m\"\u001b[39m\u001b[33meng\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   1768\u001b[39m         get_synset = \u001b[38;5;28mself\u001b[39m.synset_from_pos_and_offset\n",
      "\u001b[31mAttributeError\u001b[39m: 'NoneType' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "inputFolderPath = \"../1. Data/6. Reviews Characteristics/1. Raw\"\n",
    "outputFolderPath = \"../1. Data/6. Reviews Characteristics/2. Filtered\"\n",
    "\n",
    "# Dictionary with the characteristics of all the stations\n",
    "data = {}\n",
    "\n",
    "adjectives = {}\n",
    "\n",
    "for station in os.listdir(inputFolderPath):\n",
    "\n",
    "    if not station.lower().endswith(\".pkl\"):\n",
    "        continue\n",
    "\n",
    "    # Import the data\n",
    "    inputPath = os.path.join(inputFolderPath, station)\n",
    "    characteristics = loadCharacteristics(inputPath)\n",
    "\n",
    "    # Combine synonyms from the keys (nouns)\n",
    "    finalCharacteristics = combineKeys(characteristics)\n",
    "\n",
    "    # Combine synonyms from the adjectives\n",
    "    finalCharacteristics = combineSynonyms(finalCharacteristics)\n",
    "\n",
    "    # Remove antonyms\n",
    "    finalCharacteristics = removeAntonyms(finalCharacteristics)\n",
    "\n",
    "    # Add it to the dictionary with all the information\n",
    "    data[station[:-4]] = finalCharacteristics\n",
    "\n",
    "    # Save the processed characteristics\n",
    "    outputPath = os.path.join(outputFolderPath, station[:-4])\n",
    "\n",
    "    saveCharacteristics(outputPath + \".pkl\", finalCharacteristics)\n",
    "    saveCharacteristicsAsTxt(outputPath + \".txt\", finalCharacteristics)\n",
    "\n",
    "\n",
    "    for noun in finalCharacteristics.keys():\n",
    "\n",
    "        for adj in finalCharacteristics[noun].keys():\n",
    "\n",
    "            if adj in adjectives:\n",
    "                adjectives[adj] += finalCharacteristics[noun][adj]\n",
    "            else:\n",
    "                adjectives[adj] = finalCharacteristics[noun][adj]\n",
    "\n",
    "\n",
    "outputPath = os.path.join(outputFolderPath, \"totalAdjectives\")\n",
    "saveCharacteristics(outputPath + \".pkl\", finalCharacteristics)\n",
    "\n",
    "# Save the info as a txt\n",
    "with open(outputPath + \".txt\", \"w\", encoding = \"utf-8\") as file:\n",
    "    for ajd in adjectives.keys():\n",
    "        file.write(adj + \": \" +  str(adjectives[adj]) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba28137",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f81b01a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e36048",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52153e22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "46feab79-b84a-4d33-bb1d-d6a9daa94992",
   "metadata": {},
   "source": [
    "Una vez hemos quitado los antonimos con menos frecuencia, es decir, caracteristicas irrelevantes, vamos a añadir y guardar las características sin la frecuencia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98bfdbc2-24a2-4da0-b2bf-c1dbefa9bcf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'estación': {'sucio', 'moderno'}, 'tren': {'lento'}, 'servicio': {'malo'}, 'metro': {'limpio', 'rápido'}, 'conexión': {'buen', 'confiable'}, 'horario': {'confiable'}}\n"
     ]
    }
   ],
   "source": [
    "caracteristicasFinal = {}\n",
    "\n",
    "def quitarFrecuencias(dicF, dic):\n",
    "    for sustantivo, adjetivos in dicF.items():\n",
    "        dic[sustantivo] = {adj for adj, _ in adjetivos}\n",
    "    return dic\n",
    "\n",
    "caracteristicasFinal = quitarFrecuencias(aux, caracteristicasFinal)\n",
    "print(caracteristicasFinal)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfg-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
