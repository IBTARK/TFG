{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0ea00ee-ffa1-4e3e-b6b5-d71c0cc1f5f2",
   "metadata": {},
   "source": [
    "# Extracción de características\n",
    "\n",
    "La extracción de características consiste en extraer toda la información posible y relevante de las reviews de cada estación. Esto nos permitirá dar recomendaciones de las rutas más personalizadas al poder aplicar filtros con la información que consigamos obtener a través de las reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a493b6",
   "metadata": {},
   "source": [
    "## Imports necesarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3727997e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\evano\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\evano\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "c:\\Users\\evano\\Documents\\GitHub\\TFG\\tfg-env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json: 426kB [00:00, 18.3MB/s]                    \n",
      "2025-04-26 18:06:42 INFO: Downloaded file to C:\\Users\\evano\\stanza_resources\\resources.json\n",
      "2025-04-26 18:06:42 INFO: Downloading default packages for language: es (Spanish) ...\n",
      "2025-04-26 18:06:43 INFO: File exists: C:\\Users\\evano\\stanza_resources\\es\\default.zip\n",
      "2025-04-26 18:06:48 INFO: Finished downloading models and saved to C:\\Users\\evano\\stanza_resources\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize \n",
    "nltk.download('punkt_tab')\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download(\"stopwords\")\n",
    "import spacy\n",
    "from collections import defaultdict\n",
    "import stanza\n",
    "stanza.download(\"es\")  # Descargar el modelo en español\n",
    "from deep_translator import GoogleTranslator\n",
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff79d89",
   "metadata": {},
   "source": [
    "## Implementación de métodos para extracción de características"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77169f4-a6e6-4481-a8df-4d295c582afb",
   "metadata": {},
   "source": [
    "### N-gramas\n",
    "\n",
    "Para la extracción de características vamos a emplear n-gramas. Un n-grama es una secuencia de n palabras consecutivas en un texto. Esto nos va a permitir poder sacar la infomación relevante de las reseñas al poder aplicar distintos n-gramas y ver sus relaciones.\n",
    "\n",
    "Vamos primero a probar con unas reseñas de prueba para ver como de bien funcionan los n-gramas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f91ce0a6-04aa-49fc-a94f-d278f2dd4cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = [\n",
    "    \"La estación de metro es moderna y limpia, pero los trenes son lentos.\",\n",
    "    \"A veces hay demasiada gente y el servicio es malo.\",\n",
    "    \"El metro de Madrid es rápido y eficiente, aunque algunas estaciones están sucias.\",\n",
    "    \"Buena conexión con otras líneas, pero los horarios nunca son confiables.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da487ba0-12b2-49d8-a480-5b8c438982ae",
   "metadata": {},
   "source": [
    "Para asegurarnos de conseguir la información más relevante vamos a eliminar de nuestras reviews de prueba todas las stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd322dc2-f67d-4612-a117-7db2494b0028",
   "metadata": {},
   "source": [
    "Eliminamos las stopwords en español de las reviews de prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b96f12e-0903-47d9-8445-54838d6eaba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['estación metro moderna limpia trenes lentos', 'veces demasiada gente servicio malo', 'metro madrid rápido eficiente aunque estaciones sucias', 'buena conexión líneas horarios confiables']\n"
     ]
    }
   ],
   "source": [
    "#Obtenemos las stopwords en español\n",
    "spanishStopwords = list(stopwords.words(\"spanish\"))\n",
    "\n",
    "# Eliminar stopwords de cada review\n",
    "filteredReviews = [\n",
    "    \" \".join([word for word in word_tokenize(review.lower()) if word not in spanishStopwords and word.isalnum()])\n",
    "    for review in reviews\n",
    "]\n",
    "\n",
    "# Imprimir resultado\n",
    "print(filteredReviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a93ae17-d694-47ec-bba4-f08ef064ec5b",
   "metadata": {},
   "source": [
    "Una vez eliminadas las stopwords procedemos a usar los n-gramas para poder analizarlas y extraer las características. \n",
    "Para que sea lo más fiable posible vamos a usar n-gramas de 2 a 5, es decir, desde bigramas hasta pentagramas, para obtener una precisión que sea la mejor posible. Si solo tenemos bigramas nos vamos a dejar información relevante pero si usamos n-gramas muy grandes la precisión también se vería reducida, por lo que se optó por usar n-gramas de 2 a 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c364eb6f-1f4e-4569-a34a-5ea1e4b8f57a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ngramas encontrados: ['aunque estaciones' 'aunque estaciones sucias' 'buena conexión'\n",
      " 'buena conexión líneas' 'buena conexión líneas horarios'\n",
      " 'buena conexión líneas horarios confiables' 'conexión líneas'\n",
      " 'conexión líneas horarios' 'conexión líneas horarios confiables'\n",
      " 'demasiada gente' 'demasiada gente servicio'\n",
      " 'demasiada gente servicio malo' 'eficiente aunque'\n",
      " 'eficiente aunque estaciones' 'eficiente aunque estaciones sucias'\n",
      " 'estaciones sucias' 'estación metro' 'estación metro moderna'\n",
      " 'estación metro moderna limpia' 'estación metro moderna limpia trenes'\n",
      " 'gente servicio' 'gente servicio malo' 'horarios confiables'\n",
      " 'limpia trenes' 'limpia trenes lentos' 'líneas horarios'\n",
      " 'líneas horarios confiables' 'madrid rápido' 'madrid rápido eficiente'\n",
      " 'madrid rápido eficiente aunque'\n",
      " 'madrid rápido eficiente aunque estaciones' 'metro madrid'\n",
      " 'metro madrid rápido' 'metro madrid rápido eficiente'\n",
      " 'metro madrid rápido eficiente aunque' 'metro moderna'\n",
      " 'metro moderna limpia' 'metro moderna limpia trenes'\n",
      " 'metro moderna limpia trenes lentos' 'moderna limpia'\n",
      " 'moderna limpia trenes' 'moderna limpia trenes lentos' 'rápido eficiente'\n",
      " 'rápido eficiente aunque' 'rápido eficiente aunque estaciones'\n",
      " 'rápido eficiente aunque estaciones sucias' 'servicio malo'\n",
      " 'trenes lentos' 'veces demasiada' 'veces demasiada gente'\n",
      " 'veces demasiada gente servicio' 'veces demasiada gente servicio malo']\n",
      "Matriz de ngramas:\n",
      " [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 1\n",
      "  1 1 1 1 1 1 0 0 0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 1 0 1 1 1 1]\n",
      " [1 1 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 0\n",
      "  0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0]\n",
      " [0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 1 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Crear un vectorizador de ngramas\n",
    "vectorizer = CountVectorizer(ngram_range=(2, 5))\n",
    "\n",
    "# Aplicar a las reseñas\n",
    "X = vectorizer.fit_transform(filteredReviews)\n",
    "\n",
    "# Ver n-gramas encontrados\n",
    "ngramas = vectorizer.get_feature_names_out()\n",
    "print(\"ngramas encontrados:\", ngramas)\n",
    "\n",
    "# Ver la matriz de frecuencia\n",
    "print(\"Matriz de ngramas:\\n\", X.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1552e3d6-5396-43f0-ad63-25e833725e8d",
   "metadata": {},
   "source": [
    "Una vez obtenidos los n-gramas de las reviews filtradas sin los stopwords lo que vamos a hacer es quedarnos de cada n-grama con el sustantivo y su adjetivo adyacente más cercano. Esto es debido a que en español los adjetivos calificativos suelen ir al lado o muy cerca del sustantivo al que califica. De esta manera podremos sacar todas las características.\n",
    "\n",
    "Nos descargamos e importamos las librerías necesarias para ello. En este caso, hacemos uso de la librería spaCy de python para poder saber la categoría gramatical de las palabras en español:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6942fe5c-a068-496d-9244-25624a2f6dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"es_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "677ab081-1e37-4a6e-80e5-8bc7c8fa8421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'set'>, {'estación': {'sucio'}, 'conexión': {'horario', 'líneas'}, 'horario': {'confiable'}, 'gente': {'servicio'}, 'servicio': {'malo'}, 'tren': {'lento'}, 'línea': {'horario'}, 'madrid': {'eficiente', 'rápido'}, 'metro': {'eficiente', 'limpio', 'rápido', 'moderno'}, 'moderna': {'limpio'}, 'vez': {'demasiado'}})\n"
     ]
    }
   ],
   "source": [
    "# Crear el mapa: {sustantivo: {(adjetivo))}}\n",
    "caracteristicasNGramas = defaultdict(set)\n",
    "\n",
    "for ngrama in ngramas:\n",
    "    doc = nlp(ngrama)\n",
    "    sustantivoActual = None\n",
    "    for token in doc:\n",
    "        if token.pos_ == \"NOUN\":\n",
    "            sustantivoActual = token.lemma_.lower()\n",
    "        elif token.pos_ == \"ADJ\" and sustantivoActual:\n",
    "            caracteristicasNGramas[sustantivoActual].add(token.lemma_.lower())\n",
    "\n",
    "print(caracteristicasNGramas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae4776d-ec82-475a-a99b-5534af9140f3",
   "metadata": {},
   "source": [
    "Se puede apreciar como para las reviews de ejemplo que hemos utilizado hay incongruencias, es decir, se registra que los trenes son lentos pero que el metro es rápido. Además, la librería interpreta algunas palabras como sustantivos o como adjetivos que no son."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec0534c-7f0e-419c-9736-3fbdbbe65801",
   "metadata": {},
   "source": [
    "### Librerías con redes neuronales\n",
    "\n",
    "Con los n-gramas solo puedes ver las palabras que tengan cerca. Esto en la mayoria de casos es lo que buscas y necesitas pero sin embargo en otras ocasiones un adjetivo al final de la oración puede referirse a un sustantivo del principio o un adjetivo cerca de un sustantivo puede referirse a otro sustantivo, y esto con los n-gramas no se puede ver. Para ello, haremos uso de las distinntas librerías de python para realizar un análisis más exahustivo de cada review y poder tener resultados más precisos.\n",
    "\n",
    "El uso de estas librerías va a ser muy similar al de los n-gramas. Vamos a sacar los distintos adjetivos que esten relacionados con los sustantivos correspondientes. De esta manera se ahorra tiempo ya que podemos hacerlo evitando la repetición de los n-gramas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b50b57-ad46-42a4-a1ad-fd035fd81de0",
   "metadata": {},
   "source": [
    "#### SpaCy\n",
    "\n",
    "La primera libreía que vamos a utilizar es la de spaCy, previamente utilizada para saber que clase de palabra era cada una en las reviews. Sin embargo, esta librería no solo es capaz de saber la clase gramatical de las palabras sino que además es capaz de decirte con que palabra esta relacionada, que es justo lo que necesitamos saber para extraer las características de las distintas reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7552f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = [\n",
    "    \"La estación de metro es moderna y limpia, pero los trenes son lentos.\",\n",
    "    \"A veces hay demasiada gente y el servicio es malo.\",\n",
    "    \"El metro de Madrid es rápido y eficiente, aunque algunas estaciones están sucias.\",\n",
    "    \"Buena conexión con otras líneas, pero los horarios nunca son confiables.\",\n",
    "    \"La estacion es muy fea y está muy mal cuidada\",\n",
    "    \"Los horarios son muy utiles\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd8edcb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"es_core_news_md\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee264b38-cf0a-4736-881a-2074544535fe",
   "metadata": {},
   "source": [
    "##### Vieja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2bdaacc-a5f1-4907-9ed7-e6268f665a9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'set'>, {'estación': {'sucio', 'moderno'}, 'tren': {'lento'}, 'servicio': {'malo'}, 'metro': {'rápido'}, 'conexión': {'buen', 'confiable'}, 'horario': {'confiable'}})\n"
     ]
    }
   ],
   "source": [
    "caracteristicasSpacy = defaultdict(set)\n",
    "\n",
    "# Procesar cada review\n",
    "for review in reviews:\n",
    "    doc = nlp(review)\n",
    "    for token in doc:\n",
    "        if token.pos_ == \"ADJ\":\n",
    "            adjetivo = token.lemma_.lower()\n",
    "            for child in token.children:\n",
    "                if child.pos_ == \"NOUN\":\n",
    "                    sustantivo = child.lemma_.lower()\n",
    "                    caracteristicasSpacy[sustantivo].add(adjetivo)\n",
    "            if token.head.pos_ == \"NOUN\":\n",
    "                sustantivo = token.head.lemma_.lower()\n",
    "                caracteristicasSpacy[sustantivo].add(adjetivo)\n",
    "        elif token.pos_ == \"NOUN\":\n",
    "            sustantivo = token.lemma_.lower()\n",
    "            if token.head.pos_== \"ADJ\":\n",
    "                adjetivo = token.head.lemma_\n",
    "                caracteristicasSpacy[sustantivo].add(adjetivo)\n",
    "\n",
    "print(caracteristicasSpacy)         "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ed7276-f702-42ee-b6ab-a7cd3effbc35",
   "metadata": {},
   "source": [
    "##### Adverbio + adjetivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2ad649-2777-40a8-bbe5-5f5431f8662f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_forms(adj):\n",
    "    \"\"\"\n",
    "    Gather all adjective variants:\n",
    "      1) If there are no advmods → base lemma (\"limpio\")\n",
    "      2) If there are >=1 advmods → join them all + lemma\n",
    "         e.g. [\"muy\",\"mal\"] + \"cuidada\" → \"muy mal cuidada\"\n",
    "      3) Recursively include any conjunct ADJ (dep==\"conj\")\n",
    "         only if that conjunct has NO of its own subject\n",
    "    \"\"\"\n",
    "    # 1) get all ADV modifiers in document order\n",
    "    advs = sorted(\n",
    "        [c for c in adj.children if c.dep_ == \"advmod\" and c.pos_ == \"ADV\"],\n",
    "        key=lambda c: c.i\n",
    "    )\n",
    "    adv_lemmas = [c.lemma_.lower() for c in advs]\n",
    "    base = adj.lemma_.lower()\n",
    "\n",
    "    forms = set()\n",
    "    if adv_lemmas:\n",
    "        forms.add(\" \".join(adv_lemmas + [base]))\n",
    "    else:\n",
    "        forms.add(base)\n",
    "\n",
    "    # 2) propagate any simple conjunctions\n",
    "    for child in adj.children:\n",
    "        if child.dep_ == \"conj\" and child.pos_ == \"ADJ\":\n",
    "            # only if that conjunct has no own noun subject\n",
    "            if not any(c.dep_ == \"nsubj\" and c.pos_ == \"NOUN\"\n",
    "                       for c in child.children):\n",
    "                forms |= collect_forms(child)\n",
    "\n",
    "    return forms\n",
    "\n",
    "def find_noun(adj):\n",
    "    \"\"\"\n",
    "    Given an ADJ token, locate its corresponding NOUN (if any) by:\n",
    "      – direct modifier: dep_==\"amod\" → head NOUN\n",
    "      – predicative: child dep_==\"nsubj\" → that NOUN\n",
    "      – coordination: dep_==\"conj\" → recurse to head ADJ\n",
    "      – copular chains: dep_ in {\"acomp\",\"attr\"} on a VERB → its nsubj NOUN\n",
    "    \"\"\"\n",
    "    # a) direct modifier\n",
    "    if adj.dep_ == \"amod\" and adj.head.pos_ == \"NOUN\":\n",
    "        return adj.head\n",
    "\n",
    "    # b) predicative adjective\n",
    "    for child in adj.children:\n",
    "        if child.dep_ == \"nsubj\" and child.pos_ == \"NOUN\":\n",
    "            return child\n",
    "\n",
    "    # c) coordinated adjective: inherit from head ADJ\n",
    "    if adj.dep_ == \"conj\" and adj.head.pos_ == \"ADJ\":\n",
    "        return find_noun(adj.head)\n",
    "\n",
    "    # d) in case of acomp/attr on a verb\n",
    "    if adj.dep_ in {\"acomp\", \"attr\"} and adj.head.pos_ == \"VERB\":\n",
    "        for child in adj.head.children:\n",
    "            if child.dep_ == \"nsubj\" and child.pos_ == \"NOUN\":\n",
    "                return child\n",
    "\n",
    "    return None\n",
    "\n",
    "def extract_aspects(reviews):\n",
    "    \"\"\"\n",
    "    Returns: defaultdict(set) mapping noun → set of adjectival phrases.\n",
    "    \"\"\"\n",
    "    nlp = spacy.load(\"es_core_news_sm\")\n",
    "    aspects = defaultdict(set)\n",
    "\n",
    "    for doc in nlp.pipe(reviews, batch_size=20):\n",
    "        for token in doc:\n",
    "            # only care about ADJ tokens\n",
    "            if token.pos_ != \"ADJ\":\n",
    "                continue\n",
    "            noun = find_noun(token)\n",
    "            if noun:\n",
    "                aspects[noun.lemma_.lower()] |= collect_forms(token)\n",
    "\n",
    "    return aspects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "189eab81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(set,\n",
       "            {'estación': {'limpio', 'moderno', 'sucio'},\n",
       "             'servicio': {'malo'},\n",
       "             'metro': {'eficiente', 'rápido'},\n",
       "             'conexión': {'buen'},\n",
       "             'horario': {'mucho util', 'nunca confiable'}})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_aspects(reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ebbea1-ab35-419c-8698-13ba9d5879a3",
   "metadata": {},
   "source": [
    "#### Stanza\n",
    "\n",
    "La otra librería que vamos a utilizar es stanza que es una librería de python desarrollada por Standford. Esta librería es muy similar a la de spaCy, la cual, también permite saber la clase gramatical a la cual pertenece la palabra y con que palabra en la oración esta relacionada. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0feb8ce7-0d3e-45b7-a6fd-148cf3e62382",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_forms(adj):\n",
    "    \"\"\"\n",
    "    Gather all variants of an adjective token:\n",
    "      1) If there are NO advmods: include the base lemma (\"limpio\")\n",
    "      2) If there is >=1 advmod: join them all + lemma\n",
    "         e.g. [\"muy\",\"mal\"] + \"cuidada\" → \"muy mal cuidada\"\n",
    "      3) Propagate conjunctions only if the conjunct ADJ has NO own subject\n",
    "    \"\"\"\n",
    "    # 1) collect all ADV modifiers in document order\n",
    "    advs = sorted(\n",
    "        [c for c in adj.children if c.dep_ == \"advmod\" and c.pos_ == \"ADV\"],\n",
    "        key=lambda c: c.i\n",
    "    )\n",
    "    adv_lemmas = [c.lemma_.lower() for c in advs]\n",
    "    base = adj.lemma_.lower()\n",
    "\n",
    "    forms = set()\n",
    "    if adv_lemmas:\n",
    "        # join every adv + base\n",
    "        forms.add(\" \".join(adv_lemmas + [base]))\n",
    "    else:\n",
    "        forms.add(base)\n",
    "\n",
    "    # 2) propagate simple conjunctions\n",
    "    for child in adj.children:\n",
    "        if child.dep_ == \"conj\" and child.pos_ == \"ADJ\":\n",
    "            # only if that conjunct has no its own nsubj\n",
    "            has_own_subj = any(\n",
    "                c.dep_ == \"nsubj\" and c.pos_ == \"NOUN\"\n",
    "                for c in child.children\n",
    "            )\n",
    "            if not has_own_subj:\n",
    "                forms |= collect_forms(child)\n",
    "\n",
    "    return forms\n",
    "\n",
    "def extract_aspects(reviews):\n",
    "    \"\"\"\n",
    "    Returns defaultdict(set) mapping noun → set of adjectival forms.\n",
    "    Catches:\n",
    "      • Direct modifiers (amod)\n",
    "      • Predicative adjectives (nsubj + cop)\n",
    "      • Coordinated adjectives (conj) with or without advmods\n",
    "    \"\"\"\n",
    "    nlp = spacy.load(\"es_core_news_sm\")\n",
    "    aspects = defaultdict(set)\n",
    "\n",
    "    for doc in nlp.pipe(reviews, batch_size=20):\n",
    "        for token in doc:\n",
    "            # — 1) Direct modifier on a noun\n",
    "            if token.dep_ == \"amod\" and token.head.pos_ == \"NOUN\":\n",
    "                noun = token.head.lemma_.lower()\n",
    "                aspects[noun] |= collect_forms(token)\n",
    "\n",
    "            # — 2) Any adjective as predicative or coord:\n",
    "            if token.pos_ == \"ADJ\":\n",
    "                # a) look for noun subjects on the ADJ\n",
    "                subjects = [\n",
    "                    c for c in token.children\n",
    "                    if c.dep_ == \"nsubj\" and c.pos_ == \"NOUN\"\n",
    "                ]\n",
    "                # b) if it’s a conj-ADJ with no subj, inherit from head-ADJ\n",
    "                if not subjects and token.dep_ == \"conj\" and token.head.pos_ == \"ADJ\":\n",
    "                    subjects = [\n",
    "                        c for c in token.head.children\n",
    "                        if c.dep_ == \"nsubj\" and c.pos_ == \"NOUN\"\n",
    "                    ]\n",
    "                # assign every found subject\n",
    "                for subj in subjects:\n",
    "                    noun = subj.lemma_.lower()\n",
    "                    aspects[noun] |= collect_forms(token)\n",
    "\n",
    "    return aspects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ef6069-cfb8-4598-bec6-ccc2821e2a8e",
   "metadata": {},
   "source": [
    "### Comparación de resultados\n",
    "\n",
    "Vamos a ver que resultados sacan cada opción previamente explicadas y compararlas entre sí. De esta manera nos quedaremos con el que mejor resultados de para utilizarlo con todas y cada una de las reviews de las distintas estaciones de metro que tenemos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b71bf01c-87a2-4efa-8887-acee6a8f4388",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Las caracteristicas de los n-gramas \n",
      "\n",
      "defaultdict(<class 'set'>, {'estación': {'sucio'}, 'conexión': {'horario', 'líneas'}, 'horario': {'confiable'}, 'gente': {'servicio'}, 'servicio': {'malo'}, 'tren': {'lento'}, 'línea': {'horario'}, 'madrid': {'eficiente', 'rápido'}, 'metro': {'eficiente', 'limpio', 'rápido', 'moderno'}, 'moderna': {'limpio'}, 'vez': {'demasiado'}})\n",
      "\n",
      "\n",
      "Las caracteristicas de spacy \n",
      "\n",
      "defaultdict(<class 'set'>, {'estación': {'sucio', 'moderno'}, 'tren': {'lento'}, 'servicio': {'malo'}, 'metro': {'rápido'}, 'conexión': {'buen', 'confiable'}, 'horario': {'confiable'}})\n",
      "\n",
      "\n",
      "Las caracteristicas de stanza \n",
      "\n",
      "defaultdict(<class 'set'>, {'estación': {'sucia', 'moderno'}, 'tren': {'lento'}, 'servicio': {'malo'}, 'metro': {'rápido'}, 'conexión': {'buena', 'confiable'}, 'horario': {'confiable'}})\n"
     ]
    }
   ],
   "source": [
    "print(\"Las caracteristicas de los n-gramas \\n\")\n",
    "print(caracteristicasNGramas)\n",
    "print(\"\\n\")\n",
    "print(\"Las caracteristicas de spacy \\n\")\n",
    "print(caracteristicasSpacy)\n",
    "print(\"\\n\")\n",
    "print(\"Las caracteristicas de stanza \\n\")\n",
    "print(caracteristicasStanza)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a629dd-79bf-4236-8670-2a43d4b5c86f",
   "metadata": {},
   "source": [
    "Podemos observar como con los n-gramas parece que se sacan más características que con las librerías. Sin embargo, si lo analizamos más profundamente podemos ver como con los n-gramas se añaden palabras que no son sustantivos o que no son adjetivos o que no tienen ninguna relevancia. Esto demuestra que los resultados con las librerías son más precisas. Sin embargo, las librerías alguna característica como que la estación esta limpia también se la ha saltado. \n",
    "\n",
    "Entre las dos librerías usadas vemos como dan resultados exactamente iguales, por lo que nos quedaremos con spacy debido a que podemos ver los hijos de una palabra y la palabra con la que esta relacionada. Esto cuando haya más reviews y más largas da más seguridad en que se van a sacar el máximo de características posibles aunque se pierdan algunas como ya hemos podido ver anteriormente. \n",
    "\n",
    "Por todo esto la librería que vamos a usar para extraer todas las psoibles características va a ser spacy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1544027d-afce-4113-ae86-8c474a9264fc",
   "metadata": {},
   "source": [
    "## Obtención de todas las características de las reviews\n",
    "\n",
    "Una vez realizada la comparación nos quedaremos con la librería spacy que es la que mejor resultados obtuvo. A continuación realizamos la extracción de las caracteristicas para cada una de las distintas estaciones de metro."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69f3bca-f02c-477f-a53f-e490f7e2446a",
   "metadata": {},
   "source": [
    "Para ello vamos a realizar otra pasada a las reviews y contando su frecuencia para después ver si son relevantes o no"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c84e2c7-106e-476d-92e1-f943e42aae2b",
   "metadata": {},
   "source": [
    "Función auxiliar para contar la frecuencia:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "62826944-5b6f-446c-a2d3-fbecb8bd4587",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agrega un adjetivo con su frecuencia al sustantivo correspondiente en el diccionario.\n",
    "def addAdjective(sustantivo, adjetivo):\n",
    "    for adj, freq in caracteristicas[sustantivo]:\n",
    "        if adj == adjetivo:\n",
    "            caracteristicas[sustantivo].remove((adj, freq))\n",
    "            caracteristicas[sustantivo].add((adj, freq + 1))\n",
    "            return\n",
    "    caracteristicas[sustantivo].add((adjetivo, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a8e6b710-e324-49c6-80df-622c5a4e0341",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'set'>, {'estación': {('sucio', 2), ('moderno', 2)}, 'tren': {('lento', 2)}, 'servicio': {('malo', 2)}, 'metro': {('rápido', 2)}, 'conexión': {('confiable', 1), ('buen', 1)}, 'horario': {('confiable', 2)}})\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "caracteristicas = defaultdict(set)\n",
    "\n",
    "# Procesar cada review\n",
    "for review in reviews:\n",
    "    doc = nlp(review)\n",
    "    for token in doc:\n",
    "        if token.pos_ == \"ADJ\":\n",
    "            adjetivo = token.lemma_.lower()\n",
    "            for child in token.children:\n",
    "                if child.pos_ == \"NOUN\":\n",
    "                    sustantivo = child.lemma_.lower()\n",
    "                    addAdjective(sustantivo, adjetivo)\n",
    "            if token.head.pos_ == \"NOUN\":\n",
    "                sustantivo = token.head.lemma_.lower()\n",
    "                addAdjective(sustantivo, adjetivo)\n",
    "        elif token.pos_ == \"NOUN\":\n",
    "            sustantivo = token.lemma_.lower()\n",
    "            if token.head.pos_== \"ADJ\":\n",
    "                adjetivo = token.head.lemma_\n",
    "                addAdjective(sustantivo, adjetivo)\n",
    "\n",
    "print(caracteristicas) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dfcf568-cf09-4a1d-bb45-e49ea600515c",
   "metadata": {},
   "source": [
    "Como puede haber discrepancias entre las caracteristicas que hemos sacado debido a la variabilidad de todas las reviews solo se van a quedar los que no tengan antonimos para un mismo sustantivo o el antonimo que mayor frecuencia tenga. Por ejemplo, si tenemos que el metro esta limpio con una frecuencia de 7 y que el metro esta sucio con una frecuencia de 5, se quedaría que el metro esta limpio.\n",
    "\n",
    "Sin embargo, para poder sacar los antonimos de la mejor manera posible vamos a traducirlos al ingles y luego de vuelta al español debido a que en ingles esta mucho mejor optimizado y falla mucho menos que en español."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0353e3bb-ff80-4987-bdeb-afcb2ad0d3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Traducir palabra al ingles\n",
    "def traducir(palabra, sr, tg):\n",
    "    traduccion = GoogleTranslator(source=sr, target=tg).translate(palabra)\n",
    "    return traduccion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ebee42ff-7199-4ec0-a420-b67377af2149",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener todos los antonimos\n",
    "def getAntonimos(palabra):\n",
    "    antonimos = set()\n",
    "    for sentido in wn.synsets(palabra, pos=wn.ADJ):  # todos los significados de la palabra como adjetivo\n",
    "        for lema in sentido.lemmas(): # cada sinónimo dentro de ese significado\n",
    "            for antonimo in lema.antonyms():  # para cada sinónimo, se busca sus antónimos\n",
    "                antonimos.add(antonimo.name()) # se añade el nombre del antónimo al set\n",
    "    return antonimos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d23188ce-77c3-4208-8753-c8cebfaf1408",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quitar los antonimos que tengan menor frecuencia\n",
    "def quitarAntonimos(diccionario):\n",
    "    for sustantivo, adjs in diccionario.items():\n",
    "        lista = list(adjs) # los pasamos a lista para poder iterar\n",
    "        nuevosAdjs = set() # adjetivos que van a quedar después del filtro\n",
    "        procesados = set() # para no comparar un adjetivo más de una vez\n",
    "\n",
    "        for i, (adj1, freq1) in enumerate(lista):\n",
    "            if adj1 in procesados:\n",
    "                continue # si fue comparado se salta\n",
    "\n",
    "            antonimosIngles = getAntonimos(traducir(adj1,'es','en')) # traducimos las palabras para sacar mejor los antonimos\n",
    "            antonimos = [traducir(ant, 'en', 'es').lower() for ant in antonimosIngles]\n",
    "            emparejado = False\n",
    "\n",
    "            for j in range(i + 1, len(lista)):\n",
    "                adj2, freq2 = lista[j]\n",
    "                if adj2 in procesados: # si fue comparado se salta\n",
    "                    continue\n",
    "                    \n",
    "                if adj2 in antonimos: # se guarda el de mayor frecuencia\n",
    "                    if freq1 > freq2: \n",
    "                        nuevosAdjs.add((adj1, freq1))\n",
    "                    elif freq1 < freq2:\n",
    "                        nuevosAdjs.add((adj2, freq2))\n",
    "                        \n",
    "                    procesados.update([adj1, adj2])\n",
    "                    emparejado = True\n",
    "                    break\n",
    "\n",
    "            if not emparejado: # si no tenía ningún antónimo en la lista se agrega tal cual\n",
    "                nuevosAdjs.add((adj1, freq1))\n",
    "                procesados.add(adj1)\n",
    "\n",
    "        diccionario[sustantivo] = nuevosAdjs     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d19a1a1f-ecca-45b9-9137-c0caeb233e9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'estación': {('sucio', 2), ('moderno', 2)}, 'tren': {('lento', 2)}, 'servicio': {('malo', 2)}, 'conexión': {('confiable', 1), ('buen', 1)}, 'horario': {('confiable', 2)}, 'metro': {('limpio', 7), ('rápido', 4)}}\n"
     ]
    }
   ],
   "source": [
    "aux = {'estación': {('sucio', 2), ('moderno', 2)}, 'tren': {('lento', 2)}, 'servicio': {('malo', 2)}, 'conexión': {('confiable', 1), ('buen', 1)}, 'horario': {('confiable', 2)}, \"metro\": {(\"limpio\", 7), (\"rápido\", 4), (\"lento\", 2), (\"sucio\",5)}}\n",
    "\n",
    "quitarAntonimos(aux)\n",
    "print(aux)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46feab79-b84a-4d33-bb1d-d6a9daa94992",
   "metadata": {},
   "source": [
    "Una vez hemos quitado los antonimos con menos frecuencia, es decir, caracteristicas irrelevantes, vamos a añadir y guardar las características sin la frecuencia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "98bfdbc2-24a2-4da0-b2bf-c1dbefa9bcf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'estación': {'sucio', 'moderno'}, 'tren': {'lento'}, 'servicio': {'malo'}, 'metro': {'limpio', 'rápido'}, 'conexión': {'buen', 'confiable'}, 'horario': {'confiable'}}\n"
     ]
    }
   ],
   "source": [
    "caracteristicasFinal = {}\n",
    "\n",
    "def quitarFrecuencias(dicF, dic):\n",
    "    for sustantivo, adjetivos in dicF.items():\n",
    "        dic[sustantivo] = {adj for adj, _ in adjetivos}\n",
    "    return dic\n",
    "\n",
    "caracteristicasFinal = quitarFrecuencias(aux, caracteristicasFinal)\n",
    "print(caracteristicasFinal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606fb135-74b3-4691-b9ea-703e2c7c3377",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfg-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
